"""
ğŸ” AuditAgent + Reinforcement Learning System
ê° ì—ì´ì „íŠ¸ì˜ ì„±ê³¼ë¥¼ ëª¨ë‹ˆí„°ë§í•˜ê³  ë²Œì /ë³´ìƒì„ ì£¼ëŠ” ì‹œìŠ¤í…œ
"""

import asyncio
import json
import time
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum

import numpy as np
from openai import AsyncOpenAI
from ..ai_engine.real_data_audit_system import analyze_sensor_gaps


class AgentPerformanceGrade(Enum):
    """ì—ì´ì „íŠ¸ ì„±ê³¼ ë“±ê¸‰"""
    EXCELLENT = "A+"  # 95-100ì 
    GOOD = "A"        # 85-94ì   
    AVERAGE = "B"     # 70-84ì 
    POOR = "C"        # 50-69ì 
    FAIL = "F"        # 0-49ì 


@dataclass
class AgentAuditResult:
    """ì—ì´ì „íŠ¸ ê°ì‚¬ ê²°ê³¼"""
    agent_name: str
    task_id: str
    timestamp: datetime
    
    # ì„±ê³¼ ë©”íŠ¸ë¦­
    data_quality_score: float = 0.0      # ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ (0-1)
    task_completion_score: float = 0.0   # ì‘ì—… ì™„ì„±ë„ ì ìˆ˜ (0-1)
    accuracy_score: float = 0.0          # ì •í™•ì„± ì ìˆ˜ (0-1)
    efficiency_score: float = 0.0        # íš¨ìœ¨ì„± ì ìˆ˜ (0-1)
    innovation_score: float = 0.0        # í˜ì‹ ì„± ì ìˆ˜ (0-1)
    
    # ì¢…í•© í‰ê°€
    overall_score: float = 0.0           # ì¢…í•© ì ìˆ˜ (0-100)
    grade: AgentPerformanceGrade = AgentPerformanceGrade.AVERAGE
    
    # êµ¬ì²´ì  í”¼ë“œë°±
    strengths: List[str] = field(default_factory=list)      # ì˜í•œ ì 
    weaknesses: List[str] = field(default_factory=list)     # ë¶€ì¡±í•œ ì 
    penalties: List[str] = field(default_factory=list)      # ë²Œì  ì‚¬ìœ 
    rewards: List[str] = field(default_factory=list)        # ë³´ìƒ ì‚¬ìœ 
    
    # ê°œì„  ë°©í–¥
    improvement_suggestions: List[str] = field(default_factory=list)
    
    # ë²Œì /ë³´ìƒ ì ìˆ˜
    penalty_points: float = 0.0          # ë²Œì  (-ì ìˆ˜)
    reward_points: float = 0.0           # ë³´ìƒ (+ì ìˆ˜)


@dataclass
class AgentLearningHistory:
    """ì—ì´ì „íŠ¸ í•™ìŠµ ì´ë ¥"""
    agent_name: str
    total_tasks: int = 0
    successful_tasks: int = 0
    failed_tasks: int = 0
    
    # ì„±ê³¼ ì´ë ¥
    average_score: float = 0.0
    best_score: float = 0.0
    worst_score: float = 100.0
    
    # í•™ìŠµ ë©”íŠ¸ë¦­
    learning_rate: float = 0.1           # í•™ìŠµë¥ 
    adaptation_speed: float = 0.0        # ì ì‘ ì†ë„
    consistency_score: float = 0.0       # ì¼ê´€ì„± ì ìˆ˜
    
    # ë²Œì /ë³´ìƒ ëˆ„ì 
    total_penalties: float = 0.0
    total_rewards: float = 0.0
    current_penalty_multiplier: float = 1.0  # í˜„ì¬ ë²Œì  ë°°ìˆ˜
    
    # ìµœê·¼ ì„±ê³¼ íŠ¸ë Œë“œ
    recent_scores: List[float] = field(default_factory=list)
    improvement_trend: float = 0.0       # ê°œì„  ì¶”ì„¸ (-1~1)


class AuditAgent:
    """ğŸ” ê°ì‚¬ ì—ì´ì „íŠ¸ - ë‹¤ë¥¸ ì—ì´ì „íŠ¸ë“¤ì˜ ì„±ê³¼ë¥¼ ê°ì‹œí•˜ê³  í‰ê°€"""
    
    def __init__(self):
        self.name = "AuditAgent"
        self.role = "ì—ì´ì „íŠ¸ ì„±ê³¼ ê°ì‹œ ë° í‰ê°€"
        self.openai_client = None
        
        # ê°ì‚¬ ê¸°ì¤€
        self.audit_criteria = {
            "data_quality": {
                "ì™„ì „ì„±": 0.3,    # ë°ì´í„° ìˆ˜ì§‘ ì™„ì „ì„±
                "ì •í™•ì„±": 0.4,    # ë°ì´í„° ì •í™•ì„±
                "ì ì‹œì„±": 0.3     # ì ì ˆí•œ ì‹œê°„ ë‚´ ìˆ˜ì§‘
            },
            "task_completion": {
                "ìš”êµ¬ì‚¬í•­_ì¶©ì¡±": 0.4,  # ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ ì¶©ì¡±
                "ì‘ì—…_ì™„ì„±ë„": 0.3,    # ì‘ì—…ì˜ ì™„ì„±ë„
                "ì˜ˆì™¸ì²˜ë¦¬": 0.3       # ì˜ˆì™¸ ìƒí™© ì²˜ë¦¬
            },
            "accuracy": {
                "ê²°ê³¼_ì •í™•ì„±": 0.5,    # ê²°ê³¼ì˜ ì •í™•ì„±
                "ë…¼ë¦¬_ì¼ê´€ì„±": 0.3,    # ë…¼ë¦¬ì  ì¼ê´€ì„±
                "ì‚¬ì‹¤_ê²€ì¦": 0.2      # ì‚¬ì‹¤ ê²€ì¦
            },
            "efficiency": {
                "ì‘ë‹µ_ì†ë„": 0.3,     # ì‘ë‹µ ì†ë„
                "ë¦¬ì†ŒìŠ¤_ì‚¬ìš©": 0.4,   # ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„±
                "ì¤‘ë³µ_ì œê±°": 0.3      # ë¶ˆí•„ìš”í•œ ì¤‘ë³µ ì œê±°
            },
            "innovation": {
                "ì°½ì˜ì„±": 0.4,        # ì°½ì˜ì  ì ‘ê·¼
                "ë¬¸ì œí•´ê²°": 0.3,      # ë¬¸ì œ í•´ê²° ëŠ¥ë ¥
                "ê°œì„ ì œì•ˆ": 0.3       # ê°œì„  ì œì•ˆ
            }
        }
        
        # ë²Œì  ê¸°ì¤€
        self.penalty_criteria = {
            "ë°ì´í„°_ëˆ„ë½": -10,           # ì¤‘ìš” ë°ì´í„° ëˆ„ë½
            "ì˜ëª»ëœ_ë¶„ì„": -15,           # ì˜ëª»ëœ ë¶„ì„ ê²°ê³¼
            "ì‘ë‹µ_ì§€ì—°": -5,             # ì‘ë‹µ ì‹œê°„ ì´ˆê³¼
            "ë…¼ë¦¬_ëª¨ìˆœ": -20,            # ë…¼ë¦¬ì  ëª¨ìˆœ
            "ìš”êµ¬ì‚¬í•­_ë¯¸ì¶©ì¡±": -25,       # ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ ë¯¸ì¶©ì¡±
            "ì¤‘ë³µ_ì‘ì—…": -8,             # ë¶ˆí•„ìš”í•œ ì¤‘ë³µ
            "ìì›_ë‚­ë¹„": -12,            # ì»´í“¨íŒ… ìì› ë‚­ë¹„
        }
        
        # ë³´ìƒ ê¸°ì¤€
        self.reward_criteria = {
            "ì™„ë²½í•œ_ë°ì´í„°ìˆ˜ì§‘": 15,      # ì™„ë²½í•œ ë°ì´í„° ìˆ˜ì§‘
            "ë›°ì–´ë‚œ_ì¸ì‚¬ì´íŠ¸": 20,        # ë›°ì–´ë‚œ ë¶„ì„ ì¸ì‚¬ì´íŠ¸
            "ë¹ ë¥¸_ì‘ë‹µ": 10,             # ë¹ ë¥¸ ì‘ë‹µ ì‹œê°„
            "ì°½ì˜ì _ì ‘ê·¼": 25,           # ì°½ì˜ì  ë¬¸ì œ í•´ê²°
            "ì •í™•í•œ_ì˜ˆì¸¡": 30,           # ì •í™•í•œ ì˜ˆì¸¡
            "íš¨ìœ¨ì _ì²˜ë¦¬": 12,           # íš¨ìœ¨ì  ì²˜ë¦¬
            "ì‚¬ìš©ì_ë§Œì¡±": 35,           # ë†’ì€ ì‚¬ìš©ì ë§Œì¡±ë„
        }
    
    async def initialize(self, openai_client: AsyncOpenAI = None):
        """ê°ì‚¬ ì—ì´ì „íŠ¸ ì´ˆê¸°í™”"""
        self.openai_client = openai_client
        print(f"ğŸ” {self.name} ì´ˆê¸°í™” ì™„ë£Œ - ê°ì‚¬ ê¸°ì¤€: {len(self.audit_criteria)}ê°œ ì˜ì—­")
    
    async def audit_agent_performance(
        self, 
        agent_name: str,
        task_query: str,
        agent_context: Any,
        execution_time: float,
        user_feedback: Optional[str] = None
    ) -> AgentAuditResult:
        """ì—ì´ì „íŠ¸ ì„±ê³¼ ì¢…í•© ê°ì‚¬"""
        
        print(f"\nğŸ” === {agent_name} ì„±ê³¼ ê°ì‚¬ ì‹œì‘ ===")
        
        task_id = f"{agent_name}_{int(time.time())}"
        audit_result = AgentAuditResult(
            agent_name=agent_name,
            task_id=task_id,
            timestamp=datetime.now()
        )
        
        # 1. ë°ì´í„° í’ˆì§ˆ í‰ê°€
        audit_result.data_quality_score = await self._evaluate_data_quality(agent_context)
        
        # 2. ì‘ì—… ì™„ì„±ë„ í‰ê°€
        audit_result.task_completion_score = await self._evaluate_task_completion(
            task_query, agent_context
        )
        
        # 3. ì •í™•ì„± í‰ê°€
        audit_result.accuracy_score = await self._evaluate_accuracy(agent_context)
        
        # 4. íš¨ìœ¨ì„± í‰ê°€
        audit_result.efficiency_score = await self._evaluate_efficiency(
            execution_time, agent_context
        )
        
        # 5. í˜ì‹ ì„± í‰ê°€
        audit_result.innovation_score = await self._evaluate_innovation(agent_context)
        
        # 6. ì¢…í•© ì ìˆ˜ ê³„ì‚°
        audit_result.overall_score = self._calculate_overall_score(audit_result)
        
        # 7. ë“±ê¸‰ ë¶€ì—¬
        audit_result.grade = self._assign_grade(audit_result.overall_score)
        
        # 8. êµ¬ì²´ì  í”¼ë“œë°± ìƒì„±
        await self._generate_detailed_feedback(audit_result, agent_context)
        
        # 9. ë²Œì /ë³´ìƒ ê³„ì‚°
        await self._calculate_penalties_and_rewards(audit_result, agent_context)
        
        print(f"âœ… {agent_name} ê°ì‚¬ ì™„ë£Œ - ì¢…í•©ì ìˆ˜: {audit_result.overall_score:.1f}, ë“±ê¸‰: {audit_result.grade.value}")
        
        return audit_result
    
    async def _evaluate_data_quality(self, agent_context: Any) -> float:
        """ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ í’ˆì§ˆ í‰ê°€"""
        
        try:
            # ì‹¤ì œ ì„¼ì„œ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
            if not hasattr(agent_context, 'raw_data') or not agent_context.raw_data:
                return 0.0
            
            # íƒ€ê²Ÿ ì„¼ì„œ ì¶”ì¶œ (ResearchAgentê°€ ìˆ˜ì§‘í•œ ë°ì´í„°ì—ì„œ)
            target_sensor = agent_context.raw_data.get("target_sensor")
            if not target_sensor:
                # ì „ì²´ ì„¼ì„œ ë°ì´í„°ì—ì„œ ì²« ë²ˆì§¸ ì„¼ì„œ ì¶”ì¶œ
                all_sensors = agent_context.raw_data.get("all_sensors", [])
                if not all_sensors:
                    return 0.3  # ê¸°ë³¸ê°’
                target_sensor = all_sensors[0].get('tag_name', 'D101')  # ê¸°ë³¸ ì„¼ì„œ
            
            print(f"   ğŸ” {target_sensor} ì„¼ì„œ ì‹¤ì œ ë°ì´í„° í’ˆì§ˆ í‰ê°€ ì¤‘...")
            
            # ì‹¤ì œ ë°ì´í„° ëˆ„ë½ ë¶„ì„ (ìµœê·¼ 24ì‹œê°„)
            gap_analysis = await analyze_sensor_gaps(target_sensor, 24)
            
            # í’ˆì§ˆ ì ìˆ˜ = ì™„ì„±ë„ ë¹„ìœ¨
            data_quality_score = gap_analysis.completeness_ratio
            
            print(f"   ğŸ“Š {target_sensor} ì‹¤ì œ ì™„ì„±ë„: {data_quality_score:.4f} ({gap_analysis.quality_grade})")
            print(f"   ğŸ“ˆ ëˆ„ë½ ë°ì´í„°: {gap_analysis.missing_data_points}/{gap_analysis.expected_data_points}ê°œ")
            
            return data_quality_score
            
        except Exception as e:
            print(f"   âŒ ì‹¤ì œ ë°ì´í„° í’ˆì§ˆ í‰ê°€ ì˜¤ë¥˜: {e}")
            return 0.2  # ì˜¤ë¥˜ì‹œ ë‚®ì€ ì ìˆ˜
    
    async def _evaluate_task_completion(self, task_query: str, agent_context: Any) -> float:
        """ì‘ì—… ì™„ì„±ë„ í‰ê°€"""
        score = 0.0
        
        # ìš”êµ¬ì‚¬í•­ ì¶©ì¡±ë„ (í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨ í‰ê°€)
        query_lower = task_query.lower()
        requirements_met = 0.6  # ê¸°ë³¸ê°’
        
        if "í˜„ì¬" in query_lower and hasattr(agent_context, 'raw_data'):
            if "all_sensors" in str(agent_context.raw_data) or "focused_data" in str(agent_context.raw_data):
                requirements_met = 0.9
        
        # ì‘ì—… ì™„ì„±ë„
        completion = 0.8 if hasattr(agent_context, 'insights') else 0.4
        
        # ì˜ˆì™¸ ì²˜ë¦¬
        exception_handling = 0.7 if not getattr(agent_context, 'error', None) else 0.3
        
        score = requirements_met * 0.4 + completion * 0.3 + exception_handling * 0.3
        return min(max(score, 0.0), 1.0)
    
    async def _evaluate_accuracy(self, agent_context: Any) -> float:
        """ì •í™•ì„± í‰ê°€"""
        score = 0.7  # ê¸°ë³¸ê°’
        
        # ê²°ê³¼ ì •í™•ì„± (ì¸ì‚¬ì´íŠ¸ ì¡´ì¬ ì—¬ë¶€)
        if hasattr(agent_context, 'insights') and agent_context.insights:
            score += 0.2
            
        # ë…¼ë¦¬ ì¼ê´€ì„± (í’ˆì§ˆ ë³´ê³ ì„œ í™•ì¸)
        if hasattr(agent_context, 'quality_report') and agent_context.quality_report:
            logic_score = agent_context.quality_report.get('logic_validation', {}).get('score', 0.5)
            score = score * 0.7 + logic_score * 0.3
        
        return min(max(score, 0.0), 1.0)
    
    async def _evaluate_efficiency(self, execution_time: float, agent_context: Any) -> float:
        """íš¨ìœ¨ì„± í‰ê°€"""
        # ì‘ë‹µ ì†ë„ í‰ê°€ (5ì´ˆ ì´í•˜ excellent, 10ì´ˆ ì´í•˜ good)
        if execution_time < 5:
            speed_score = 1.0
        elif execution_time < 10:
            speed_score = 0.8
        elif execution_time < 20:
            speed_score = 0.6
        else:
            speed_score = 0.3
        
        # ë¦¬ì†ŒìŠ¤ ì‚¬ìš© í‰ê°€ (ë°ì´í„° í¬ê¸° ê¸°ë°˜)
        resource_score = 0.8  # ê¸°ë³¸ê°’
        
        # ì¤‘ë³µ ì œê±° í‰ê°€
        deduplication_score = 0.7  # ê¸°ë³¸ê°’
        
        score = speed_score * 0.3 + resource_score * 0.4 + deduplication_score * 0.3
        return min(max(score, 0.0), 1.0)
    
    async def _evaluate_innovation(self, agent_context: Any) -> float:
        """í˜ì‹ ì„± í‰ê°€"""
        score = 0.5  # ê¸°ë³¸ê°’
        
        # ì°½ì˜ì  ì ‘ê·¼ (ë‹¤ì–‘í•œ ì¸ì‚¬ì´íŠ¸ ìƒì„±)
        if hasattr(agent_context, 'insights') and agent_context.insights:
            insight_count = len(agent_context.insights)
            creativity_score = min(insight_count / 3.0, 1.0)
            score += creativity_score * 0.3
            
        # ë¬¸ì œ í•´ê²° ëŠ¥ë ¥
        problem_solving_score = 0.6  # ê¸°ë³¸ê°’
        
        # ê°œì„  ì œì•ˆ
        if hasattr(agent_context, 'quality_report'):
            recommendations = agent_context.quality_report.get('recommendations', [])
            improvement_score = min(len(recommendations) / 3.0, 1.0)
            score += improvement_score * 0.2
        
        return min(max(score, 0.0), 1.0)
    
    def _calculate_overall_score(self, audit_result: AgentAuditResult) -> float:
        """ì¢…í•© ì ìˆ˜ ê³„ì‚° (0-100)"""
        weights = {
            "data_quality": 0.25,
            "task_completion": 0.30,
            "accuracy": 0.25,
            "efficiency": 0.15,
            "innovation": 0.05
        }
        
        weighted_score = (
            audit_result.data_quality_score * weights["data_quality"] +
            audit_result.task_completion_score * weights["task_completion"] +
            audit_result.accuracy_score * weights["accuracy"] +
            audit_result.efficiency_score * weights["efficiency"] +
            audit_result.innovation_score * weights["innovation"]
        ) * 100
        
        return min(max(weighted_score, 0.0), 100.0)
    
    def _assign_grade(self, overall_score: float) -> AgentPerformanceGrade:
        """ì ìˆ˜ì— ë”°ë¥¸ ë“±ê¸‰ ë¶€ì—¬"""
        if overall_score >= 95:
            return AgentPerformanceGrade.EXCELLENT
        elif overall_score >= 85:
            return AgentPerformanceGrade.GOOD
        elif overall_score >= 70:
            return AgentPerformanceGrade.AVERAGE
        elif overall_score >= 50:
            return AgentPerformanceGrade.POOR
        else:
            return AgentPerformanceGrade.FAIL
    
    async def _generate_detailed_feedback(self, audit_result: AgentAuditResult, agent_context: Any):
        """êµ¬ì²´ì  í”¼ë“œë°± ìƒì„±"""
        # ì˜í•œ ì 
        if audit_result.data_quality_score >= 0.8:
            audit_result.strengths.append("ğŸŒŸ ìš°ìˆ˜í•œ ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬")
        if audit_result.accuracy_score >= 0.8:
            audit_result.strengths.append("ğŸ¯ ë†’ì€ ì •í™•ì„±ê³¼ ì‹ ë¢°ì„±")
        if audit_result.efficiency_score >= 0.8:
            audit_result.strengths.append("âš¡ íš¨ìœ¨ì ì¸ ì‘ì—… ì²˜ë¦¬")
            
        # ë¶€ì¡±í•œ ì 
        if audit_result.data_quality_score < 0.6:
            audit_result.weaknesses.append("âŒ ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ ë¶€ì¡±")
        if audit_result.task_completion_score < 0.6:
            audit_result.weaknesses.append("ğŸ“‹ ì‘ì—… ì™„ì„±ë„ ë¯¸í¡")
        if audit_result.accuracy_score < 0.6:
            audit_result.weaknesses.append("ğŸ¯ ì •í™•ì„± ê°œì„  í•„ìš”")
            
        # ê°œì„  ë°©í–¥
        if audit_result.innovation_score < 0.5:
            audit_result.improvement_suggestions.append("ğŸ’¡ ì°½ì˜ì  ì ‘ê·¼ ë°©ì‹ ë„ì…")
        if audit_result.efficiency_score < 0.7:
            audit_result.improvement_suggestions.append("âš¡ ì‘ì—… íš¨ìœ¨ì„± ê°œì„ ")
    
    async def _calculate_penalties_and_rewards(self, audit_result: AgentAuditResult, agent_context: Any):
        """ë²Œì /ë³´ìƒ ê³„ì‚°"""
        # ë²Œì  ê³„ì‚°
        if audit_result.data_quality_score < 0.5:
            audit_result.penalties.append("ë°ì´í„°_ëˆ„ë½")
            audit_result.penalty_points += self.penalty_criteria["ë°ì´í„°_ëˆ„ë½"]
            
        if audit_result.accuracy_score < 0.5:
            audit_result.penalties.append("ì˜ëª»ëœ_ë¶„ì„")
            audit_result.penalty_points += self.penalty_criteria["ì˜ëª»ëœ_ë¶„ì„"]
            
        if audit_result.task_completion_score < 0.6:
            audit_result.penalties.append("ìš”êµ¬ì‚¬í•­_ë¯¸ì¶©ì¡±")
            audit_result.penalty_points += self.penalty_criteria["ìš”êµ¬ì‚¬í•­_ë¯¸ì¶©ì¡±"]
        
        # ë³´ìƒ ê³„ì‚°
        if audit_result.data_quality_score >= 0.9:
            audit_result.rewards.append("ì™„ë²½í•œ_ë°ì´í„°ìˆ˜ì§‘")
            audit_result.reward_points += self.reward_criteria["ì™„ë²½í•œ_ë°ì´í„°ìˆ˜ì§‘"]
            
        if audit_result.accuracy_score >= 0.9:
            audit_result.rewards.append("ë›°ì–´ë‚œ_ì¸ì‚¬ì´íŠ¸")
            audit_result.reward_points += self.reward_criteria["ë›°ì–´ë‚œ_ì¸ì‚¬ì´íŠ¸"]
            
        if audit_result.efficiency_score >= 0.9:
            audit_result.rewards.append("íš¨ìœ¨ì _ì²˜ë¦¬")
            audit_result.reward_points += self.reward_criteria["íš¨ìœ¨ì _ì²˜ë¦¬"]


class ReinforcementLearningSystem:
    """ğŸ§  ê°•í™”í•™ìŠµ ê¸°ë°˜ ì—ì´ì „íŠ¸ ì„±ê³¼ ê´€ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.agent_histories: Dict[str, AgentLearningHistory] = {}
        self.audit_agent = AuditAgent()
        
        # í•™ìŠµ íŒŒë¼ë¯¸í„°
        self.learning_decay = 0.95        # í•™ìŠµë¥  ê°ì†Œìœ¨
        self.penalty_amplifier = 1.2      # ë²Œì  ì¦í­ ê³„ìˆ˜
        self.reward_multiplier = 1.1      # ë³´ìƒ ë°°ìˆ˜
        self.min_learning_rate = 0.01     # ìµœì†Œ í•™ìŠµë¥ 
        
    async def initialize(self, openai_client: AsyncOpenAI = None):
        """ê°•í™”í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        await self.audit_agent.initialize(openai_client)
        print("ğŸ§  ê°•í™”í•™ìŠµ ê¸°ë°˜ ì„±ê³¼ ê´€ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def evaluate_and_learn(
        self,
        agent_name: str,
        task_query: str,
        agent_context: Any,
        execution_time: float,
        user_feedback: Optional[str] = None
    ) -> Tuple[AgentAuditResult, AgentLearningHistory]:
        """ì—ì´ì „íŠ¸ í‰ê°€ ë° í•™ìŠµ"""
        
        # 1. ê°ì‚¬ ìˆ˜í–‰
        audit_result = await self.audit_agent.audit_agent_performance(
            agent_name, task_query, agent_context, execution_time, user_feedback
        )
        
        # 2. í•™ìŠµ ì´ë ¥ ì—…ë°ì´íŠ¸
        history = await self._update_learning_history(agent_name, audit_result)
        
        # 3. ê°•í™”í•™ìŠµ ì ìš©
        await self._apply_reinforcement_learning(agent_name, audit_result, history)
        
        # 4. ì„±ê³¼ ë¦¬í¬íŠ¸ ìƒì„±
        await self._generate_performance_report(audit_result, history)
        
        return audit_result, history
    
    async def _update_learning_history(
        self, 
        agent_name: str, 
        audit_result: AgentAuditResult
    ) -> AgentLearningHistory:
        """í•™ìŠµ ì´ë ¥ ì—…ë°ì´íŠ¸"""
        
        if agent_name not in self.agent_histories:
            self.agent_histories[agent_name] = AgentLearningHistory(agent_name=agent_name)
        
        history = self.agent_histories[agent_name]
        
        # ê¸°ë³¸ í†µê³„ ì—…ë°ì´íŠ¸
        history.total_tasks += 1
        if audit_result.overall_score >= 70:
            history.successful_tasks += 1
        else:
            history.failed_tasks += 1
            
        # ì ìˆ˜ ì´ë ¥ ì—…ë°ì´íŠ¸
        history.recent_scores.append(audit_result.overall_score)
        if len(history.recent_scores) > 10:  # ìµœê·¼ 10ê°œë§Œ ìœ ì§€
            history.recent_scores.pop(0)
            
        # í‰ê·  ì ìˆ˜ ì—…ë°ì´íŠ¸
        history.average_score = sum(history.recent_scores) / len(history.recent_scores)
        history.best_score = max(history.best_score, audit_result.overall_score)
        history.worst_score = min(history.worst_score, audit_result.overall_score)
        
        # ê°œì„  ì¶”ì„¸ ê³„ì‚°
        if len(history.recent_scores) >= 3:
            recent_trend = np.polyfit(range(len(history.recent_scores)), history.recent_scores, 1)[0]
            history.improvement_trend = max(-1, min(1, recent_trend / 10))  # -1~1 ë²”ìœ„ë¡œ ì •ê·œí™”
        
        # ë²Œì /ë³´ìƒ ëˆ„ì 
        history.total_penalties += audit_result.penalty_points
        history.total_rewards += audit_result.reward_points
        
        return history
    
    async def _apply_reinforcement_learning(
        self,
        agent_name: str,
        audit_result: AgentAuditResult,
        history: AgentLearningHistory
    ):
        """ê°•í™”í•™ìŠµ ì ìš©"""
        
        # 1. í•™ìŠµë¥  ì¡°ì •
        if audit_result.overall_score < 60:
            # ì„±ê³¼ê°€ ë‚®ìœ¼ë©´ í•™ìŠµë¥  ì¦ê°€
            history.learning_rate = min(0.3, history.learning_rate * 1.2)
        elif audit_result.overall_score > 85:
            # ì„±ê³¼ê°€ ë†’ìœ¼ë©´ í•™ìŠµë¥  ê°ì†Œ
            history.learning_rate = max(self.min_learning_rate, history.learning_rate * self.learning_decay)
        
        # 2. ë²Œì  ë°°ìˆ˜ ì¡°ì • (ì—°ì† ì‹¤íŒ¨ì‹œ ë²Œì  ì¦ê°€)
        if audit_result.overall_score < 50:
            history.current_penalty_multiplier = min(3.0, history.current_penalty_multiplier * self.penalty_amplifier)
        elif audit_result.overall_score > 80:
            history.current_penalty_multiplier = max(1.0, history.current_penalty_multiplier * 0.9)
        
        # 3. ì ì‘ ì†ë„ ê³„ì‚°
        if len(history.recent_scores) >= 5:
            score_variance = np.var(history.recent_scores[-5:])
            history.adaptation_speed = 1.0 / (1.0 + score_variance / 100)  # ë¶„ì‚°ì´ ë‚®ì„ìˆ˜ë¡ ë†’ì€ ì ì‘ ì†ë„
        
        # 4. ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°
        if len(history.recent_scores) >= 3:
            consistency = 1.0 - (np.std(history.recent_scores[-3:]) / 100)
            history.consistency_score = max(0, consistency)
        
        print(f"ğŸ“Š {agent_name} ê°•í™”í•™ìŠµ ì ìš©:")
        print(f"   í•™ìŠµë¥ : {history.learning_rate:.3f}")
        print(f"   ë²Œì ë°°ìˆ˜: {history.current_penalty_multiplier:.2f}")
        print(f"   ì ì‘ì†ë„: {history.adaptation_speed:.3f}")
        print(f"   ì¼ê´€ì„±: {history.consistency_score:.3f}")
    
    async def _generate_performance_report(
        self, 
        audit_result: AgentAuditResult, 
        history: AgentLearningHistory
    ):
        """ì„±ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        print(f"\nğŸ“‹ === {audit_result.agent_name} ì„±ê³¼ ë¦¬í¬íŠ¸ ===")
        print(f"ğŸ¯ ì¢…í•© ì ìˆ˜: {audit_result.overall_score:.1f}/100 ({audit_result.grade.value})")
        print(f"ğŸ“Š ìƒì„¸ ì ìˆ˜:")
        print(f"   - ë°ì´í„° í’ˆì§ˆ: {audit_result.data_quality_score:.2f}")
        print(f"   - ì‘ì—… ì™„ì„±ë„: {audit_result.task_completion_score:.2f}")
        print(f"   - ì •í™•ì„±: {audit_result.accuracy_score:.2f}")
        print(f"   - íš¨ìœ¨ì„±: {audit_result.efficiency_score:.2f}")
        print(f"   - í˜ì‹ ì„±: {audit_result.innovation_score:.2f}")
        
        print(f"ğŸ’¯ ì„±ê³¼ ì´ë ¥:")
        print(f"   - ì´ ì‘ì—…: {history.total_tasks}íšŒ")
        print(f"   - ì„±ê³µë¥ : {(history.successful_tasks/history.total_tasks*100):.1f}%")
        print(f"   - í‰ê·  ì ìˆ˜: {history.average_score:.1f}")
        print(f"   - ê°œì„  ì¶”ì„¸: {history.improvement_trend:+.2f}")
        
        if audit_result.penalties:
            print(f"âš ï¸ ë²Œì  ì‚¬ìœ : {', '.join(audit_result.penalties)} ({audit_result.penalty_points:.0f}ì )")
            
        if audit_result.rewards:
            print(f"ğŸ† ë³´ìƒ ì‚¬ìœ : {', '.join(audit_result.rewards)} (+{audit_result.reward_points:.0f}ì )")
            
        if audit_result.improvement_suggestions:
            print(f"ğŸ’¡ ê°œì„  ë°©í–¥:")
            for suggestion in audit_result.improvement_suggestions:
                print(f"   - {suggestion}")
        
        print(f"=====================================\n")
    
    def get_agent_performance_summary(self, agent_name: str) -> Optional[Dict]:
        """ì—ì´ì „íŠ¸ ì„±ê³¼ ìš”ì•½"""
        if agent_name not in self.agent_histories:
            return None
            
        history = self.agent_histories[agent_name]
        
        return {
            "agent_name": agent_name,
            "total_tasks": history.total_tasks,
            "success_rate": (history.successful_tasks / history.total_tasks * 100) if history.total_tasks > 0 else 0,
            "average_score": history.average_score,
            "current_grade": AgentPerformanceGrade.EXCELLENT.value if history.average_score >= 95 else
                           AgentPerformanceGrade.GOOD.value if history.average_score >= 85 else
                           AgentPerformanceGrade.AVERAGE.value if history.average_score >= 70 else
                           AgentPerformanceGrade.POOR.value if history.average_score >= 50 else
                           AgentPerformanceGrade.FAIL.value,
            "improvement_trend": history.improvement_trend,
            "learning_rate": history.learning_rate,
            "penalty_multiplier": history.current_penalty_multiplier,
            "total_penalties": history.total_penalties,
            "total_rewards": history.total_rewards
        }


# ì „ì—­ ê°•í™”í•™ìŠµ ì‹œìŠ¤í…œ
rl_system = ReinforcementLearningSystem()


async def initialize_audit_system(openai_client: AsyncOpenAI = None):
    """ê°ì‚¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    await rl_system.initialize(openai_client)


async def audit_agent_performance(
    agent_name: str,
    task_query: str,
    agent_context: Any,
    execution_time: float,
    user_feedback: Optional[str] = None
) -> Tuple[AgentAuditResult, AgentLearningHistory]:
    """ì—ì´ì „íŠ¸ ì„±ê³¼ ê°ì‚¬ ë° í•™ìŠµ"""
    return await rl_system.evaluate_and_learn(
        agent_name, task_query, agent_context, execution_time, user_feedback
    )


def get_all_agents_summary() -> Dict[str, Dict]:
    """ëª¨ë“  ì—ì´ì „íŠ¸ ì„±ê³¼ ìš”ì•½"""
    summaries = {}
    for agent_name in rl_system.agent_histories.keys():
        summaries[agent_name] = rl_system.get_agent_performance_summary(agent_name)
    return summaries