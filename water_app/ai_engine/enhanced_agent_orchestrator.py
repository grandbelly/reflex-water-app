"""Enhanced Multi-Agent System with Clear R&R (Roles & Responsibilities)"""

import asyncio
import os
import time
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field

from openai import AsyncOpenAI
from water_app.queries.latest import latest_snapshot
from water_app.queries.qc import qc_rules
from ..ai_engine.audit_agent_system import initialize_audit_system, audit_agent_performance
from ..ai_engine.response_validator import ResponseValidator, generate_validated_response
from ..ai_engine.dynamic_rag_engine import DynamicRAGEngine
from ..ai_engine.five_w1h_agent import FiveW1HAgent
# Load environment variables
def load_env():
    """Simple environment loader"""
    import os
    from pathlib import Path
    
    # .env íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
    env_file = Path(__file__).parent.parent.parent / ".env"
    if env_file.exists():
        with open(env_file, 'r', encoding='utf-8') as f:
            for line in f:
                if '=' in line and not line.strip().startswith('#'):
                    key, value = line.strip().split('=', 1)
                    os.environ[key] = value


@dataclass
class AgentContext:
    """Enhanced Agent Context with specific data domains"""
    query: str
    
    # Data Domains (ê° ì—ì´ì „íŠ¸ë³„ ì „ë‹´ ì˜ì—­)
    raw_data: Dict[str, Any] = field(default_factory=dict)  # ResearchAgent ì „ë‹´
    insights: Dict[str, Any] = field(default_factory=dict)  # AnalysisAgent ì „ë‹´  
    quality_report: Dict[str, Any] = field(default_factory=dict)  # ReviewAgent ì „ë‹´
    
    # Shared Context
    final_response: str = ""
    confidence_score: float = 0.0


class BaseAgent:
    """Base Agent with enhanced logging and specialization"""
    
    def __init__(self, name: str, role: str, specialization: str):
        self.name = name
        self.role = role
        self.specialization = specialization  # ì „ë¬¸ ë¶„ì•¼
        self.rag_engine = None
        
    async def initialize(self, rag_engine=None):
        self.rag_engine = rag_engine
        print(f"ğŸ”§ {self.name} ì´ˆê¸°í™” ì™„ë£Œ - ì „ë¬¸ë¶„ì•¼: {self.specialization}")
        
    async def process(self, context: AgentContext) -> AgentContext:
        raise NotImplementedError("Each agent must implement process method")


class EnhancedResearchAgent(BaseAgent):
    """ğŸ•µï¸ Data Collection Specialist - ë°ì´í„° ìˆ˜ì§‘ ì „ë¬¸ê°€"""
    
    def __init__(self):
        super().__init__(
            "ResearchAgent", 
            "ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬",
            "ì„¼ì„œ ë°ì´í„°, QC ê·œì¹™, ì´ë ¥ ë°ì´í„° ìˆ˜ì§‘ ì „ë¬¸"
        )
        
    async def process(self, context: AgentContext) -> AgentContext:
        """Smart Data Collection Based on Query Analysis"""
        
        print(f"ğŸ§  {self.name} ì‹¤í–‰ ì¤‘... ì§ˆì˜ ë¶„ì„: '{context.query[:50]}...'")
        
        # 1. Query Classification (ì§ˆì˜ ìœ í˜• ë¶„ë¥˜)
        query_type = self._classify_query(context.query)
        print(f"ğŸ“‹ ì§ˆì˜ ìœ í˜• ë¶„ë¥˜: {query_type}")
        
        # 2. Targeted Data Collection (ìœ í˜•ë³„ ë§ì¶¤ ë°ì´í„° ìˆ˜ì§‘)
        if query_type == "correlation_analysis":
            await self._collect_correlation_data(context)
        elif query_type == "comparison_analysis":
            await self._collect_comparison_data(context)
        elif query_type == "current_status":
            await self._collect_current_data(context)
        elif query_type == "historical_trend":
            await self._collect_historical_data(context)
        elif query_type == "qc_violation":
            await self._collect_qc_focused_data(context)
        elif query_type == "system_overview":
            await self._collect_comprehensive_data(context)
        else:
            await self._collect_adaptive_data(context)
            
        # 3. Data Quality Assessment
        quality_score = self._assess_data_quality(context.raw_data)
        context.raw_data["quality_score"] = quality_score
        
        print(f"âœ… {self.name} ì™„ë£Œ - ìˆ˜ì§‘ëœ ë°ì´í„°: {list(context.raw_data.keys())}, í’ˆì§ˆì ìˆ˜: {quality_score:.2f}")
        return context
    
    def _classify_query(self, query: str) -> str:
        """Intelligent Query Classification"""
        query_lower = query.lower()
        
        # ìƒê´€ì„±/ìƒê´€ë¶„ì„ ì§ˆì˜
        if any(keyword in query_lower for keyword in ["ìƒê´€", "ê´€ê³„", "correlation", "ì—°ê´€"]):
            return "correlation_analysis"
        
        # í˜„ì¬ ìƒíƒœ ì§ˆì˜
        elif any(keyword in query_lower for keyword in ["í˜„ì¬", "ì§€ê¸ˆ", "ìƒíƒœëŠ”"]):
            return "current_status"
            
        # ë¹„êµ ë¶„ì„ ì§ˆì˜ (ì–´ì œì™€ ë¹„êµ)
        elif any(keyword in query_lower for keyword in ["ì–´ì œ", "ë¹„êµ", "ë³€í–ˆ", "ë³€í™”", "compare", "change"]):
            return "comparison_analysis"
        
        # ì´ë ¥/íŠ¸ë Œë“œ ì§ˆì˜  
        elif any(keyword in query_lower for keyword in ["ì¼ì£¼ì¼", "íŠ¸ë Œë“œ", "ì¶”ì„¸"]):
            return "historical_trend"
            
        # QC ìœ„ë°˜ ì§ˆì˜
        elif any(keyword in query_lower for keyword in ["ìœ„ë°˜", "ê²½ê³ ", "ì´ˆê³¼", "ì„ê³„"]):
            return "qc_violation"
            
        # ì‹œìŠ¤í…œ ì „ì²´ ê°œìš”
        elif any(keyword in query_lower for keyword in ["ì „ì²´", "ìš”ì•½", "ì¢…í•©", "ì „ë°˜ì "]):
            return "system_overview"
            
        return "adaptive"
    
    async def _collect_current_data(self, context: AgentContext):
        """í˜„ì¬ ìƒíƒœ ì¤‘ì‹¬ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            # ì—¬ëŸ¬ ì„¼ì„œ íƒœê·¸ ì¶”ì¶œ (ìƒê´€ë¶„ì„ ë“±ì„ ìœ„í•´)
            sensor_tags = self._extract_sensor_tags(context.query)
            
            if sensor_tags:
                # ì—¬ëŸ¬ ì„¼ì„œê°€ ìˆìœ¼ë©´ ëª¨ë‘ ìˆ˜ì§‘
                if len(sensor_tags) > 1:
                    context.raw_data["target_sensors"] = sensor_tags
                    all_sensor_data = []
                    for tag in sensor_tags:
                        data = await latest_snapshot(tag)
                        if data:
                            all_sensor_data.extend(data)
                    context.raw_data["focused_data"] = all_sensor_data
                    print(f"ğŸ” Enhanced Agent - ë‹¤ì¤‘ ì„¼ì„œ ë°ì´í„° ë¡œë“œ: {sensor_tags}")
                else:
                    # ë‹¨ì¼ ì„¼ì„œë§Œ ìˆìœ¼ë©´ ê¸°ì¡´ ë¡œì§
                    sensor_tag = sensor_tags[0]
                    sensor_data = await latest_snapshot(sensor_tag)
                    context.raw_data["target_sensor"] = sensor_tag
                    context.raw_data["focused_data"] = sensor_data
                    
                # QC ê·œì¹™ ìˆ˜ì§‘
                qc_data = []
                for tag in sensor_tags:
                    qc = await qc_rules(tag)
                    if qc:
                        qc_data.extend(qc)
                context.raw_data["relevant_qc"] = qc_data
            else:
                # ì „ì²´ ì„¼ì„œ í˜„ì¬ ìƒíƒœ
                sensor_data = await latest_snapshot(None)
                context.raw_data["all_sensors"] = sensor_data
                print(f"ğŸ” Enhanced Agent - ì„¼ì„œ ë°ì´í„° ë¡œë“œë¨: {len(sensor_data) if sensor_data else 0}ê°œ")
                
                # ì „ì²´ QC ê·œì¹™
                qc_data = await qc_rules(None)
                context.raw_data["relevant_qc"] = qc_data
            
        except Exception as e:
            context.raw_data["error"] = f"í˜„ì¬ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}"
    
    async def _collect_historical_data(self, context: AgentContext):
        """ì´ë ¥ ë°ì´í„° ì¤‘ì‹¬ ìˆ˜ì§‘"""
        try:
            from water_app.db import q
            
            # ì‹œê°„ ë²”ìœ„ ì¶”ì¶œ
            days = self._extract_time_range(context.query)
            sensor_tag = self._extract_sensor_tag(context.query)
            
            # ì´ë ¥ ë°ì´í„° ì¡°íšŒ
            start_date = datetime.now() - timedelta(days=days)
            hist_query = '''
            SELECT tag_name, COUNT(*) as count, 
                   MIN(value) as min_val, MAX(value) as max_val,
                   AVG(value) as avg_val, MIN(ts) as start_time, MAX(ts) as end_time
            FROM public.influx_hist 
            WHERE ts >= %s
            ''' + (f" AND tag_name = '{sensor_tag}'" if sensor_tag else "") + '''
            GROUP BY tag_name ORDER BY count DESC LIMIT 10
            '''
            
            result = await q(hist_query, (start_date,))
            context.raw_data["historical_summary"] = result
            context.raw_data["time_range_days"] = days
            context.raw_data["target_sensor"] = sensor_tag
            
        except Exception as e:
            context.raw_data["error"] = f"ì´ë ¥ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}"
    
    async def _collect_qc_focused_data(self, context: AgentContext):
        """QC ìœ„ë°˜ ì¤‘ì‹¬ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            # í˜„ì¬ ì„¼ì„œ ë°ì´í„°
            sensor_data = await latest_snapshot(None)
            qc_data = await qc_rules(None)
            
            # í˜„ì¬ ì„¼ì„œ ë°ì´í„° ì €ì¥ (response validatorì—ì„œ í•„ìš”)
            context.raw_data["current_sensors"] = sensor_data
            context.raw_data["relevant_qc"] = qc_data
            
            # QC ìœ„ë°˜ ì‚¬í•­ë§Œ í•„í„°ë§
            violations = []
            for sensor in sensor_data:
                for qc in qc_data:
                    if qc.get('tag_name') == sensor.get('tag_name'):
                        value = float(sensor.get('value', 0))
                        max_val = float(qc.get('max_val', float('inf')))
                        min_val = float(qc.get('min_val', float('-inf')))
                        warn_max = float(qc.get('warn_max', max_val))
                        warn_min = float(qc.get('warn_min', min_val))
                        crit_max = float(qc.get('crit_max', max_val))
                        crit_min = float(qc.get('crit_min', min_val))
                        
                        # ìœ„í—˜, ê²½ê³ , ìœ„ë°˜ íŒë‹¨
                        if value > crit_max or value < crit_min:
                            severity = 'critical'
                            threshold = crit_max if value > crit_max else crit_min
                        elif value > warn_max or value < warn_min:
                            severity = 'warning'
                            threshold = warn_max if value > warn_max else warn_min
                        elif value > max_val or value < min_val:
                            severity = 'violation'
                            threshold = max_val if value > max_val else min_val
                        else:
                            continue
                        
                        violations.append({
                            'sensor': sensor.get('tag_name'),
                            'current_value': value,
                            'threshold': threshold,
                            'violation_type': 'max' if value > threshold else 'min',
                            'severity': severity,
                            'severity_score': abs(value - threshold)
                        })
            
            context.raw_data["violations"] = violations
            context.raw_data["total_sensors"] = len(sensor_data)
            context.raw_data["violation_rate"] = len(violations) / len(sensor_data) * 100 if sensor_data else 0
            
        except Exception as e:
            context.raw_data["error"] = f"QC ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}"
    
    async def _collect_comprehensive_data(self, context: AgentContext):
        """ì¢…í•©ì  ì‹œìŠ¤í…œ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            # ëª¨ë“  ê¸°ë³¸ ë°ì´í„° ìˆ˜ì§‘
            await self._collect_current_data(context)
            
            # ì‹œìŠ¤í…œ í†µê³„ ì¶”ê°€
            from water_app.db import q
            
            stats_query = '''
            SELECT 
                COUNT(DISTINCT tag_name) as total_sensors,
                COUNT(*) as total_records,
                MIN(ts) as oldest_record,
                MAX(ts) as latest_record
            FROM public.influx_latest
            '''
            
            stats = await q(stats_query)
            context.raw_data["system_stats"] = stats[0] if stats else {}
            
        except Exception as e:
            context.raw_data["error"] = f"ì¢…í•© ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}"
    
    async def _collect_correlation_data(self, context: AgentContext):
        """ìƒê´€ë¶„ì„ìš© ë°ì´í„° ìˆ˜ì§‘"""
        try:
            # ì—¬ëŸ¬ ì„¼ì„œ íƒœê·¸ ì¶”ì¶œ
            sensor_tags = self._extract_sensor_tags(context.query)
            
            if len(sensor_tags) >= 2:
                print(f"ğŸ”— ìƒê´€ë¶„ì„ ë°ì´í„° ìˆ˜ì§‘: {sensor_tags}")
                context.raw_data["correlation_sensors"] = sensor_tags
                
                # ê° ì„¼ì„œì˜ í˜„ì¬ ë°ì´í„°
                all_sensor_data = []
                for tag in sensor_tags:
                    data = await latest_snapshot(tag)
                    if data:
                        all_sensor_data.extend(data)
                context.raw_data["sensor_data"] = all_sensor_data
                
                # 30ì¼ ì´ë ¥ ë°ì´í„° (ìƒê´€ë¶„ì„ìš©)
                from water_app.db import q
                from datetime import datetime, timedelta
                
                start_date = datetime.now() - timedelta(days=30)
                
                # psycopg3ì—ì„œ IN ì ˆ ì²˜ë¦¬
                placeholders = ', '.join(['%s'] * len(sensor_tags))
                hist_query = f'''
                SELECT tag_name, 
                       COUNT(*) as count,
                       MIN(value) as min_val, 
                       MAX(value) as max_val,
                       AVG(value) as avg_val,
                       STDDEV(value) as std_val
                FROM public.influx_hist 
                WHERE ts >= %s AND tag_name IN ({placeholders})
                GROUP BY tag_name
                '''
                
                params = [start_date] + sensor_tags
                result = await q(hist_query, tuple(params))
                context.raw_data["correlation_stats"] = result
                
                # ìƒ˜í”Œ ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜ì§‘ (ê° ì„¼ì„œë³„ ìµœê·¼ 10ê°œ)
                sample_points = []
                for tag in sensor_tags[:2]:  # ìµœëŒ€ 2ê°œ ì„¼ì„œë§Œ
                    sample_query = '''
                    SELECT tag_name, ts, value
                    FROM public.influx_hist
                    WHERE tag_name = %s AND ts >= %s
                    ORDER BY ts DESC
                    LIMIT 10
                    '''
                    sample_result = await q(sample_query, (tag, start_date))
                    if sample_result:
                        sample_points.extend(sample_result)
                
                context.raw_data["correlation_samples"] = sample_points
                if sample_points:
                    print(f"[DATA] Collected {len(sample_points)} sample data points for correlation")
                
                # QC ê·œì¹™
                qc_data = []
                for tag in sensor_tags:
                    qc = await qc_rules(tag)
                    if qc:
                        qc_data.extend(qc)
                context.raw_data["relevant_qc"] = qc_data
                
            else:
                # ì„¼ì„œê°€ ë¶€ì¡±í•˜ë©´ ì¼ë°˜ ë°ì´í„° ìˆ˜ì§‘
                await self._collect_current_data(context)
                
        except Exception as e:
            import traceback
            error_msg = f"ìƒê´€ë¶„ì„ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}\n{traceback.format_exc()}"
            print(f"[ERROR] {error_msg}")
            context.raw_data["error"] = error_msg
    
    async def _collect_comparison_data(self, context: AgentContext):
        """ë¹„êµ ë¶„ì„ìš© ë°ì´í„° ìˆ˜ì§‘ (ì–´ì œ vs ì˜¤ëŠ˜)"""
        try:
            from water_app.db import q
            from datetime import datetime, timedelta
            
            # ì‹œê°„ ë²”ìœ„ ì„¤ì •
            now = datetime.now()
            yesterday_start = now - timedelta(days=2)
            yesterday_end = now - timedelta(days=1)
            today_start = now - timedelta(days=1)
            
            # ë¹„êµ ì¿¼ë¦¬
            comparison_query = """
            WITH yesterday_data AS (
                SELECT 
                    tag_name,
                    AVG(value) as yesterday_avg,
                    MIN(value) as yesterday_min,
                    MAX(value) as yesterday_max,
                    COUNT(*) as yesterday_count
                FROM influx_hist
                WHERE ts >= %s AND ts < %s
                GROUP BY tag_name
            ),
            today_data AS (
                SELECT 
                    tag_name,
                    AVG(value) as today_avg,
                    MIN(value) as today_min,
                    MAX(value) as today_max,
                    COUNT(*) as today_count
                FROM influx_hist
                WHERE ts >= %s AND ts < %s
                GROUP BY tag_name
            )
            SELECT 
                COALESCE(y.tag_name, t.tag_name) as tag_name,
                y.yesterday_avg,
                t.today_avg,
                t.today_avg - y.yesterday_avg as avg_change,
                ABS((t.today_avg - y.yesterday_avg) / NULLIF(y.yesterday_avg, 0) * 100) as pct_change,
                y.yesterday_min,
                y.yesterday_max,
                t.today_min,
                t.today_max
            FROM yesterday_data y
            FULL OUTER JOIN today_data t ON y.tag_name = t.tag_name
            WHERE y.yesterday_avg IS NOT NULL AND t.today_avg IS NOT NULL
            ORDER BY ABS((t.today_avg - y.yesterday_avg) / NULLIF(y.yesterday_avg, 0)) DESC NULLS LAST
            LIMIT 10
            """
            
            result = await q(comparison_query, (yesterday_start, yesterday_end, today_start, now))
            context.raw_data["comparison_data"] = result
            
            # ê°€ì¥ í° ë³€í™” ìš”ì•½
            if result:
                top_changes = []
                for row in result[:5]:  # ìƒìœ„ 5ê°œ
                    change_info = {
                        'tag_name': row.get('tag_name'),
                        'yesterday_avg': row.get('yesterday_avg', 0),
                        'today_avg': row.get('today_avg', 0),
                        'change': row.get('avg_change', 0),
                        'pct_change': row.get('pct_change', 0)
                    }
                    top_changes.append(change_info)
                context.raw_data["top_changes"] = top_changes
                
                # ë³€í™”ìœ¨ì´ ê°€ì¥ í° ì„¼ì„œ
                if top_changes:
                    max_change = top_changes[0]
                    context.raw_data["max_change_sensor"] = max_change
            
            # í˜„ì¬ ìƒíƒœë„ í•¨ê»˜ ìˆ˜ì§‘
            from water_app.queries.latest import latest_snapshot
            sensor_data = await latest_snapshot(None)
            context.raw_data["current_sensors"] = sensor_data
            
            # QC ê·œì¹™
            from water_app.queries.qc import qc_rules
            qc_data = await qc_rules(None)
            context.raw_data["relevant_qc"] = qc_data
            
            print(f"[COMPARISON] Data collected: {len(result)} sensors compared")
            
        except Exception as e:
            import traceback
            error_msg = f"ë¹„êµ ë¶„ì„ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}\n{traceback.format_exc()}"
            print(f"[ERROR] {error_msg}")
            context.raw_data["error"] = error_msg
    
    async def _collect_adaptive_data(self, context: AgentContext):
        """ì ì‘í˜• ë°ì´í„° ìˆ˜ì§‘"""
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë™ì  ìˆ˜ì§‘
        await self._collect_current_data(context)
    
    def _extract_sensor_tag(self, query: str) -> Optional[str]:
        """ì„¼ì„œ íƒœê·¸ ì¶”ì¶œ (ë‹¨ì¼)"""
        import re
        pattern = r'D\d{3}'
        match = re.search(pattern, query.upper())
        return match.group() if match else None
    
    def _extract_sensor_tags(self, query: str) -> List[str]:
        """ì„¼ì„œ íƒœê·¸ ì¶”ì¶œ (ë³µìˆ˜)"""
        import re
        pattern = r'D\d{3}'
        matches = re.findall(pattern, query.upper())
        return matches if matches else []
    
    def _extract_time_range(self, query: str) -> int:
        """ì‹œê°„ ë²”ìœ„ ì¶”ì¶œ"""
        query_lower = query.lower()
        if 'ì¼ì£¼ì¼' in query_lower or '1ì£¼ì¼' in query_lower:
            return 7
        elif 'ì–´ì œ' in query_lower:
            return 1
        elif 'í•œë‹¬' in query_lower or '1ë‹¬' in query_lower:
            return 30
        return 7  # ê¸°ë³¸ê°’
    
    def _assess_data_quality(self, raw_data: Dict) -> float:
        """ë°ì´í„° í’ˆì§ˆ í‰ê°€"""
        score = 0.0
        
        # ë°ì´í„° ì¡´ì¬ ì—¬ë¶€
        if raw_data and not raw_data.get("error"):
            score += 0.5
            
        # ë°ì´í„° ì™„ì„±ë„
        if any(key in raw_data for key in ["all_sensors", "focused_data", "violations"]):
            score += 0.3
            
        # QC ê·œì¹™ ì—°ë™
        if "relevant_qc" in raw_data:
            score += 0.2
            
        return min(score, 1.0)


class EnhancedAnalysisAgent(BaseAgent):
    """ğŸ” Intelligence & Insight Generator - ì§€ëŠ¥í˜• ì¸ì‚¬ì´íŠ¸ ìƒì„±ê¸°"""
    
    def __init__(self):
        super().__init__(
            "AnalysisAgent",
            "ë°ì´í„° ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸ ë„ì¶œ", 
            "íŒ¨í„´ ë¶„ì„, ì´ìƒ íƒì§€, ì˜ˆì¸¡ì  ì¸ì‚¬ì´íŠ¸ ì „ë¬¸"
        )
        
    async def process(self, context: AgentContext) -> AgentContext:
        """Intelligent Analysis & Insight Generation"""
        
        print(f"ğŸ§  {self.name} ì‹¤í–‰ ì¤‘... ë¶„ì„ ëŒ€ìƒ: {list(context.raw_data.keys())}")
        
        # 1. Pattern Analysis (íŒ¨í„´ ë¶„ì„)
        await self._analyze_patterns(context)
        
        # 2. Anomaly Detection (ì´ìƒ íƒì§€)
        await self._detect_anomalies(context)
        
        # 3. Predictive Insights (ì˜ˆì¸¡ì  ì¸ì‚¬ì´íŠ¸)
        await self._generate_insights(context)
        
        # 4. Confidence Assessment (ì‹ ë¢°ë„ í‰ê°€)
        context.confidence_score = self._calculate_confidence(context.insights)
        
        print(f"âœ… {self.name} ì™„ë£Œ - ì¸ì‚¬ì´íŠ¸: {list(context.insights.keys())}, ì‹ ë¢°ë„: {context.confidence_score:.2f}")
        return context
        
    async def _analyze_patterns(self, context: AgentContext):
        """íŒ¨í„´ ë¶„ì„"""
        patterns = {}
        
        # ì„¼ì„œ ê°’ ë¶„í¬ íŒ¨í„´
        if "all_sensors" in context.raw_data:
            sensors = context.raw_data["all_sensors"]
            values = [s.get("value", 0) for s in sensors if s.get("value")]
            
            if values:
                patterns["value_distribution"] = {
                    "mean": sum(values) / len(values),
                    "range": max(values) - min(values), 
                    "outliers": len([v for v in values if abs(v - sum(values)/len(values)) > 2 * (max(values) - min(values))/len(values)])
                }
        
        # ìƒê´€ë¶„ì„ íŒ¨í„´ ì¶”ê°€
        if "correlation_sensors" in context.raw_data:
            correlation_sensors = context.raw_data["correlation_sensors"]
            if "correlation_stats" in context.raw_data:
                stats = context.raw_data["correlation_stats"]
                patterns["correlation"] = {
                    "sensors": correlation_sensors,
                    "stats": stats,
                    "type": "correlation_analysis"
                }
                
                # PandasAnalysisEngineì„ ì‚¬ìš©í•œ ìƒê´€ë¶„ì„ (30ì¼ ë°ì´í„°)
                from ..ai_engine.pandas_analysis_engine import PandasAnalysisEngine
                try:
                    engine = PandasAnalysisEngine()
                    result = await engine.analyze_sensor_data(
                        sensors=correlation_sensors,
                        analysis_type='correlation',
                        hours=720  # 30ì¼
                    )
                    if result.correlations:
                        patterns["correlation"]["coefficients"] = result.correlations
                        patterns["correlation"]["heatmap"] = result.heatmap_data
                except Exception as e:
                    patterns["correlation"]["error"] = str(e)
        
        context.insights["patterns"] = patterns
    
    async def _detect_anomalies(self, context: AgentContext):
        """ì´ìƒ ìƒí™© íƒì§€"""
        anomalies = []
        
        # QC ìœ„ë°˜ ê¸°ë°˜ ì´ìƒ íƒì§€
        if "violations" in context.raw_data:
            violations = context.raw_data["violations"]
            
            # ì‹¬ê°ë„ë³„ ë¶„ë¥˜ (ë¬¸ìì—´ ê¸°ë°˜)
            critical = [v for v in violations if v.get("severity") == "critical"]
            warning = [v for v in violations if v.get("severity") == "warning"]
            minor = [v for v in violations if v.get("severity") == "violation"]
            
            anomalies.append({
                "type": "qc_violations",
                "critical_count": len(critical),
                "warning_count": len(warning),
                "minor_count": len(minor),
                "most_severe": max(violations, key=lambda x: x.get("severity_score", 0)) if violations else None
            })
        
        context.insights["anomalies"] = anomalies
    
    async def _generate_insights(self, context: AgentContext):
        """ì˜ˆì¸¡ì  ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []
        
        # ì‹œìŠ¤í…œ ê±´ê°•ë„ í‰ê°€
        if "violation_rate" in context.raw_data:
            violation_rate = context.raw_data["violation_rate"]
            
            if violation_rate > 20:
                insights.append("âš ï¸ ë†’ì€ ìœ„ë°˜ìœ¨ ê°ì§€ - ì‹œìŠ¤í…œ ì ê²€ í•„ìš”")
            elif violation_rate > 10:
                insights.append("ğŸ” ê²½ê³„ ìˆ˜ì¤€ ìœ„ë°˜ìœ¨ - ëª¨ë‹ˆí„°ë§ ê°•í™” ê¶Œì¥") 
            else:
                insights.append("âœ… ì•ˆì •ì ì¸ ì‹œìŠ¤í…œ ìƒíƒœ ìœ ì§€ ì¤‘")
        
        # íŠ¹ì • ì„¼ì„œ ì¸ì‚¬ì´íŠ¸
        if "target_sensor" in context.raw_data:
            target = context.raw_data["target_sensor"]
            insights.append(f"ğŸ¯ {target} ì„¼ì„œ ì§‘ì¤‘ ë¶„ì„ ì™„ë£Œ")
        
        context.insights["predictions"] = insights
    
    def _calculate_confidence(self, insights: Dict) -> float:
        """ë¶„ì„ ì‹ ë¢°ë„ ê³„ì‚°"""
        score = 0.0
        
        if insights.get("patterns"):
            score += 0.3
        if insights.get("anomalies"): 
            score += 0.4
        if insights.get("predictions"):
            score += 0.3
            
        return score


class EnhancedReviewAgent(BaseAgent):
    """ğŸ” Quality Assurance & Validation Expert - í’ˆì§ˆ ë³´ì¦ ì „ë¬¸ê°€"""
    
    def __init__(self):
        super().__init__(
            "ReviewAgent",
            "í’ˆì§ˆ ê²€ì¦ ë° ìµœì¢… ìŠ¹ì¸",
            "ê²°ê³¼ ê²€ì¦, ì •í™•ì„± í™•ì¸, ê°œì„ ì•ˆ ì œì‹œ ì „ë¬¸"
        )
        
    async def process(self, context: AgentContext) -> AgentContext:
        """Comprehensive Quality Review & Validation"""
        
        print(f"ğŸ§  {self.name} ì‹¤í–‰ ì¤‘... ê²€í†  ëŒ€ìƒ: ë°ì´í„° í’ˆì§ˆ + ë¶„ì„ ì‹ ë¢°ë„")
        
        # 1. Data Validity Check (ë°ì´í„° ìœ íš¨ì„± ê²€ì¦)
        data_validation = self._validate_data_quality(context)
        
        # 2. Analysis Accuracy Review (ë¶„ì„ ì •í™•ì„± ê²€í† )
        analysis_validation = self._validate_analysis_quality(context) 
        
        # 3. Logic Consistency Check (ë…¼ë¦¬ ì¼ê´€ì„± ê²€ì¦)
        logic_validation = self._validate_logic_consistency(context)
        
        # 4. Improvement Recommendations (ê°œì„  ê¶Œê³ ì‚¬í•­)
        recommendations = self._generate_recommendations(context)
        
        # 5. Final Quality Report
        context.quality_report = {
            "data_validation": data_validation,
            "analysis_validation": analysis_validation, 
            "logic_validation": logic_validation,
            "recommendations": recommendations,
            "overall_quality": (data_validation["score"] + analysis_validation["score"] + logic_validation["score"]) / 3
        }
        
        print(f"âœ… {self.name} ì™„ë£Œ - ì „ì²´ í’ˆì§ˆì ìˆ˜: {context.quality_report['overall_quality']:.2f}")
        return context
    
    def _validate_data_quality(self, context: AgentContext) -> Dict:
        """ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
        score = 0.0
        issues = []
        
        # ë°ì´í„° ì¡´ì¬ì„± í™•ì¸
        if context.raw_data and not context.raw_data.get("error"):
            score += 0.5
        else:
            issues.append("ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜ ë°œìƒ")
            
        # ë°ì´í„° ì™„ì „ì„± í™•ì¸
        if context.raw_data.get("quality_score", 0) > 0.7:
            score += 0.3
        else:
            issues.append("ë°ì´í„° ì™„ì „ì„± ë¶€ì¡±")
            
        # QC ê·œì¹™ ì—°ë™ í™•ì¸
        if "relevant_qc" in context.raw_data:
            score += 0.2
        else:
            issues.append("QC ê·œì¹™ ì—°ë™ ë¶€ì¡±")
            
        return {"score": score, "issues": issues}
    
    def _validate_analysis_quality(self, context: AgentContext) -> Dict:
        """ë¶„ì„ í’ˆì§ˆ ê²€ì¦"""
        score = 0.0
        issues = []
        
        # ì¸ì‚¬ì´íŠ¸ ì¡´ì¬ì„± í™•ì¸
        if context.insights and len(context.insights) > 0:
            score += 0.4
        else:
            issues.append("ì˜ë¯¸ìˆëŠ” ì¸ì‚¬ì´íŠ¸ ë¶€ì¡±")
        
        # ì‹ ë¢°ë„ í™•ì¸
        if context.confidence_score > 0.7:
            score += 0.3
        else:
            issues.append("ë¶„ì„ ì‹ ë¢°ë„ ë‚®ìŒ")
            
        # íŒ¨í„´ ë¶„ì„ í™•ì¸
        if context.insights.get("patterns"):
            score += 0.3
        else:
            issues.append("íŒ¨í„´ ë¶„ì„ ë¶€ì¡±")
            
        return {"score": score, "issues": issues}
    
    def _validate_logic_consistency(self, context: AgentContext) -> Dict:
        """ë…¼ë¦¬ ì¼ê´€ì„± ê²€ì¦"""
        score = 0.8  # ê¸°ë³¸ì ìœ¼ë¡œ ë†’ì€ ì ìˆ˜ (ëª¨ìˆœ ë°œê²¬ì‹œ ì°¨ê°)
        issues = []
        
        # ë°ì´í„°-ê²°ë¡  ì¼ê´€ì„± í™•ì¸
        violations = context.raw_data.get("violations", [])
        predictions = context.insights.get("predictions", [])
        
        if violations and not any("ìœ„ë°˜" in pred for pred in predictions):
            score -= 0.3
            issues.append("ìœ„ë°˜ ì‚¬í•­ê³¼ ê²°ë¡  ë¶ˆì¼ì¹˜")
            
        return {"score": score, "issues": issues}
    
    def _generate_recommendations(self, context: AgentContext) -> List[str]:
        """ê°œì„  ê¶Œê³ ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ë°ì´í„° í’ˆì§ˆ ê°œì„ 
        if context.raw_data.get("quality_score", 1.0) < 0.5:
            recommendations.append("ğŸ”§ ë°ì´í„° ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤ ì ê²€ í•„ìš”")
            
        # ì‹¬ê°í•œ ìœ„ë°˜ ì‚¬í•­ ëŒ€ì‘
        violations = context.raw_data.get("violations", [])
        critical_violations = [v for v in violations if v.get("severity") == "critical"]
        
        if critical_violations:
            recommendations.append(f"ğŸš¨ {len(critical_violations)}ê°œ ì‹¬ê°í•œ ìœ„ë°˜ ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš”")
            
        # ëª¨ë‹ˆí„°ë§ ê°•í™”
        if context.raw_data.get("violation_rate", 0) > 10:
            recommendations.append("ğŸ“Š ëª¨ë‹ˆí„°ë§ ì£¼ê¸° ë‹¨ì¶• ê¶Œì¥")
            
        return recommendations


class EnhancedMultiAgentOrchestrator:
    """Enhanced Multi-Agent System with AuditAgent & Reinforcement Learning"""
    
    def __init__(self):
        self.research_agent = EnhancedResearchAgent()
        self.analysis_agent = EnhancedAnalysisAgent()
        self.review_agent = EnhancedReviewAgent()
        self.dynamic_rag = None  # Dynamic RAG Engine ì¶”ê°€
        self.five_w1h_agent = FiveW1HAgent()  # 5W1H Agent ì¶”ê°€
        self.openai_client = None
        self.audit_system_initialized = False
        
    async def initialize(self, rag_engine=None):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        print("ğŸš€ Enhanced Multi-Agent System ì´ˆê¸°í™” ì¤‘...")
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        load_env()
        api_key = os.getenv('OPENAI_API_KEY')
        if api_key:
            self.openai_client = AsyncOpenAI(api_key=api_key)
            print("âœ… Enhanced OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            
        # Dynamic RAG Engine ì´ˆê¸°í™”
        try:
            self.dynamic_rag = DynamicRAGEngine()
            await self.dynamic_rag.initialize()
            print("âœ… Dynamic RAG Engine ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ Dynamic RAG Engine ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.dynamic_rag = None

        # ê° ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        await self.research_agent.initialize(rag_engine)
        await self.analysis_agent.initialize(rag_engine)
        await self.review_agent.initialize(rag_engine)
        
        # AuditAgent ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        if not self.audit_system_initialized:
            await initialize_audit_system(self.openai_client)
            self.audit_system_initialized = True
            print("ğŸ” AuditAgent + ê°•í™”í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        
        print("ğŸ¯ Enhanced Multi-Agent System with Audit ì¤€ë¹„ ì™„ë£Œ")
    
    async def process_query(self, query: str) -> str:
        """Enhanced Multi-Agent Processing Pipeline"""
        
        print(f"\n{'='*60}")
        print(f"ğŸ§  Enhanced Multi-Agent Processing ì‹œì‘")
        print(f"ğŸ“ Query: {query}")
        print(f"{'='*60}")
        
        # ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
        context = AgentContext(query=query)
        
        try:
            total_start_time = time.time()
            
            # 1ë‹¨ê³„: ì „ë¬¸ ë°ì´í„° ìˆ˜ì§‘ + ê°ì‚¬
            print(f"\nğŸ” 1ë‹¨ê³„: ì „ë¬¸ ë°ì´í„° ìˆ˜ì§‘")
            start_time = time.time()
            context = await self.research_agent.process(context)
            execution_time = time.time() - start_time
            
            # ResearchAgent ê°ì‚¬
            if self.audit_system_initialized:
                audit_result, learning_history = await audit_agent_performance(
                    "ResearchAgent", query, context, execution_time
                )
            
            # 2ë‹¨ê³„: ì§€ëŠ¥í˜• ë¶„ì„ + ê°ì‚¬
            print(f"\nğŸ“Š 2ë‹¨ê³„: ì§€ëŠ¥í˜• ë¶„ì„")
            start_time = time.time()
            context = await self.analysis_agent.process(context)
            execution_time = time.time() - start_time
            
            # AnalysisAgent ê°ì‚¬
            if self.audit_system_initialized:
                audit_result, learning_history = await audit_agent_performance(
                    "AnalysisAgent", query, context, execution_time
                )
            
            # 3ë‹¨ê³„: í’ˆì§ˆ ê²€ì¦ + ê°ì‚¬
            print(f"\nğŸ” 3ë‹¨ê³„: í’ˆì§ˆ ê²€ì¦")
            start_time = time.time()
            context = await self.review_agent.process(context)
            execution_time = time.time() - start_time
            
            # ReviewAgent ê°ì‚¬
            if self.audit_system_initialized:
                audit_result, learning_history = await audit_agent_performance(
                    "ReviewAgent", query, context, execution_time
                )
            
            # 4ë‹¨ê³„: Dynamic RAGë¡œ ì‹¤ì‹œê°„ ë°ì´í„° ì¡°íšŒ
            print(f"\nğŸ”„ 4ë‹¨ê³„: Dynamic RAGë¡œ ì‹¤ì‹œê°„ ë°ì´í„° ì¡°íšŒ")
            if self.dynamic_rag:
                try:
                    rag_response = await self.dynamic_rag.process_natural_language_query(query)
                    context.raw_data["dynamic_rag_data"] = rag_response.get('data', [])
                    context.raw_data["dynamic_rag_sql"] = rag_response.get('sql', '')
                    context.raw_data["dynamic_rag_metadata"] = rag_response.get('metadata', {})
                    print(f"âœ… Dynamic RAG ì¡°íšŒ ì™„ë£Œ: {rag_response['metadata'].get('row_count', 0)}ê°œ ë°ì´í„°")
                except Exception as e:
                    print(f"âš ï¸ Dynamic RAG ì¡°íšŒ ì‹¤íŒ¨: {e}")

            # 5ë‹¨ê³„: 5W1H êµ¬ì¡°í™”
            print(f"\nğŸ“‹ 5ë‹¨ê³„: 5W1H ì›ì¹™ìœ¼ë¡œ êµ¬ì¡°í™”")
            result = await self.five_w1h_agent.process(context)

            # 6ë‹¨ê³„: ìµœì¢… ì‘ë‹µ ìƒì„± (í• ë£¨ì‹œë„¤ì´ì…˜ ê²€ì¦ ì ìš©)
            print(f"\nğŸ¤– 6ë‹¨ê³„: ìµœì¢… ì‘ë‹µ ìƒì„± (ê²€ì¦ ì ìš©)")
            
            # ì»¨í…ìŠ¤íŠ¸ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            context_dict = {
                'current_data': context.raw_data.get('focused_data', context.raw_data.get('all_sensors', [])),
                'current_sensors': context.raw_data.get('current_sensors', []),
                'qc_rules': context.raw_data.get('relevant_qc', []),
                'query': query,
                'correlation_data': context.insights.get('patterns', {}).get('correlation', {}),
                'correlation_sensors': context.raw_data.get('correlation_sensors', []),
                'correlation_stats': context.raw_data.get('correlation_stats', []),
                'correlation_samples': context.raw_data.get('correlation_samples', []),
                'comparison_data': context.raw_data.get('comparison_data', []),
                'top_changes': context.raw_data.get('top_changes', []),
                'max_change_sensor': context.raw_data.get('max_change_sensor', {})
            }
            
            # ê²€ì¦ëœ ì‘ë‹µ ìƒì„± ì‹œë„
            try:
                validated_response = await generate_validated_response(query, context_dict)
                print("âœ… í• ë£¨ì‹œë„¤ì´ì…˜ ê²€ì¦ í†µê³¼")
                
                # ì‹œê°í™” ë°ì´í„°ì™€ í•¨ê»˜ ë°˜í™˜
                if hasattr(context, 'visualizations') and context.visualizations:
                    return {
                        'text': validated_response,
                        'visualizations': context.visualizations
                    }
                return validated_response
            except Exception as e:
                print(f"âš ï¸ ê²€ì¦ ì‹¤íŒ¨, ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©: {e}")
                final_response = await self._generate_enhanced_response(query, context)
            
            total_execution_time = time.time() - total_start_time
            
            print(f"\nâœ… Enhanced Multi-Agent Processing ì™„ë£Œ")
            print(f"ğŸ“‹ í’ˆì§ˆì ìˆ˜: {context.quality_report.get('overall_quality', 0):.2f}")
            print(f"ğŸ¯ ì‹ ë¢°ë„: {context.confidence_score:.2f}")
            print(f"â±ï¸ ì´ ì‹¤í–‰ì‹œê°„: {total_execution_time:.2f}ì´ˆ")
            print(f"{'='*60}\n")
            
            # final_responseê°€ ë”•ì…”ë„ˆë¦¬ì´ë©´ ê·¸ëŒ€ë¡œ ë¦¬í„´ (ì‹œê°í™” ë°ì´í„° í¬í•¨)
            return final_response
            
        except Exception as e:
            error_msg = f"âŒ Enhanced Multi-Agent ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}"
            print(error_msg)
            return error_msg
    
    async def _generate_enhanced_response(self, query: str, context: AgentContext) -> str:
        """Dynamic RAG + 5W1H ê¸°ë°˜ ìµœì¢… ì‘ë‹µ ìƒì„±"""

        # 5W1H êµ¬ì¡°í™”ëœ ì‘ë‹µì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        if hasattr(context, 'five_w1h'):
            return self._format_5w1h_response(context)

        
        if not self.openai_client:
            return self._generate_fallback_response(context)
            
        try:
            # ì „ë¬¸í™”ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ - í™˜ê° ë°©ì§€ ê°•í™”
            system_prompt = """ë‹¹ì‹ ì€ ì‚°ì—…ìš© ì„¼ì„œ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì˜ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì¤‘ìš” ê·œì¹™ (ì ˆëŒ€ ì¤€ìˆ˜):
1. Multi-Agentê°€ ì œê³µí•œ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
2. ì œê³µë˜ì§€ ì•Šì€ ìˆ˜ì¹˜, ì„ê³„ê°’, í’ˆì§ˆì ìˆ˜ë¥¼ ì ˆëŒ€ ì°½ì‘í•˜ì§€ ë§ˆì„¸ìš”
3. ë°ì´í„°ê°€ ì—†ìœ¼ë©´ "ë°ì´í„° ì—†ìŒ"ì´ë¼ê³  ëª…í™•íˆ í‘œì‹œí•˜ì„¸ìš”
4. ëª¨ë“  ìˆ«ìëŠ” ì œê³µëœ ê°’ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš” (ì¶”ì • ê¸ˆì§€)

ì‘ë‹µ ì›ì¹™:
1. ì‚¬ìš©ì ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µë³€
2. ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ìƒíƒœ ì œì‹œ
3. ë°œê²¬ëœ ì´ìƒ ì‚¬í•­ì´ë‚˜ íŒ¨í„´ì„ ëª…í™•íˆ ì„¤ëª…
4. ì‹¤í–‰ ê°€ëŠ¥í•œ ê¶Œì¥ì‚¬í•­ ì œê³µ
5. í•œêµ­ì–´ë¡œ ì „ë¬¸ì ì´ë©´ì„œë„ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…

ë¶„ì„ ê²°ê³¼ì˜ ì‹ ë¢°ë„ì™€ í’ˆì§ˆ ì ìˆ˜ë¥¼ íˆ¬ëª…í•˜ê²Œ ê³µê°œí•˜ì„¸ìš”."""

            # ì»¨í…ìŠ¤íŠ¸ ì¡°í•©
            context_summary = self._build_context_summary(context)
            
            user_prompt = f"""ì‚¬ìš©ì ì§ˆë¬¸: {query}

Multi-Agent ë¶„ì„ ê²°ê³¼:
{context_summary}

ìœ„ ì „ë¬¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."""

            # OpenAI API í˜¸ì¶œ
            response = await self.openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.2,  # í™˜ê° ë°©ì§€ë¥¼ ìœ„í•´ 0.7 â†’ 0.2ë¡œ ë‚®ì¶¤
                max_tokens=1000
            )
            
            ai_response = response.choices[0].message.content.strip()
            
            # ì‹œê°í™” ë°ì´í„° ì¶”ê°€
            from ..ai_engine.visualization_generator import generate_visualization_data, format_visualization_response
            
            sensor_data = context.raw_data.get("all_sensors", [])
            qc_data = context.raw_data.get("relevant_qc", [])
            print(f"ğŸ¨ ì‹œê°í™” ìƒì„±ê¸°ì— ì „ë‹¬ë˜ëŠ” ë°ì´í„°: ì„¼ì„œ {len(sensor_data)}ê°œ, QC {len(qc_data)}ê°œ")
            
            viz_data = await generate_visualization_data(
                query,
                sensor_data,
                qc_data,
                context.raw_data.get("historical_summary")
            )
            
            if viz_data:
                return format_visualization_response(ai_response, viz_data)
            
            return ai_response
            
        except Exception as e:
            print(f"âŒ OpenAI ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return self._generate_fallback_response(context)
    
    def _build_context_summary(self, context: AgentContext) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ìš”ì•½ ìƒì„±"""
        summary_parts = []
        
        # ë°ì´í„° ìˆ˜ì§‘ ê²°ê³¼
        summary_parts.append("ğŸ” **ë°ì´í„° ìˆ˜ì§‘ ê²°ê³¼**:")
        if context.raw_data:
            for key, value in context.raw_data.items():
                if key != "error":
                    summary_parts.append(f"- {key}: {str(value)[:100]}...")
        
        # ë¶„ì„ ì¸ì‚¬ì´íŠ¸
        summary_parts.append("\nğŸ“Š **ë¶„ì„ ì¸ì‚¬ì´íŠ¸**:")
        if context.insights:
            for key, value in context.insights.items():
                summary_parts.append(f"- {key}: {str(value)[:100]}...")
        
        # í’ˆì§ˆ ë³´ê³ ì„œ
        summary_parts.append(f"\nğŸ” **í’ˆì§ˆ ê²€ì¦**: {context.quality_report.get('overall_quality', 0):.2f}/1.0")
        if context.quality_report.get("recommendations"):
            summary_parts.append("ê¶Œê³ ì‚¬í•­:")
            for rec in context.quality_report["recommendations"]:
                summary_parts.append(f"- {rec}")
        
        return "\n".join(summary_parts)
    
    def _format_5w1h_response(self, context: AgentContext) -> str:
        """5W1H ì›ì¹™ì— ë”°ë¥¸ ì‘ë‹µ í¬ë§·íŒ…"""
        w1h = context.five_w1h

        # Dynamic RAG ë°ì´í„°ì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
        timestamp_info = ""
        if "dynamic_rag_data" in context.raw_data:
            rag_data = context.raw_data["dynamic_rag_data"]
            if rag_data and len(rag_data) > 0:
                # ì²« ë²ˆì§¸ ë°ì´í„°ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ ì‚¬ìš©
                first_data = rag_data[0]
                if 'timestamp' in first_data:
                    timestamp_info = f" [{first_data['timestamp']}]"
                elif 'latest_bucket' in first_data:
                    timestamp_info = f" [{first_data['latest_bucket']}]"

        response = f"""## ğŸ¯ 6í•˜ì›ì¹™ ê¸°ë°˜ ì •í™•í•œ ë¶„ì„ ê²°ê³¼{timestamp_info}

### ğŸ“ WHO (ëˆ„ê°€/ë¬´ì—‡ì´)
{w1h.who}

### ğŸ“Œ WHAT (ë¬´ì—‡ì„)
{w1h.what}

### ğŸ—ºï¸ WHERE (ì–´ë””ì„œ)
{w1h.where}

### â° WHEN (ì–¸ì œ)
{w1h.when}

### ğŸ’¡ WHY (ì™œ)
{w1h.why}

### ğŸ”§ HOW (ì–´ë–»ê²Œ)
{w1h.how}

---
ğŸ“Š **ë°ì´í„° ì‹ ë¢°ë„**: Dynamic RAG Engine ì‹¤ì‹œê°„ ì¡°íšŒ
ğŸ” **ë¶„ì„ ì‹ ë¢°ë„**: {context.confidence_score:.2f}
ğŸ“‹ **í’ˆì§ˆ ì ìˆ˜**: {context.quality_report.get('overall_quality', 0):.2f}"""

        return response

    def _generate_fallback_response(self, context: AgentContext) -> str:
        """Fallback ì‘ë‹µ ìƒì„±"""
        # 5W1H êµ¬ì¡°í™”ëœ ì‘ë‹µì´ ìˆìœ¼ë©´ ì‚¬ìš©
        if hasattr(context, 'five_w1h'):
            return self._format_5w1h_response(context)

        return f"""ğŸ¤– **Enhanced Multi-Agent ë¶„ì„ ê²°ê³¼**

ğŸ“Š ë°ì´í„° ìˆ˜ì§‘: {len(context.raw_data)} ê°œ ì˜ì—­
ğŸ” ë¶„ì„ ì‹ ë¢°ë„: {context.confidence_score:.2f}
ğŸ“‹ í’ˆì§ˆ ì ìˆ˜: {context.quality_report.get('overall_quality', 0):.2f}

{self._build_context_summary(context)}"""


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ 
enhanced_orchestrator = EnhancedMultiAgentOrchestrator()


async def initialize_enhanced_multi_agent_system(rag_engine=None):
    """Enhanced Multi-Agent ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    await enhanced_orchestrator.initialize(rag_engine)


async def get_enhanced_multi_agent_response(query: str) -> str:
    """Enhanced Multi-Agent ì‘ë‹µ ìƒì„±"""
    return await enhanced_orchestrator.process_query(query)