"""
Response Validator - í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ë¥¼ ìœ„í•œ ì‘ë‹µ ê²€ì¦ ì‹œìŠ¤í…œ
"""

import json
import re
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass, asdict


@dataclass
class SensorData:
    """ì„¼ì„œ ë°ì´í„° êµ¬ì¡°í™”"""
    tag_name: str
    value: float
    status: str  # normal, warning, critical
    timestamp: Optional[str] = None
    unit: Optional[str] = None
    qc_min: Optional[float] = None
    qc_max: Optional[float] = None
    confidence: float = 1.0  # 0.0 ~ 1.0


@dataclass
class AnalysisResponse:
    """êµ¬ì¡°í™”ëœ ë¶„ì„ ì‘ë‹µ"""
    query_type: str  # status, alert, trend, summary
    sensor_data: List[SensorData]
    summary: str
    alerts: List[str]
    recommendations: List[str]
    confidence_score: float
    data_source: str  # database, cache, prediction
    timestamp: str
    has_complete_data: bool
    missing_data: List[str]


class ResponseValidator:
    """ì‘ë‹µ ê²€ì¦ ë° í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€"""
    
    def __init__(self):
        self.known_sensors = set()
        self.known_ranges = {}
        self.db_connection = None
        self.confidence_threshold = 0.7
        
        # ì„¼ì„œ íƒ€ì…ê³¼ ë‹¨ìœ„ ë§¤í•‘ (hallucination_prevention.pyì™€ ë™ê¸°í™”)
        self.sensor_type_units = {
            'D100': {'type': 'temperature', 'unit': 'Â°C', 'name': 'ì˜¨ë„'},
            'D101': {'type': 'pressure', 'unit': 'bar', 'name': 'ì••ë ¥'},
            'D102': {'type': 'flow', 'unit': 'L/min', 'name': 'ìœ ëŸ‰'},
            'D200': {'type': 'vibration', 'unit': 'mm/s', 'name': 'ì§„ë™'},
            'D300': {'type': 'power', 'unit': '%', 'name': 'ì „ë ¥'}
        }
        
    def validate_sensor_value(self, tag_name: str, value: Any, context: Dict) -> Tuple[bool, str]:
        """ì„¼ì„œ ê°’ ê²€ì¦"""
        
        # 1. ê°’ì´ ìˆ«ìì¸ì§€ í™•ì¸
        try:
            float_value = float(value)
        except (TypeError, ValueError):
            return False, f"Invalid value type for {tag_name}: {value}"
        
        # 2. ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ì‹¤ì œ ë°ì´í„°ì™€ ë¹„êµ
        actual_data = context.get('current_data', [])
        for sensor in actual_data:
            if sensor.get('tag_name') == tag_name:
                actual_value = sensor.get('value')
                if actual_value is not None:
                    # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸ (ë¶€ë™ì†Œìˆ˜ì  ì˜¤ì°¨ í—ˆìš©)
                    if abs(float(actual_value) - float_value) > 0.01:
                        return False, f"Value mismatch for {tag_name}: reported {value} vs actual {actual_value}"
                break
        
        # 3. QC ë²”ìœ„ í™•ì¸
        qc_rules = context.get('qc_rules', [])
        for rule in qc_rules:
            if rule.get('tag_name') == tag_name:
                min_val = rule.get('min_val')
                max_val = rule.get('max_val')
                if min_val is not None and float_value < float(min_val):
                    return True, f"Value {value} below minimum {min_val}"
                if max_val is not None and float_value > float(max_val):
                    return True, f"Value {value} above maximum {max_val}"
                break
                
        return True, "Valid"
    
    def extract_numbers_from_text(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ì ì¶”ì¶œ"""
        # ìˆ«ì íŒ¨í„´ ì°¾ê¸° (ì •ìˆ˜, ì†Œìˆ˜ì )
        pattern = r'-?\d+\.?\d*'
        numbers = re.findall(pattern, text)
        return [float(n) for n in numbers if n]
    
    def validate_with_database(self, response: str, db_data: List[Dict]) -> float:
        """DB ë°ì´í„°ì™€ ì‹¤ì‹œê°„ ê²€ì¦"""
        if not db_data:
            return 0.5
        
        mentioned_values = self.extract_numbers_from_text(response)
        actual_values = [float(d['value']) for d in db_data if 'value' in d]
        
        if not mentioned_values:
            return 1.0
        
        match_count = 0
        for val in mentioned_values:
            for actual in actual_values:
                if abs(val - actual) < 0.01:
                    match_count += 1
                    break
        
        return match_count / len(mentioned_values) if mentioned_values else 1.0
    
    def detect_hallucination(self, response: str, context: Dict) -> Dict[str, Any]:
        """í• ë£¨ì‹œë„¤ì´ì…˜ íƒì§€ - ê°œì„ ëœ ë²„ì „"""
        
        issues = []
        confidence = 1.0
        
        # 1. ì‘ë‹µì—ì„œ ìˆ«ì ì¶”ì¶œ
        numbers_in_response = self.extract_numbers_from_text(response)
        
        # 2. ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ì‹¤ì œ ìˆ«ì
        actual_numbers = set()
        for data in context.get('current_data', []):
            if 'value' in data:
                actual_numbers.add(float(data['value']))
        for rule in context.get('qc_rules', []):
            for key in ['min_val', 'max_val', 'warn_min', 'warn_max', 'crit_min', 'crit_max']:
                if rule.get(key) is not None:
                    actual_numbers.add(float(rule[key]))
        # ë¹„êµ ë°ì´í„°ì˜ ìˆ«ìë“¤ë„ ì¶”ê°€
        for comp in context.get('comparison_data', []):
            for key in ['yesterday_avg', 'today_avg', 'avg_change', 'pct_change', 'yesterday_min', 'yesterday_max', 'today_min', 'today_max']:
                if comp.get(key) is not None:
                    actual_numbers.add(float(comp[key]))
        # ìƒê´€ë¶„ì„ ë°ì´í„°ì˜ ìˆ«ìë“¤ë„ ì¶”ê°€  
        for stat in context.get('correlation_stats', []):
            for key in ['avg_val', 'mean_val', 'std_val', 'count']:
                if stat.get(key) is not None:
                    actual_numbers.add(float(stat[key]))
        
        # 3. í• ë£¨ì‹œë„¤ì´ì…˜ ì²´í¬ - ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
        for num in numbers_in_response:
            found = False
            for actual in actual_numbers:
                if abs(num - actual) < 0.01:
                    found = True
                    break
            
            if not found and num > 10000:
                issues.append(f"Suspicious large number {num} not found in context")
                confidence *= 0.8
            elif not found and 100 < num < 10000:
                # ì¤‘ê°„ í¬ê¸° ìˆ«ìëŠ” ê²½ê³ ë§Œ
                confidence *= 0.95
        
        # 4. ê¸ˆì§€ëœ íŒ¨í„´ ì²´í¬ (ì œê±° - ë„ˆë¬´ ì—„ê²©í•¨)
        # ì´ ë¶€ë¶„ì„ ì œê±°í•˜ì—¬ ì •ìƒ ë²”ìœ„, í’ˆì§ˆ ì ìˆ˜ ë“±ì„ í—ˆìš©
        
        return {
            'has_hallucination': len(issues) > 0,
            'issues': issues,
            'confidence': confidence,
            'numbers_found': list(numbers_in_response),
            'numbers_expected': list(actual_numbers)
        }
    
    def create_structured_response(self, query: str, context: Dict) -> AnalysisResponse:
        """êµ¬ì¡°í™”ëœ ì‘ë‹µ ìƒì„± (í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€)"""
        
        # ì¿¼ë¦¬ íƒ€ì… ë¶„ë¥˜
        query_lower = query.lower()
        if 'ìƒê´€' in query_lower or 'correlation' in query_lower or 'ê´€ê³„' in query_lower:
            query_type = 'correlation'
        elif 'ì–´ì œ' in query_lower or 'ë¹„êµ' in query_lower or 'ë³€í™”' in query_lower or 'ë³€í–ˆ' in query_lower:
            query_type = 'comparison'
        elif 'ê²½ê³ ' in query_lower or 'ì•ŒëŒ' in query_lower or 'alert' in query_lower or 'ì£¼ì˜' in query_lower or 'ìœ„í—˜' in query_lower or 'ì•Œë¦¼' in query_lower:
            query_type = 'alert'
        elif 'ìƒíƒœ' in query_lower or 'status' in query_lower:
            query_type = 'status'
        elif 'íŠ¸ë Œë“œ' in query_lower:
            query_type = 'trend'
        else:
            query_type = 'summary'
        
        # ì„¼ì„œ ë°ì´í„° ìˆ˜ì§‘ (ì‹¤ì œ ë°ì´í„°ë§Œ)
        sensor_data = []
        missing_data = []
        
        # ìƒê´€ë¶„ì„ì˜ ê²½ìš° correlation_statsì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        if query_type == 'correlation' and 'correlation_stats' in context:
            current_data = []
            for stat in context.get('correlation_stats', []):
                current_data.append({
                    'tag_name': stat.get('tag_name'),
                    'value': stat.get('avg_val', stat.get('mean_val', 0)),
                    'ts': ''
                })
        # ë¹„êµ ë¶„ì„ì˜ ê²½ìš° comparison_dataì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        elif query_type == 'comparison' and 'comparison_data' in context:
            current_data = []
            comp_data = context.get('comparison_data', [])
            for comp in comp_data:
                current_data.append({
                    'tag_name': comp.get('tag_name'),
                    'value': comp.get('today_avg', 0),
                    'ts': ''
                })
        else:
            current_data = context.get('current_data', [])
            if not current_data:
                current_data = context.get('current_sensors', [])
            if not current_data:
                current_data = context.get('focused_data', [])
        
        qc_rules = context.get('qc_rules', [])
        qc_lookup = {r['tag_name']: r for r in qc_rules if 'tag_name' in r}
        
        for data in current_data:
            tag_name = data.get('tag_name')
            value = data.get('value')
            
            if tag_name and value is not None:
                # QC ê·œì¹™ í™•ì¸
                qc_rule = qc_lookup.get(tag_name, {})
                
                # ìƒíƒœ íŒì •
                status = 'normal'
                if qc_rule:
                    warn_min = qc_rule.get('warn_min')
                    warn_max = qc_rule.get('warn_max')
                    crit_min = qc_rule.get('crit_min')
                    crit_max = qc_rule.get('crit_max')
                    
                    float_value = float(value)
                    if (crit_min and float_value < float(crit_min)) or \
                       (crit_max and float_value > float(crit_max)):
                        status = 'critical'
                    elif (warn_min and float_value < float(warn_min)) or \
                         (warn_max and float_value > float(warn_max)):
                        status = 'warning'
                
                sensor_data.append(SensorData(
                    tag_name=tag_name,
                    value=float(value),
                    status=status,
                    timestamp=data.get('ts', ''),
                    unit=self._infer_unit(tag_name),
                    qc_min=float(qc_rule.get('min_val', 0)) if qc_rule else None,
                    qc_max=float(qc_rule.get('max_val', 0)) if qc_rule else None,
                    confidence=1.0  # ì‹¤ì œ ë°ì´í„°ëŠ” 100% ì‹ ë¢°
                ))
            else:
                missing_data.append(tag_name or 'Unknown')
        
        # ìƒê´€/ë¹„êµ ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬
        correlation_info = {}
        if query_type == 'comparison':
            # ë¹„êµ ë¶„ì„ ë°ì´í„° ì¶”ê°€
            if 'comparison_data' in context:
                correlation_info['comparison_data'] = context.get('comparison_data', [])
            if 'top_changes' in context:
                correlation_info['top_changes'] = context.get('top_changes', [])
        elif query_type == 'correlation':
            # correlation_dataê°€ ìˆìœ¼ë©´ ê°€ì ¸ì˜¤ê¸°
            if 'correlation_data' in context:
                corr_data = context.get('correlation_data', {})
                if 'coefficients' in corr_data:
                    correlation_info.update(corr_data)
            
            # correlation_sensors ì¶”ê°€
            if 'correlation_sensors' in context:
                correlation_info['sensors'] = context.get('correlation_sensors', [])
            
            # correlation_stats ì¶”ê°€
            if 'correlation_stats' in context:
                correlation_info['stats'] = context.get('correlation_stats', [])
            
            # ìƒ˜í”Œ ë°ì´í„° ì¶”ê°€
            if 'correlation_samples' in context:
                correlation_info['samples'] = context.get('correlation_samples', [])
        
        # ìƒê´€ì •ë³´ê°€ ë¹„ì–´ìˆì§€ ì•Šì„ ë•Œë§Œ ì‚¬ìš©
        if query_type == 'correlation' and not correlation_info:
            correlation_info = None
        
        # ìš”ì•½ ìƒì„± (ë°ì´í„° ê¸°ë°˜)
        summary = self._generate_safe_summary(query_type, sensor_data, missing_data, correlation_info)
        
        # ì•Œë¦¼ ìƒì„± (ì‹¤ì œ ë°ì´í„°ë§Œ)
        alerts = []
        for sensor in sensor_data:
            if sensor.status == 'critical':
                alerts.append(f"ğŸš¨ {sensor.tag_name}: {sensor.value} (ìœ„í—˜)")
            elif sensor.status == 'warning':
                alerts.append(f"âš ï¸ {sensor.tag_name}: {sensor.value} (ì£¼ì˜)")
        
        # ê¶Œì¥ì‚¬í•­ (í…œí”Œë¦¿ ê¸°ë°˜)
        recommendations = self._generate_recommendations(sensor_data, query_type)
        
        return AnalysisResponse(
            query_type=query_type,
            sensor_data=sensor_data,
            summary=summary,
            alerts=alerts,
            recommendations=recommendations,
            confidence_score=1.0 if sensor_data else 0.0,
            data_source='database' if sensor_data else 'none',
            timestamp=datetime.now().isoformat(),
            has_complete_data=len(missing_data) == 0,
            missing_data=missing_data
        )
    
    def _infer_unit(self, tag_name: str) -> str:
        """ì„¼ì„œëª…ì—ì„œ ë‹¨ìœ„ ì¶”ë¡  (ì •í™•í•œ ë§¤í•‘ ì‚¬ìš©)"""
        # ì •í™•í•œ ì„¼ì„œ ë§¤í•‘ ìš°ì„ 
        if tag_name in self.sensor_type_units:
            return self.sensor_type_units[tag_name]['unit']
        
        # ì‹œë¦¬ì¦ˆë³„ ë§¤í•‘ (í´ë°±)
        if tag_name.startswith('D100'):
            return 'Â°C'
        elif tag_name.startswith('D101'):
            return 'bar'
        elif tag_name.startswith('D102'):
            return 'L/min'
        elif tag_name.startswith('D200'):
            return 'mm/s'
        elif tag_name.startswith('D300'):
            return '%'
        elif tag_name.startswith('D1'):
            return 'Â°C'
        elif tag_name.startswith('D2'):
            return 'bar'
        elif tag_name.startswith('D3'):
            return 'rpm'
        return ''
    
    def _generate_safe_summary(self, query_type: str, sensor_data: List[SensorData], missing: List[str], correlation_info: Dict = None) -> str:
        """ì•ˆì „í•œ ìš”ì•½ ìƒì„± (í…œí”Œë¦¿ ê¸°ë°˜)"""
        
        if not sensor_data:
            return "í˜„ì¬ ì„¼ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
        
        total = len(sensor_data)
        normal = len([s for s in sensor_data if s.status == 'normal'])
        warning = len([s for s in sensor_data if s.status == 'warning'])
        critical = len([s for s in sensor_data if s.status == 'critical'])
        
        if query_type == 'comparison':
            # ë¹„êµ ë¶„ì„ ê²°ê³¼
            if 'comparison_data' in correlation_info and correlation_info.get('comparison_data'):
                comp_data = correlation_info.get('comparison_data', [])
                if comp_data:
                    # ê°€ì¥ í° ë³€í™” ì„¼ì„œ
                    top_sensor = comp_data[0]
                    tag = top_sensor.get('tag_name', '')
                    pct = top_sensor.get('pct_change', 0)
                    change = top_sensor.get('avg_change', 0)
                    
                    if pct == 0:
                        return "ëª¨ë“  ì„¼ì„œê°€ ì–´ì œì™€ ë™ì¼í•œ ê°’ì„ ìœ ì§€í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì„¼ì„œ ì‘ë™ ìƒíƒœë¥¼ ì ê²€í•´ì£¼ì„¸ìš”."
                    else:
                        direction = "ì¦ê°€" if change > 0 else "ê°ì†Œ"
                        return f"{tag} ì„¼ì„œê°€ ì–´ì œ ëŒ€ë¹„ {pct:.1f}% {direction}í–ˆìŠµë‹ˆë‹¤."
            return "ì–´ì œì™€ ì˜¤ëŠ˜ì˜ ì„¼ì„œ ë°ì´í„°ë¥¼ ë¹„êµ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤."
        elif query_type == 'correlation':
            if correlation_info and 'coefficients' in correlation_info:
                coeffs = correlation_info['coefficients']
                sensors = correlation_info.get('sensors', [])
                stats = correlation_info.get('stats', [])
                
                if len(sensors) >= 2:
                    # ìƒê´€ê³„ìˆ˜ì™€ ë°ì´í„° í†µê³„ í‘œì‹œ
                    summary_parts = []
                    if coeffs:
                        for key, value in coeffs.items():
                            summary_parts.append(f"{sensors[0]}ì™€ {sensors[1]}ì˜ ìƒê´€ê³„ìˆ˜: {value:.4f}")
                            break
                    else:
                        summary_parts.append(f"{sensors[0]}ì™€ {sensors[1]}ì˜ ìƒê´€ë¶„ì„")
                    
                    # ë°ì´í„° í†µê³„ ì¶”ê°€
                    if stats:
                        summary_parts.append("\në¶„ì„ ë°ì´í„°:")
                        for stat in stats[:2]:  # ìµœëŒ€ 2ê°œ ì„¼ì„œë§Œ í‘œì‹œ
                            tag = stat.get('tag_name', '')
                            count = stat.get('count', 0)
                            mean = stat.get('mean_val', 0) if stat.get('mean_val') else stat.get('avg_val', 0)
                            std = stat.get('std_val', 0)
                            summary_parts.append(f"  - {tag}: {count}ê°œ ë°ì´í„° (í‰ê· : {mean:.2f}, í‘œì¤€í¸ì°¨: {std:.2f})")
                    
                    # ìƒ˜í”Œ ë°ì´í„° í¬ì¸íŠ¸ ì¶”ê°€
                    samples = correlation_info.get('samples', [])
                    if samples:
                        summary_parts.append("\nìµœê·¼ ë°ì´í„° ìƒ˜í”Œ:")
                        # ì„¼ì„œë³„ë¡œ ê·¸ë£¹í•‘
                        by_sensor = {}
                        for sample in samples[:10]:  # ìµœëŒ€ 10ê°œë§Œ
                            tag = sample.get('tag_name', '')
                            if tag not in by_sensor:
                                by_sensor[tag] = []
                            by_sensor[tag].append(sample.get('value', 0))
                        
                        for tag, values in by_sensor.items():
                            values_str = ', '.join([f"{v:.2f}" for v in values[:5]])  # ê° ì„¼ì„œë³„ ìµœëŒ€ 5ê°œ
                            summary_parts.append(f"  - {tag}: [{values_str}]")
                    
                    summary_parts.append("(30ì¼ ë°ì´í„° ê¸°ë°˜)")
                    return "\n".join(summary_parts)
            return "ìƒê´€ë¶„ì„ ë°ì´í„°ë¥¼ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤."
        elif query_type == 'status':
            return f"ì „ì²´ {total}ê°œ ì„¼ì„œ ì¤‘ ì •ìƒ {normal}ê°œ, ì£¼ì˜ {warning}ê°œ, ìœ„í—˜ {critical}ê°œ"
        elif query_type == 'alert':
            if warning + critical == 0:
                return "í˜„ì¬ ëª¨ë“  ì„¼ì„œê°€ ì •ìƒ ë²”ìœ„ ë‚´ì— ìˆìŠµë‹ˆë‹¤."
            else:
                return f"ì£¼ì˜ê°€ í•„ìš”í•œ ì„¼ì„œ {warning + critical}ê°œ ë°œê²¬ (ì£¼ì˜ {warning}ê°œ, ìœ„í—˜ {critical}ê°œ)"
        else:
            return f"ì„¼ì„œ {total}ê°œ ëª¨ë‹ˆí„°ë§ ì¤‘"
    
    def _generate_recommendations(self, sensor_data: List[SensorData], query_type: str) -> List[str]:
        """ê¶Œì¥ì‚¬í•­ ìƒì„± (í…œí”Œë¦¿ ê¸°ë°˜)"""
        
        recommendations = []
        
        critical_sensors = [s for s in sensor_data if s.status == 'critical']
        warning_sensors = [s for s in sensor_data if s.status == 'warning']
        
        if critical_sensors:
            recommendations.append("ì¦‰ì‹œ ìœ„í—˜ ìƒíƒœ ì„¼ì„œë¥¼ ì ê²€í•˜ì„¸ìš”")
            recommendations.append("ì‹œìŠ¤í…œ ì•ˆì „ì„ ìœ„í•´ í•„ìš”ì‹œ ê°€ë™ì„ ì¤‘ë‹¨í•˜ì„¸ìš”")
        
        if warning_sensors:
            recommendations.append("ì£¼ì˜ ìƒíƒœ ì„¼ì„œì˜ ì¶”ì´ë¥¼ ë©´ë°€íˆ ëª¨ë‹ˆí„°ë§í•˜ì„¸ìš”")
            
        if not critical_sensors and not warning_sensors:
            recommendations.append("ì •ê¸°ì ì¸ ëª¨ë‹ˆí„°ë§ì„ ê³„ì†í•˜ì„¸ìš”")
            
        return recommendations
    
    def format_json_response(self, response: AnalysisResponse) -> str:
        """JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ í¬ë§·"""
        
        # dataclassë¥¼ dictë¡œ ë³€í™˜
        response_dict = asdict(response)
        
        # JSON ë¬¸ìì—´ë¡œ ë³€í™˜ (í•œê¸€ ìœ ì§€)
        return json.dumps(response_dict, ensure_ascii=False, indent=2)
    
    def format_human_response(self, response: AnalysisResponse) -> str:
        """ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ í˜•ì‹ìœ¼ë¡œ í¬ë§·"""
        
        lines = []
        
        # ìš”ì•½
        lines.append(f"[INFO] {response.summary}")
        lines.append("")
        
        # ì•Œë¦¼
        if response.alerts:
            lines.append("[ALERT] ì•Œë¦¼:")
            for alert in response.alerts:
                lines.append(f"  {alert}")
            lines.append("")
        
        # ì„¼ì„œ ìƒì„¸
        if response.sensor_data:
            lines.append("[DATA] ì„¼ì„œ ìƒíƒœ:")
            for sensor in response.sensor_data[:5]:  # ìµœëŒ€ 5ê°œë§Œ
                status_icon = "[OK]" if sensor.status == "normal" else "[WARN]" if sensor.status == "warning" else "[CRIT]"
                lines.append(f"  {status_icon} {sensor.tag_name}: {sensor.value}{sensor.unit}")
            lines.append("")
        
        # ê¶Œì¥ì‚¬í•­
        if response.recommendations:
            lines.append("[RECOMMEND] ê¶Œì¥ì‚¬í•­:")
            for rec in response.recommendations:
                lines.append(f"  - {rec}")
            lines.append("")
        
        # ë°ì´í„° ë¶€ì¡± ê²½ê³ 
        if response.missing_data:
            lines.append(f"[WARN] ë°ì´í„° ì—†ìŒ: {', '.join(response.missing_data[:3])}")
        
        # ì‹ ë¢°ë„ (ì‹¤ì œ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ)
        if response.sensor_data:
            lines.append(f"[SCORE] ë°ì´í„° ìˆ˜: {len(response.sensor_data)}ê°œ")
        
        return "\n".join(lines)


# ì‚¬ìš© ì˜ˆì‹œ
async def generate_validated_response(query: str, context: Dict) -> str:
    """ê²€ì¦ëœ ì‘ë‹µ ìƒì„± (OpenAI API ì‚¬ìš©)"""
    print(f"\nğŸ¯ [VALIDATOR] generate_validated_response ì‹œì‘")
    print(f"   Query: {query}")
    print(f"   Context keys: {list(context.keys())}")
    
    from openai import AsyncOpenAI
    import os
    
    validator = ResponseValidator()
    
    # 1. êµ¬ì¡°í™”ëœ ì‘ë‹µ ìƒì„± (ë°ì´í„° ê²€ì¦ìš©)
    print(f"ğŸ“„ [VALIDATOR] êµ¬ì¡°í™”ëœ ì‘ë‹µ ìƒì„± ì¤‘...")
    structured_response = validator.create_structured_response(query, context)
    print(f"   - ì„¼ì„œ ë°ì´í„°: {len(structured_response.sensor_data)}ê°œ")
    print(f"   - ì•Œë¦¼: {len(structured_response.alerts)}ê°œ")
    print(f"   - ê¶Œì¥ì‚¬í•­: {len(structured_response.recommendations)}ê°œ")
    
    # 2. OpenAI APIë¥¼ í†µí•œ ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µ ìƒì„±
    try:
        api_key = os.getenv("OPENAI_API_KEY")
        print(f"ğŸ”‘ [VALIDATOR] API í‚¤ í™•ì¸: {api_key[:10] if api_key else 'None'}...")
        
        if not api_key or api_key == "dummy-key":
            raise ValueError("OpenAI API key not configured properly")
        
        client = AsyncOpenAI(api_key=api_key)
        print(f"âœ… [VALIDATOR] OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì„±ê³µ")
        
        # ì»¨í…ìŠ¤íŠ¸ ìš”ì•½ ìƒì„±
        print(f"ğŸ“ [VALIDATOR] ì»¨í…ìŠ¤íŠ¸ ìš”ì•½ ìƒì„± ì¤‘...")
        context_summary = []
        
        # ì„¼ì„œ ë°ì´í„° ìš”ì•½
        if structured_response.sensor_data:
            context_summary.append("í˜„ì¬ ì„¼ì„œ ìƒíƒœ:")
            sensor_count = 0
            for sensor in structured_response.sensor_data[:10]:  # ìµœëŒ€ 10ê°œ
                status_kr = "ì •ìƒ" if sensor.status == "normal" else "ì£¼ì˜" if sensor.status == "warning" else "ìœ„í—˜"
                context_summary.append(f"- {sensor.tag_name}: {sensor.value}{sensor.unit} ({status_kr})")
                sensor_count += 1
            print(f"   ì„¼ì„œ ë°ì´í„° ìš”ì•½: {sensor_count}ê°œ")
        
        # ë¹„êµ ë°ì´í„° ìš”ì•½
        if context.get('comparison_data'):
            context_summary.append("\nì–´ì œ ëŒ€ë¹„ ë³€í™”:")
            comp_count = 0
            for comp in context['comparison_data'][:5]:  # ìµœëŒ€ 5ê°œ
                change = comp.get('pct_change', 0)
                if change != 0:
                    direction = "ì¦ê°€" if comp.get('avg_change', 0) > 0 else "ê°ì†Œ"
                    context_summary.append(f"- {comp['tag_name']}: {abs(change):.1f}% {direction}")
                else:
                    context_summary.append(f"- {comp['tag_name']}: ë³€í™” ì—†ìŒ")
                comp_count += 1
            print(f"   ë¹„êµ ë°ì´í„° ìš”ì•½: {comp_count}ê°œ")
        
        # ìƒê´€ë¶„ì„ ë°ì´í„° ìš”ì•½
        if context.get('correlation_stats'):
            context_summary.append("\nìƒê´€ë¶„ì„ ë°ì´í„°:")
            corr_count = 0
            for stat in context['correlation_stats'][:2]:
                context_summary.append(f"- {stat['tag_name']}: í‰ê·  {stat.get('mean_val', 0):.2f}")
                corr_count += 1
            print(f"   ìƒê´€ë¶„ì„ ìš”ì•½: {corr_count}ê°œ")
        
        context_text = "\n".join(context_summary)
        print(f"ğŸ“ [VALIDATOR] ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(context_text)} ë¬¸ì")
        
        system_prompt = """ë‹¹ì‹ ì€ ì‚°ì—… ì„¼ì„œ ëª¨ë‹ˆí„°ë§ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì„¼ì„œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.

ì¤‘ìš” ê·œì¹™:
1. ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œë§Œ ë‹µë³€ (ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”)
2. êµ¬ì²´ì ì¸ ì„¼ì„œëª…ê³¼ ìˆ˜ì¹˜ë¥¼ í¬í•¨í•˜ì„¸ìš”
3. ìœ„í—˜/ê²½ê³  ìƒíƒœê°€ ìˆë‹¤ë©´ ë°˜ë“œì‹œ ì–¸ê¸‰í•˜ì„¸ìš”
4. ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”
5. í•„ìš”ì‹œ ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì„¸ìš”"""

        user_prompt = f"""ì‚¬ìš©ì ì§ˆë¬¸: {query}

ì„¼ì„œ ë°ì´í„°:
{context_text}

ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."""
        
        print(f"ğŸ¤– [VALIDATOR] OpenAI API í˜¸ì¶œ ì¤‘...")
        print(f"   ëª¨ë¸: gpt-3.5-turbo")
        print(f"   í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(user_prompt)} ë¬¸ì")
        
        response = await client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.3,
            max_tokens=500
        )
        
        ai_response = response.choices[0].message.content.strip()
        print(f"âœ… [VALIDATOR] OpenAI ì‘ë‹µ ìˆ˜ì‹ : {len(ai_response)} ë¬¸ì")
        print(f"   ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {ai_response[:100]}...")
        
        # 3. ìƒì„±ëœ ì‘ë‹µì— ëŒ€í•œ í• ë£¨ì‹œë„¤ì´ì…˜ ì²´í¬
        print(f"ğŸ›¡ï¸ [VALIDATOR] í• ë£¨ì‹œë„¤ì´ì…˜ ì²´í¬ ì¤‘...")
        hallucination_check = validator.detect_hallucination(ai_response, context)
        
        if hallucination_check['has_hallucination']:
            print(f"âš ï¸ [VALIDATOR] í• ë£¨ì‹œë„¤ì´ì…˜ ê°ì§€ë¨: {hallucination_check['issues']}")
            print(f"ğŸ”„ [VALIDATOR] ì•ˆì „í•œ í…œí”Œë¦¿ ì‘ë‹µìœ¼ë¡œ ëŒ€ì²´")
            # í• ë£¨ì‹œë„¤ì´ì…˜ì´ ê°ì§€ë˜ë©´ ì•ˆì „í•œ í…œí”Œë¦¿ ì‘ë‹µìœ¼ë¡œ ëŒ€ì²´
            return validator.format_human_response(structured_response)
        
        print(f"âœ… [VALIDATOR] í• ë£¨ì‹œë„¤ì´ì…˜ ì²´í¬ í†µê³¼")
        print(f"ğŸ¯ [VALIDATOR] ìµœì¢… ì‘ë‹µ ë°˜í™˜")
        return ai_response
        
    except Exception as e:
        print(f"âŒ [VALIDATOR] OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        import traceback
        print(f"ğŸ“‹ [VALIDATOR] ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
        print(f"ğŸ”„ [VALIDATOR] í…œí”Œë¦¿ ì‘ë‹µìœ¼ë¡œ í´ë°±")
        # API í˜¸ì¶œ ì‹¤íŒ¨ì‹œ í…œí”Œë¦¿ ì‘ë‹µ ë°˜í™˜
        return validator.format_human_response(structured_response)