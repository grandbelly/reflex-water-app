"""
Enhanced RAG Engine with pg_vector
pg_vector í™•ì¥ì„ ì‚¬ìš©í•œ ê³ ì„±ëŠ¥ ë²¡í„° ê²€ìƒ‰ RAG ì—”ì§„
"""

import asyncio
import numpy as np
from typing import List, Dict, Any, Optional
from openai import OpenAI
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import json

from water_app.db import q, execute_query


class PgVectorRAGEngine:
    """pg_vector ê¸°ë°˜ ê³ ì„±ëŠ¥ RAG ì—”ì§„"""
    
    def __init__(self, openai_api_key: str = None):
        self.openai_client = OpenAI(api_key=openai_api_key) if openai_api_key else None
        self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        self.knowledge_cache = []
        self.embedding_model = "text-embedding-ada-002"
        self.embedding_dimension = 1536
        
    async def initialize(self):
        """RAG ì—”ì§„ ì´ˆê¸°í™”"""
        print("ğŸš€ pg_vector RAG ì—”ì§„ ì´ˆê¸°í™” ì¤‘...")
        
        # 1. pg_vector í™•ì¥ í™•ì¸
        await self._check_pgvector_extension()
        
        # 2. ë²¡í„° ì»¬ëŸ¼ í™•ì¸
        await self._check_vector_column()
        
        # 3. ì§€ì‹ë² ì´ìŠ¤ ìºì‹œ ì—…ë°ì´íŠ¸
        await self._update_knowledge_cache()
        
        # 4. ë²¡í„°í™”ë˜ì§€ ì•Šì€ ì§€ì‹ ì„ë² ë”© ìƒì„±
        await self._generate_missing_embeddings()
        
        print("âœ… pg_vector RAG ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ!")
        
    async def _check_pgvector_extension(self):
        """pg_vector í™•ì¥ ì„¤ì¹˜ í™•ì¸"""
        try:
            sql = "SELECT * FROM pg_extension WHERE extname = 'vector'"
            result = await q(sql, ())
            if not result:
                print("âš ï¸ pg_vector í™•ì¥ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                print("   ê´€ë¦¬ì ê¶Œí•œìœ¼ë¡œ 'CREATE EXTENSION vector;' ì‹¤í–‰ í•„ìš”")
            else:
                print("âœ… pg_vector í™•ì¥ í™•ì¸ë¨")
        except Exception as e:
            print(f"âŒ pg_vector í™•ì¥ í™•ì¸ ì‹¤íŒ¨: {e}")
            
    async def _check_vector_column(self):
        """ë²¡í„° ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸"""
        try:
            sql = """
                SELECT column_name, data_type 
                FROM information_schema.columns 
                WHERE table_name = 'ai_knowledge_base' 
                AND column_name = 'content_embedding'
            """
            result = await q(sql, ())
            if not result:
                print("âš ï¸ content_embedding ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                print("   ALTER TABLE ai_knowledge_base ADD COLUMN content_embedding vector(1536); ì‹¤í–‰ í•„ìš”")
            else:
                print("âœ… content_embedding ë²¡í„° ì»¬ëŸ¼ í™•ì¸ë¨")
        except Exception as e:
            print(f"âŒ ë²¡í„° ì»¬ëŸ¼ í™•ì¸ ì‹¤íŒ¨: {e}")
            
    async def _update_knowledge_cache(self):
        """ì§€ì‹ë² ì´ìŠ¤ ìºì‹œ ì—…ë°ì´íŠ¸"""
        try:
            sql = """
                SELECT id, content, content_type, metadata, created_at,
                       content_embedding IS NOT NULL as is_vectorized
                FROM ai_knowledge_base 
                ORDER BY created_at DESC
            """
            knowledge_data = await q(sql, ())
            
            if knowledge_data:
                self.knowledge_cache = knowledge_data
                vectorized_count = sum(1 for item in knowledge_data if item['is_vectorized'])
                print(f"ğŸ“š ì§€ì‹ë² ì´ìŠ¤ ìºì‹œ ì—…ë°ì´íŠ¸: {len(knowledge_data)}ê°œ í•­ëª© (ë²¡í„°í™”: {vectorized_count}ê°œ)")
            else:
                print("âš ï¸ ì§€ì‹ë² ì´ìŠ¤ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            print(f"âŒ ì§€ì‹ë² ì´ìŠ¤ ìºì‹œ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            
    async def _generate_missing_embeddings(self):
        """ë²¡í„°í™”ë˜ì§€ ì•Šì€ ì§€ì‹ì˜ ì„ë² ë”© ìƒì„±"""
        if not self.openai_client:
            print("âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ì„ë² ë”©ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        try:
            # ë²¡í„°í™”ë˜ì§€ ì•Šì€ ì§€ì‹ ì¡°íšŒ
            sql = """
                SELECT id, content 
                FROM ai_knowledge_base 
                WHERE content_embedding IS NULL
                LIMIT 50
            """
            unvectorized = await q(sql, ())
            
            if not unvectorized:
                print("âœ… ëª¨ë“  ì§€ì‹ì´ ë²¡í„°í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                return
                
            print(f"ğŸ”„ {len(unvectorized)}ê°œ ì§€ì‹ì˜ ì„ë² ë”© ìƒì„± ì¤‘...")
            
            for item in unvectorized:
                try:
                    # OpenAI ì„ë² ë”© ìƒì„±
                    response = self.openai_client.embeddings.create(
                        input=item['content'],
                        model=self.embedding_model
                    )
                    embedding = response.data[0].embedding
                    
                    # ë²¡í„° ì—…ë°ì´íŠ¸
                    update_sql = """
                        UPDATE ai_knowledge_base 
                        SET content_embedding = %s::vector 
                        WHERE id = %s
                    """
                    await execute_query(update_sql, (embedding, item['id']))
                    
                    print(f"  âœ… ID {item['id']}: ì„ë² ë”© ìƒì„± ì™„ë£Œ")
                    
                except Exception as e:
                    print(f"  âŒ ID {item['id']}: ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ - {e}")
                    continue
                    
            # ìºì‹œ ì¬ì—…ë°ì´íŠ¸
            await self._update_knowledge_cache()
            
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            
    async def semantic_search_vector(self, query: str, top_k: int = 5, 
                                   threshold: float = 0.7) -> List[Dict[str, Any]]:
        """pg_vectorë¥¼ ì‚¬ìš©í•œ ê³ ì„±ëŠ¥ ë²¡í„° ê²€ìƒ‰"""
        
        if not self.openai_client:
            print("âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ë²¡í„° ê²€ìƒ‰ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return await self.semantic_search_tfidf(query, top_k)
            
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            response = self.openai_client.embeddings.create(
                input=query,
                model=self.embedding_model
            )
            query_embedding = response.data[0].embedding
            
            # pg_vector ê²€ìƒ‰ í•¨ìˆ˜ í˜¸ì¶œ
            sql = """
                SELECT * FROM search_knowledge_vector(
                    %s::vector, %s, %s
                )
            """
            params = (query_embedding, threshold, top_k)
            
            results = await q(sql, params)
            
            if results:
                print(f"ğŸ” ë²¡í„° ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ (ì„ê³„ê°’: {threshold})")
                return results
            else:
                print(f"ğŸ” ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (ì„ê³„ê°’: {threshold})")
                return []
                
        except Exception as e:
            print(f"âŒ ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            # í´ë°±: TF-IDF ê²€ìƒ‰
            return await self.semantic_search_tfidf(query, top_k)
            
    async def semantic_search_tfidf(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """ê¸°ì¡´ TF-IDF ê¸°ë°˜ ê²€ìƒ‰ (í´ë°±)"""
        
        if not self.knowledge_cache:
            await self._update_knowledge_cache()
            
        if not self.knowledge_cache:
            return []
            
        try:
            # ì¿¼ë¦¬ ë²¡í„°í™”
            query_vector = self.vectorizer.fit_transform([query])
            
            # ëª¨ë“  ì§€ì‹ ë²¡í„°í™”  
            texts = [item['content'] for item in self.knowledge_cache]
            knowledge_vectors = self.vectorizer.transform(texts)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = cosine_similarity(query_vector, knowledge_vectors)[0]
            
            # ìƒìœ„ Kê°œ ê²°ê³¼ ì¶”ì¶œ
            top_indices = np.argsort(similarities)[::-1][:top_k]
            
            results = []
            for idx in top_indices:
                if similarities[idx] > 0.1:  # ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’
                    item = self.knowledge_cache[idx].copy()
                    item['similarity_score'] = float(similarities[idx])
                    results.append(item)
                    
            return results
            
        except Exception as e:
            print(f"âŒ TF-IDF ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
            
    async def hybrid_search(self, query: str, top_k: int = 5,
                           vector_weight: float = 0.7, text_weight: float = 0.3) -> List[Dict[str, Any]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„° + í…ìŠ¤íŠ¸)"""
        
        if not self.openai_client:
            return await self.semantic_search_tfidf(query, top_k)
            
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            response = self.openai_client.embeddings.create(
                input=query,
                model=self.embedding_model
            )
            query_embedding = response.data[0].embedding
            
            # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í•¨ìˆ˜ í˜¸ì¶œ
            sql = """
                SELECT * FROM search_knowledge_hybrid(
                    %s, %s::vector, %s, %s, %s
                )
            """
            params = (query, query_embedding, vector_weight, text_weight, top_k)
            
            results = await q(sql, params)
            
            if results:
                print(f"ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
                return results
            else:
                print("ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                return []
                
        except Exception as e:
            print(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return await self.semantic_search_tfidf(query, top_k)
            
    async def search_sensor_knowledge_vector(self, sensor_tag: str, query: str, 
                                           top_k: int = 10) -> List[Dict[str, Any]]:
        """ì„¼ì„œë³„ ë²¡í„° ê²€ìƒ‰"""
        
        if not self.openai_client:
            return await self._search_sensor_knowledge_tfidf(sensor_tag, query, top_k)
            
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            response = self.openai_client.embeddings.create(
                input=query,
                model=self.embedding_model
            )
            query_embedding = response.data[0].embedding
            
            # ì„¼ì„œë³„ ë²¡í„° ê²€ìƒ‰ í•¨ìˆ˜ í˜¸ì¶œ
            sql = """
                SELECT * FROM search_sensor_knowledge_vector(
                    %s, %s::vector, 0.6, %s
                )
            """
            params = (sensor_tag, query_embedding, top_k)
            
            results = await q(sql, params)
            
            if results:
                print(f"ğŸ” ì„¼ì„œ {sensor_tag} ë²¡í„° ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
                return results
            else:
                print(f"ğŸ” ì„¼ì„œ {sensor_tag} ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                return []
                
        except Exception as e:
            print(f"âŒ ì„¼ì„œ ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return await self._search_sensor_knowledge_tfidf(sensor_tag, query, top_k)
            
    async def _search_sensor_knowledge_tfidf(self, sensor_tag: str, query: str, 
                                            top_k: int = 10) -> List[Dict[str, Any]]:
        """ì„¼ì„œë³„ TF-IDF ê²€ìƒ‰ (í´ë°±)"""
        
        try:
            sql = """
                SELECT id, content, content_type, metadata, created_at
                FROM ai_knowledge_base 
                WHERE metadata->>'sensor_tag' = %s 
                   OR metadata->>'primary_sensor' = %s
                   OR metadata->>'secondary_sensor' = %s
                ORDER BY created_at DESC
                LIMIT %s
            """
            params = [sensor_tag] * 3 + [top_k]
            
            results = await q(sql, params)
            return results or []
            
        except Exception as e:
            print(f"âŒ ì„¼ì„œ TF-IDF ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
            
    async def get_knowledge_stats(self) -> Dict[str, Any]:
        """ì§€ì‹ë² ì´ìŠ¤ í†µê³„ ì¡°íšŒ"""
        try:
            sql = "SELECT * FROM knowledge_base_stats"
            stats = await q(sql, ())
            
            # ë²¡í„° í’ˆì§ˆ ê²€ì¦
            quality_sql = "SELECT * FROM validate_vector_quality()"
            quality = await q(quality_sql, ())
            
            return {
                "stats": stats,
                "quality": quality,
                "total_cache": len(self.knowledge_cache),
                "vectorized_cache": sum(1 for item in self.knowledge_cache if item.get('is_vectorized', False))
            }
            
        except Exception as e:
            print(f"âŒ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
            
    async def optimize_vector_indexes(self):
        """ë²¡í„° ì¸ë±ìŠ¤ ìµœì í™”"""
        try:
            print("ğŸ”§ ë²¡í„° ì¸ë±ìŠ¤ ìµœì í™” ì¤‘...")
            
            # ì¸ë±ìŠ¤ í†µê³„ ìˆ˜ì§‘
            sql = "ANALYZE ai_knowledge_base"
            await execute_query(sql, ())
            
            # ë²¡í„° ì¸ë±ìŠ¤ ì¬êµ¬ì¶• (í•„ìš”ì‹œ)
            sql = "REINDEX INDEX CONCURRENTLY idx_ai_knowledge_vector"
            await execute_query(sql, ())
            
            print("âœ… ë²¡í„° ì¸ë±ìŠ¤ ìµœì í™” ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ ì¸ë±ìŠ¤ ìµœì í™” ì‹¤íŒ¨: {e}")
            
    async def cleanup_old_embeddings(self, days: int = 30):
        """ì˜¤ë˜ëœ ì„ë² ë”© ì •ë¦¬"""
        try:
            print(f"ğŸ§¹ {days}ì¼ ì´ìƒ ëœ ì„ë² ë”© ì •ë¦¬ ì¤‘...")
            
            sql = """
                UPDATE ai_knowledge_base 
                SET content_embedding = NULL 
                WHERE updated_at < NOW() - INTERVAL '%s days'
                AND content_embedding IS NOT NULL
            """
            await execute_query(sql, (days,))
            
            print("âœ… ì˜¤ë˜ëœ ì„ë² ë”© ì •ë¦¬ ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ì •ë¦¬ ì‹¤íŒ¨: {e}")


# ì‚¬ìš© ì˜ˆì‹œ
async def test_pgvector_rag():
    """pg_vector RAG ì—”ì§„ í…ŒìŠ¤íŠ¸"""
    
    # OpenAI API í‚¤ ì„¤ì • í•„ìš”
    openai_api_key = "your-api-key-here"
    
    rag_engine = PgVectorRAGEngine(openai_api_key)
    
    # ì´ˆê¸°í™”
    await rag_engine.initialize()
    
    # ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    query = "ì˜¨ë„ ì„¼ì„œ D100ì˜ ì •ìƒ ë²”ìœ„ëŠ”?"
    results = await rag_engine.semantic_search_vector(query, top_k=3)
    
    print(f"ğŸ” ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
    for i, result in enumerate(results, 1):
        print(f"{i}. {result['content'][:100]}... (ìœ ì‚¬ë„: {result.get('similarity', 'N/A'):.3f})")
        
    # í†µê³„ ì¡°íšŒ
    stats = await rag_engine.get_knowledge_stats()
    print(f"ğŸ“Š ì§€ì‹ë² ì´ìŠ¤ í†µê³„: {stats}")


if __name__ == "__main__":
    asyncio.run(test_pgvector_rag())

