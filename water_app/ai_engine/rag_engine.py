"""
Real RAG (Retrieval-Augmented Generation) Engine
ì§„ì§œ RAG ì‹œìŠ¤í…œ - ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ ë° ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‘ë‹µ ìƒì„±
"""

import json
import asyncio
import os
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime

# OpenAI integration
from openai import AsyncOpenAI

# Text processing and similarity  
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Database and existing modules
from water_app.queries.latest import latest_snapshot
from water_app.queries.qc import qc_rules  
from .knowledge_builder import search_knowledge, get_sensor_knowledge
from .hallucination_prevention import HallucinationPrevention, ValidationResult
from .w5h1_formatter import W5H1Formatter, W5H1Response


class RAGEngine:
    """ì§„ì§œ RAG ì—”ì§„ - ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ ë° ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‘ë‹µ ìƒì„±"""
    
    def __init__(self):
        self.vectorizer = None
        self.knowledge_cache = []
        self.last_cache_update = None
        self.openai_client = None
        self.hallucination_prevention = None
        self.w5h1_formatter = W5H1Formatter()
        
    async def initialize(self):
        """RAG ì—”ì§„ ì´ˆê¸°í™”"""
        # í™˜ê²½ë³€ìˆ˜ ê°•ì œ ë¡œë“œ
        from ..ksys_app import load_env
        load_env()
        
        # ë³´ì•ˆ ì„¤ì •ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
        from water_app.utils.secure_config import get_api_key_manager
        api_manager = get_api_key_manager()
        api_key = api_manager.get_openai_key()
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        if api_key:
            self.openai_client = AsyncOpenAI(api_key=api_key)
            masked_key = api_manager.secure_config.mask_api_key(api_key)
            print(f"âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ (í‚¤: {masked_key})")
        else:
            print("âš ï¸ OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. í…œí”Œë¦¿ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
        
        # í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        db_dsn = os.getenv('TS_DSN', 'postgresql://postgres:admin@192.168.1.80:5432/EcoAnP?sslmode=disable')
        self.hallucination_prevention = HallucinationPrevention(db_dsn)
        print("ğŸ›¡ï¸ í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            
        await self._update_knowledge_cache()
        print("ğŸ§  RAG ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
        
    async def _update_knowledge_cache(self):
        """ì§€ì‹ë² ì´ìŠ¤ ìºì‹œ ì—…ë°ì´íŠ¸"""
        try:
            # ëª¨ë“  ì§€ì‹ ë¡œë“œ
            from water_app.db import q
            sql = "SELECT id, content, content_type, metadata FROM ai_engine.knowledge_base ORDER BY created_at"
            knowledge_data = await q(sql, ())
            
            if knowledge_data:
                self.knowledge_cache = knowledge_data
                
                # TF-IDF ë²¡í„°í™”
                texts = [item['content'] for item in knowledge_data]
                self.vectorizer = TfidfVectorizer(
                    max_features=1000,
                    stop_words='english',
                    ngram_range=(1, 2)
                )
                self.vectorizer.fit(texts)
                
                self.last_cache_update = datetime.now()
                print(f"ğŸ“š ì§€ì‹ë² ì´ìŠ¤ ìºì‹œ ì—…ë°ì´íŠ¸: {len(knowledge_data)}ê°œ í•­ëª©")
            else:
                print("ğŸ“š ì§€ì‹ë² ì´ìŠ¤ í…Œì´ë¸”ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. OpenAI ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
                
        except Exception as e:
            print(f"ğŸ“š ì§€ì‹ë² ì´ìŠ¤ ìºì‹œ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ (ì •ìƒ): {e}")
            print("ğŸ“š OpenAI ì „ìš© ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
            # í…Œì´ë¸”ì´ ì—†ì–´ë„ ê³„ì† ì§„í–‰
            
    async def semantic_search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ - TF-IDF ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°"""
        print(f"\nğŸ” [RAG] semantic_search ì‹œì‘: query='{query}', top_k={top_k}")
        
        if not self.vectorizer or not self.knowledge_cache:
            print(f"ğŸ“š [RAG] ìºì‹œ ì—…ë°ì´íŠ¸ í•„ìš” - vectorizer: {self.vectorizer is not None}, cache: {len(self.knowledge_cache) if self.knowledge_cache else 0}")
            await self._update_knowledge_cache()
            
        if not self.knowledge_cache:
            print(f"âš ï¸ [RAG] ì§€ì‹ë² ì´ìŠ¤ ìºì‹œ ë¹„ì–´ìˆìŒ")
            return []
            
        try:
            # ì¿¼ë¦¬ ë²¡í„°í™”
            print(f"ğŸ”¢ [RAG] ì¿¼ë¦¬ ë²¡í„°í™” ì¤‘...")
            query_vector = self.vectorizer.transform([query])
            print(f"   ë²¡í„° í˜•íƒœ: {query_vector.shape}, 0ì´ ì•„ë‹Œ ê°’: {query_vector.nnz}")
            
            # ëª¨ë“  ì§€ì‹ ë²¡í„°í™”  
            texts = [item['content'] for item in self.knowledge_cache]
            print(f"ğŸ“š [RAG] ì§€ì‹ë² ì´ìŠ¤ ë²¡í„°í™”: {len(texts)}ê°œ í•­ëª©")
            knowledge_vectors = self.vectorizer.transform(texts)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = cosine_similarity(query_vector, knowledge_vectors)[0]
            print(f"ğŸ“Š [RAG] ìœ ì‚¬ë„ ê³„ì‚° ì™„ë£Œ: min={similarities.min():.3f}, max={similarities.max():.3f}, mean={similarities.mean():.3f}")
            
            # ìƒìœ„ Kê°œ ê²°ê³¼ ì¶”ì¶œ
            top_indices = np.argsort(similarities)[::-1][:top_k]
            print(f"ğŸ¯ [RAG] ìƒìœ„ {top_k}ê°œ ì¸ë±ìŠ¤: {top_indices.tolist()}")
            
            results = []
            for idx in top_indices:
                sim_score = float(similarities[idx])
                if sim_score > 0.1:  # ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’
                    item = self.knowledge_cache[idx].copy()
                    item['similarity_score'] = sim_score
                    results.append(item)
                    print(f"   âœ… í¬í•¨: idx={idx}, ìœ ì‚¬ë„={sim_score:.3f}, íƒ€ì…={item.get('content_type')}")
                else:
                    print(f"   âŒ ì œì™¸: idx={idx}, ìœ ì‚¬ë„={sim_score:.3f} (ì„ê³„ê°’ 0.1 ë¯¸ë‹¬)")
            
            print(f"ğŸ” [RAG] semantic_search ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
            return results
            
        except Exception as e:
            print(f"âŒ [RAG] semantic_search ì‹¤íŒ¨: {e}")
            import traceback
            print(traceback.format_exc())
            return []
            
    async def extract_sensor_tags(self, query: str) -> List[str]:
        """ì¿¼ë¦¬ì—ì„œ ì„¼ì„œ íƒœê·¸ ì¶”ì¶œ"""
        print(f"\nğŸ·ï¸ [RAG] extract_sensor_tags: query='{query}'")
        import re
        
        # D + ìˆ«ì íŒ¨í„´ ë§¤ì¹­
        pattern = r'D\d{3}'
        matches = re.findall(pattern, query.upper())
        print(f"   íŒ¨í„´ ë§¤ì¹­ ê²°ê³¼: {matches}")
        
        # ì¼ë°˜ì ì¸ ì„¼ì„œ ì–¸ê¸‰ë„ í¬í•¨
        sensor_keywords = {
            'ì˜¨ë„': ['D100'],
            'ì••ë ¥': ['D101'], 
            'ìœ ëŸ‰': ['D102'],
            'ì§„ë™': ['D200'],
            'ì „ë ¥': ['D300']
        }
        
        for keyword, tags in sensor_keywords.items():
            if keyword in query:
                print(f"   í‚¤ì›Œë“œ '{keyword}' ë°œê²¬ -> íƒœê·¸ ì¶”ê°€: {tags}")
                matches.extend(tags)
        
        unique_tags = list(set(matches))  # ì¤‘ë³µ ì œê±°
        print(f"ğŸ·ï¸ [RAG] ì¶”ì¶œëœ ì„¼ì„œ íƒœê·¸: {unique_tags}")
        return unique_tags
        
    async def build_context(self, query: str) -> Dict[str, Any]:
        """ì§ˆë¬¸ì— ëŒ€í•œ ì¢…í•©ì ì¸ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        print(f"\nğŸ“‹ [RAG] build_context ì‹œì‘: query='{query}'")
        
        context = {
            "query": query,
            "sensor_tags": [],
            "current_data": [],
            "qc_rules": [], 
            "relevant_knowledge": [],
            "sensor_specific_knowledge": [],
            "sensor_metadata": {}  # ì„¼ì„œ íƒ€ì…ê³¼ ë‹¨ìœ„ ì •ë³´
        }
        
        try:
            # 1. ì„¼ì„œ íƒœê·¸ ì¶”ì¶œ
            context["sensor_tags"] = await self.extract_sensor_tags(query)
            print(f"ğŸ“‹ [RAG] 1ë‹¨ê³„ ì™„ë£Œ - ì„¼ì„œ íƒœê·¸: {context['sensor_tags']}")
            
            # 1.5. ì„¼ì„œ ë©”íƒ€ë°ì´í„° ì„¤ì •
            for tag in context["sensor_tags"]:
                if tag == 'D100':
                    context["sensor_metadata"][tag] = {'type': 'temperature', 'unit': 'Â°C', 'name': 'ì˜¨ë„'}
                elif tag == 'D101':
                    context["sensor_metadata"][tag] = {'type': 'pressure', 'unit': 'bar', 'name': 'ì••ë ¥'}
                elif tag == 'D102':
                    context["sensor_metadata"][tag] = {'type': 'flow', 'unit': 'L/min', 'name': 'ìœ ëŸ‰'}
                elif tag.startswith('D2'):
                    context["sensor_metadata"][tag] = {'type': 'vibration', 'unit': 'mm/s', 'name': 'ì§„ë™'}
                elif tag.startswith('D3'):
                    context["sensor_metadata"][tag] = {'type': 'power', 'unit': '%', 'name': 'ì „ë ¥'}
            
            # 2. í˜„ì¬ ì„¼ì„œ ë°ì´í„° ì¡°íšŒ
            if context["sensor_tags"]:
                print(f"ğŸ“Š [RAG] 2ë‹¨ê³„ - íŠ¹ì • ì„¼ì„œ ë°ì´í„° ì¡°íšŒ ì¤‘...")
                for tag in context["sensor_tags"]:
                    sensor_data = await latest_snapshot(tag)
                    if sensor_data:
                        context["current_data"].extend(sensor_data)
                        print(f"   {tag}: {len(sensor_data)}ê°œ ë°ì´í„°")
                    else:
                        print(f"   {tag}: ë°ì´í„° ì—†ìŒ")
            else:
                # ì „ì²´ ë°ì´í„° ì¡°íšŒ
                print(f"ğŸ“Š [RAG] 2ë‹¨ê³„ - ì „ì²´ ì„¼ì„œ ë°ì´í„° ì¡°íšŒ ì¤‘...")
                all_data = await latest_snapshot(None)
                context["current_data"] = all_data[:10] if all_data else []
                print(f"   ì „ì²´ ë°ì´í„°: {len(context['current_data'])}ê°œ")
                
            # 3. QC ê·œì¹™ ì¡°íšŒ
            if context["sensor_tags"]:
                print(f"âš–ï¸ [RAG] 3ë‹¨ê³„ - QC ê·œì¹™ ì¡°íšŒ ì¤‘...")
                for tag in context["sensor_tags"]:
                    qc_data = await qc_rules(tag)
                    if qc_data:
                        context["qc_rules"].extend(qc_data)
                        print(f"   {tag}: {len(qc_data)}ê°œ QC ê·œì¹™")
                    else:
                        print(f"   {tag}: QC ê·œì¹™ ì—†ìŒ")
            
            # 4. ì˜ë¯¸ë¡ ì  ì§€ì‹ ê²€ìƒ‰
            print(f"ğŸ§  [RAG] 4ë‹¨ê³„ - ì˜ë¯¸ë¡ ì  ì§€ì‹ ê²€ìƒ‰ ì¤‘...")
            context["relevant_knowledge"] = await self.semantic_search(query, top_k=5)
            print(f"   ê´€ë ¨ ì§€ì‹: {len(context['relevant_knowledge'])}ê°œ")
            
            # 5. ì„¼ì„œë³„ íŠ¹í™” ì§€ì‹
            if context["sensor_tags"]:
                print(f"ğŸ”§ [RAG] 5ë‹¨ê³„ - ì„¼ì„œë³„ íŠ¹í™” ì§€ì‹ ì¡°íšŒ ì¤‘...")
                for tag in context["sensor_tags"]:
                    sensor_knowledge = await get_sensor_knowledge(tag)
                    if sensor_knowledge:
                        context["sensor_specific_knowledge"].extend(sensor_knowledge)
                        print(f"   {tag}: {len(sensor_knowledge)}ê°œ íŠ¹í™” ì§€ì‹")
                    else:
                        print(f"   {tag}: íŠ¹í™” ì§€ì‹ ì—†ìŒ")
                    
        except Exception as e:
            print(f"âŒ [RAG] build_context ì‹¤íŒ¨: {e}")
            import traceback
            print(traceback.format_exc())
        
        print(f"ğŸ“‹ [RAG] build_context ì™„ë£Œ:")
        print(f"   - ì„¼ì„œ íƒœê·¸: {len(context['sensor_tags'])}ê°œ")
        print(f"   - í˜„ì¬ ë°ì´í„°: {len(context['current_data'])}ê°œ")
        print(f"   - QC ê·œì¹™: {len(context['qc_rules'])}ê°œ")
        print(f"   - ê´€ë ¨ ì§€ì‹: {len(context['relevant_knowledge'])}ê°œ")
        print(f"   - íŠ¹í™” ì§€ì‹: {len(context['sensor_specific_knowledge'])}ê°œ")
        
        return context
        
    def _format_context_for_llm(self, context: Dict[str, Any]) -> str:
        """LLMì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…"""
        
        formatted = f"ì‚¬ìš©ì ì§ˆë¬¸: {context['query']}\n\n"
        
        # í˜„ì¬ ì„¼ì„œ ë°ì´í„°
        if context["current_data"]:
            formatted += "ğŸ“Š í˜„ì¬ ì„¼ì„œ ë°ì´í„°:\n"
            for data in context["current_data"][:5]:  # ìµœëŒ€ 5ê°œë§Œ
                tag = data.get('tag_name', 'Unknown')
                value = data.get('value', 'N/A')
                ts = data.get('ts', 'N/A')
                formatted += f"- {tag}: {value} (ì‹œê°„: {ts})\n"
            formatted += "\n"
            
        # ê´€ë ¨ ì§€ì‹
        if context["relevant_knowledge"]:
            formatted += "ğŸ§  ê´€ë ¨ ë„ë©”ì¸ ì§€ì‹:\n"
            for knowledge in context["relevant_knowledge"]:
                score = knowledge.get('similarity_score', 0)
                content = knowledge.get('content', '')
                formatted += f"- (ìœ ì‚¬ë„: {score:.2f}) {content}\n"
            formatted += "\n"
            
        # ì„¼ì„œë³„ ì „ë¬¸ ì§€ì‹
        if context["sensor_specific_knowledge"]:
            formatted += "ğŸ”§ ì„¼ì„œ ì „ë¬¸ ì§€ì‹:\n"
            for knowledge in context["sensor_specific_knowledge"]:
                content_type = knowledge.get('content_type', '')
                content = knowledge.get('content', '')
                formatted += f"- [{content_type}] {content}\n"
            formatted += "\n"
            
        # QC ê·œì¹™
        if context["qc_rules"]:
            formatted += "âš–ï¸ í’ˆì§ˆ ê´€ë¦¬ ê·œì¹™:\n"
            for rule in context["qc_rules"]:
                tag = rule.get('tag_name', '')
                min_val = rule.get('min_val', 'N/A')
                max_val = rule.get('max_val', 'N/A') 
                formatted += f"- {tag}: ë²”ìœ„ {min_val} ~ {max_val}\n"
                
        return formatted
        
    async def generate_response(self, query: str, use_w5h1: bool = True) -> str:
        """RAG ê¸°ë°˜ ì‘ë‹µ ìƒì„±
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            use_w5h1: 6í•˜ì›ì¹™ í¬ë§· ì‚¬ìš© ì—¬ë¶€
        """
        
        try:
            # 1. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = await self.build_context(query)
            
            # 2. ê·œì¹™ ê¸°ë°˜ ë¹ ë¥¸ ì‘ë‹µ (íŠ¹ì • íŒ¨í„´)
            quick_response = await self._try_quick_response(query, context)
            if quick_response:
                # 6í•˜ì›ì¹™ í¬ë§·íŒ… ì ìš©
                if use_w5h1:
                    quick_response = self.w5h1_formatter.format_response(
                        quick_response, 
                        question=query,
                        context=context,
                        format_type='markdown'
                    )
                return quick_response
                
            # 3. LLM ê¸°ë°˜ ì‘ë‹µ ìƒì„±
            response = await self._generate_contextual_response(query, context)
            
            # 4. 6í•˜ì›ì¹™ í¬ë§·íŒ… ì ìš©
            if use_w5h1:
                response = self.w5h1_formatter.format_response(
                    response,
                    question=query,
                    context=context,
                    format_type='markdown'
                )
            
            return response
            
        except Exception as e:
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            
    async def _try_quick_response(self, query: str, context: Dict[str, Any]) -> Optional[str]:
        """ê·œì¹™ ê¸°ë°˜ ë¹ ë¥¸ ì‘ë‹µ ì‹œë„"""
        
        query_lower = query.lower()
        
        # í˜„ì¬ ìƒíƒœ ì§ˆë¬¸
        if any(word in query_lower for word in ['í˜„ì¬', 'ìƒíƒœ', 'current', 'status']):
            if context["sensor_tags"]:
                return self._format_current_status_response(context)
                
        # ê²½ê³ /ì•ŒëŒ ì§ˆë¬¸  
        if any(word in query_lower for word in ['ê²½ê³ ', 'ì•ŒëŒ', 'ìœ„í—˜', 'ì´ìƒ', 'alert', 'warning']):
            return self._format_alert_response(context)
            
        # ë„ì›€ë§/ì‚¬ìš©ë²•
        if any(word in query_lower for word in ['ë„ì›€', 'ë„ì›€ë§', 'ì‚¬ìš©ë²•', 'help']):
            return self._format_help_response()
            
        return None
        
    def _format_current_status_response(self, context: Dict[str, Any]) -> str:
        """í˜„ì¬ ìƒíƒœ ì‘ë‹µ í¬ë§·"""
        
        if not context["current_data"]:
            return "í˜„ì¬ ì„¼ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        response = "ğŸ“Š í˜„ì¬ ì„¼ì„œ ìƒíƒœ:\n\n"
        
        for data in context["current_data"][:5]:
            tag = data.get('tag_name', 'Unknown')
            value = data.get('value', 'N/A') 
            ts = data.get('ts', 'N/A')
            
            # ê´€ë ¨ ì§€ì‹ ì°¾ê¸°
            sensor_info = ""
            for knowledge in context["sensor_specific_knowledge"]:
                if knowledge.get('metadata', {}).get('sensor_tag') == tag:
                    if knowledge.get('content_type') == 'sensor_spec':
                        sensor_info = f" - {knowledge['content'][:100]}..."
                        break
                        
            response += f"ğŸ”¸ **{tag}**: {value}\n"
            response += f"   â° {ts}\n"
            if sensor_info:
                response += f"   â„¹ï¸ {sensor_info}\n"
            response += "\n"
            
        # ê´€ë ¨ ì§€ì‹ ì¶”ê°€
        if context["relevant_knowledge"]:
            response += "\nğŸ’¡ **ê´€ë ¨ ì •ë³´**:\n"
            for knowledge in context["relevant_knowledge"][:2]:
                response += f"â€¢ {knowledge['content'][:150]}...\n"
                
        return response
        
    def _format_alert_response(self, context: Dict[str, Any]) -> str:
        """ê²½ê³  ìƒíƒœ ì‘ë‹µ í¬ë§·"""  
        
        # QC ê·œì¹™ê³¼ í˜„ì¬ ë°ì´í„° ë¹„êµ
        alerts = []
        
        for data in context["current_data"]:
            tag = data.get('tag_name')
            value = data.get('value')
            
            if not tag or value is None:
                continue
                
            # QC ê·œì¹™ ì°¾ê¸°
            for rule in context["qc_rules"]:
                if rule.get('tag_name') == tag:
                    try:
                        val = float(value)
                        min_val = rule.get('min_val')
                        max_val = rule.get('max_val')
                        
                        if min_val and val < float(min_val):
                            alerts.append(f"ğŸš¨ {tag}: {value} (ìµœì†Œê°’ {min_val} ë¯¸ë§Œ)")
                        elif max_val and val > float(max_val):
                            alerts.append(f"ğŸš¨ {tag}: {value} (ìµœëŒ€ê°’ {max_val} ì´ˆê³¼)")
                            
                    except (ValueError, TypeError):
                        continue
                        
        if alerts:
            response = "âš ï¸ **ê²½ê³  ìƒíƒœ ê°ì§€**:\n\n"
            for alert in alerts[:5]:
                response += f"{alert}\n"
                
            # ê´€ë ¨ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ì§€ì‹ ì¶”ê°€
            troubleshooting = [k for k in context["relevant_knowledge"] 
                             if k.get('content_type') == 'troubleshooting']
            if troubleshooting:
                response += "\nğŸ”§ **ê¶Œì¥ ì¡°ì¹˜**:\n"
                for item in troubleshooting[:2]:
                    response += f"â€¢ {item['content']}\n"
                    
            return response
        else:
            return "âœ… í˜„ì¬ ëª¨ë“  ì„¼ì„œê°€ ì •ìƒ ë²”ìœ„ ë‚´ì— ìˆìŠµë‹ˆë‹¤."
            
    def _format_help_response(self) -> str:
        """ë„ì›€ë§ ì‘ë‹µ"""
        return """
ğŸ¤– **AI ì„¼ì„œ ì–´ì‹œìŠ¤í„´íŠ¸ ì‚¬ìš©ë²•**

ë‹¤ìŒê³¼ ê°™ì€ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”:

ğŸ“Š **ìƒíƒœ ì¡°íšŒ**:
â€¢ "D101 ì„¼ì„œ í˜„ì¬ ìƒíƒœëŠ”?"
â€¢ "ì „ì²´ ì„¼ì„œ ìƒíƒœ ì•Œë ¤ì¤˜"

âš ï¸ **ë¬¸ì œ ì§„ë‹¨**: 
â€¢ "ê²½ê³  ìƒíƒœì¸ ì„¼ì„œ ìˆì–´?"
â€¢ "D100 ì˜¨ë„ê°€ ë†’ì€ ì´ìœ ê°€ ë­ì•¼?"

ğŸ“ˆ **ë¶„ì„ ìš”ì²­**:
â€¢ "ì–´ì œì™€ ë¹„êµí•´ì„œ ì–´ë–¤ ì„¼ì„œê°€ ë³€í–ˆì–´?"
â€¢ "D101ê³¼ D102 ìƒê´€ê´€ê³„ ë¶„ì„í•´ì¤˜"

ğŸ”§ **ìœ ì§€ë³´ìˆ˜**:
â€¢ "ì •ê¸° ì ê²€ ì¼ì • ì•Œë ¤ì¤˜" 
â€¢ "ì„¼ì„œ êµì²´ í›„ ì£¼ì˜ì‚¬í•­ì€?"

ìì—°ì–´ë¡œ í¸í•˜ê²Œ ì§ˆë¬¸í•˜ì„¸ìš”! ğŸš€
        """
        
    async def _generate_contextual_response(self, query: str, context: Dict[str, Any]) -> str:
        """OpenAI ê¸°ë°˜ ì§€ëŠ¥í˜• ì‘ë‹µ ìƒì„±"""
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ìˆìœ¼ë©´ ì‹¤ì œ LLM ì‚¬ìš©
        if self.openai_client:
            return await self._generate_openai_response(query, context)
        
        # Fallback: ê·œì¹™ ê¸°ë°˜ ì‘ë‹µ
        return await self._generate_template_response(query, context)
    
    async def _generate_openai_response(self, query: str, context: Dict[str, Any]) -> str:
        """OpenAI GPTë¥¼ ì‚¬ìš©í•œ ì‹¤ì œ AI ì‘ë‹µ ìƒì„±"""
        
        # ì»¨í…ìŠ¤íŠ¸ ê²€ì¦ ë° ë¡œê¹…
        has_data = bool(context.get("current_data"))
        has_qc = bool(context.get("qc_rules"))
        has_knowledge = bool(context.get("relevant_knowledge"))
        
        print(f"ğŸ” RAG ì»¨í…ìŠ¤íŠ¸ ìƒíƒœ:")
        print(f"   - sensor_tags: {len(context.get('sensor_tags', []))}ê°œ")
        print(f"   - current_data: {len(context.get('current_data', []))}ê°œ")
        print(f"   - qc_rules: {len(context.get('qc_rules', []))}ê°œ")
        print(f"   - relevant_knowledge: {len(context.get('relevant_knowledge', []))}ê°œ")
        
        # ë°ì´í„°ê°€ ì „í˜€ ì—†ìœ¼ë©´ ì•ˆì „í•œ ì‘ë‹µ ë°˜í™˜
        if not has_data and not has_qc and not has_knowledge:
            print("âš ï¸ RAG ì»¨í…ìŠ¤íŠ¸ ë¹„ì–´ìˆìŒ - ë°ì´í„° ì—†ìŒ ì‘ë‹µ")
            return ("ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì— ë¬¸ì œê°€ ìˆê±°ë‚˜ ê´€ë ¨ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"
                   "ë‹¤ìŒì„ í™•ì¸í•´ì£¼ì„¸ìš”:\n"
                   "â€¢ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒíƒœ\n"
                   "â€¢ ì„¼ì„œëª…ì´ ì •í™•í•œì§€ (ì˜ˆ: D100, D101)\n"
                   "â€¢ ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ì´ ì •ìƒ ì‘ë™ ì¤‘ì¸ì§€")
        
        try:
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„± - í™˜ê° ë°©ì§€ ê°•í™”
            system_prompt = """ë‹¹ì‹ ì€ ì‚°ì—…ìš© ì„¼ì„œ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì˜ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì¤‘ìš” ê·œì¹™:
1. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
2. ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ìˆ˜ì¹˜, ì„ê³„ê°’, í’ˆì§ˆì ìˆ˜ë¥¼ ì ˆëŒ€ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”
3. í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” "ë°ì´í„° ì—†ìŒ" ë˜ëŠ” "í™•ì¸ ë¶ˆê°€"ë¡œ ì‘ë‹µí•˜ì„¸ìš”
4. ìˆ«ìëŠ” ë°˜ë“œì‹œ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì œê³µëœ ê°’ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”

ì‘ë‹µ ì§€ì¹¨:
- í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê²Œ ë‹µë³€
- ì •í™•í•œ ë°ì´í„° ê¸°ë°˜ ë‹µë³€
- ì¶”ì¸¡ì´ë‚˜ ê°€ì • ê¸ˆì§€
- ì´ëª¨ì§€ëŠ” êµ¬ì¡°í™”ë¥¼ ìœ„í•´ì„œë§Œ ì‚¬ìš©"""

            # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ êµ¬ì„±
            context_info = []
            
            if context.get("current_data"):
                context_info.append("=== í˜„ì¬ ì„¼ì„œ ë°ì´í„° ===")
                for data in context["current_data"][:5]:
                    context_info.append(f"- {data.get('tag_name', 'Unknown')}: {data.get('value', 'N/A')} (ì—…ë°ì´íŠ¸: {data.get('ts', 'N/A')})")
            
            if context.get("relevant_knowledge"):
                context_info.append("\n=== ê´€ë ¨ ì „ë¬¸ ì§€ì‹ ===")
                for knowledge in context["relevant_knowledge"][:3]:
                    context_info.append(f"- [{knowledge.get('content_type', 'info')}] {knowledge.get('content', '')[:200]}...")
            
            if context.get("qc_violations"):
                context_info.append("\n=== QC ìœ„ë°˜ ì‚¬í•­ ===")
                for violation in context["qc_violations"][:3]:
                    context_info.append(f"- {violation}")
            
            user_prompt = f"""ì§ˆë¬¸: {query}

ì»¨í…ìŠ¤íŠ¸ ì •ë³´:
{chr(10).join(context_info) if context_info else 'í˜„ì¬ ì´ìš© ê°€ëŠ¥í•œ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.'}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."""

            # OpenAI API í˜¸ì¶œ - temperature ë‚®ì¶¤ (í™˜ê° ë°©ì§€)
            response = await self.openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=800,
                temperature=0.2  # 0.7 â†’ 0.2ë¡œ ë‚®ì¶¤ (í™˜ê° ê°ì†Œ)
            )
            
            ai_response = response.choices[0].message.content
            
            # í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ ê²€ì¦
            if self.hallucination_prevention:
                # ì°¸ì¡°í•œ ì§€ì‹ ë² ì´ìŠ¤ ID ì¶”ì¶œ
                kb_ids = [k.get('id') for k in context.get('relevant_knowledge', []) if k.get('id')]
                
                # ì‘ë‹µ ê²€ì¦
                validation_result = await self.hallucination_prevention.validate_response(
                    ai_response, 
                    context,
                    kb_ids
                )
                
                # ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ ê²½ê³  ì¶”ê°€
                if validation_result.confidence < 0.7:
                    ai_response = await self.hallucination_prevention.enhance_response_with_disclaimer(
                        ai_response,
                        validation_result
                    )
                    
                    # ë¡œê·¸ ê¸°ë¡
                    print(f"âš ï¸ í• ë£¨ì‹œë„¤ì´ì…˜ ê²€ì¦ - ì‹ ë¢°ë„: {validation_result.confidence:.2f}")
                    if validation_result.issues:
                        print(f"   ì´ìŠˆ: {', '.join(validation_result.issues)}")
            
            return ai_response
            
        except Exception as e:
            print(f"OpenAI ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return await self._generate_template_response(query, context)
    
    async def _generate_template_response(self, query: str, context: Dict[str, Any]) -> str:
        """í…œí”Œë¦¿ ê¸°ë°˜ Fallback ì‘ë‹µ"""
        
        # ì§€ì‹ ê¸°ë°˜ ì‘ë‹µ ìƒì„±
        if context.get("relevant_knowledge"):
            response = "ğŸ§  **ê´€ë ¨ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ë“œë¦½ë‹ˆë‹¤**:\n\n"
            
            for knowledge in context["relevant_knowledge"][:3]:
                content = knowledge['content']
                content_type = knowledge.get('content_type', '')
                similarity = knowledge.get('similarity_score', 0)
                
                type_emoji = {
                    'sensor_spec': 'ğŸ“Š',
                    'troubleshooting': 'ğŸ”§', 
                    'maintenance': 'âš™ï¸',
                    'operational_pattern': 'ğŸ“ˆ',
                    'correlation': 'ğŸ”—'
                }.get(content_type, 'ğŸ’¡')
                
                response += f"{type_emoji} **{content_type.title()}** (ê´€ë ¨ë„: {similarity:.1%})\n"
                response += f"{content}\n\n"
                
            # í˜„ì¬ ë°ì´í„°ë„ í¬í•¨
            if context.get("current_data"):
                response += "ğŸ“Š **í˜„ì¬ ë°ì´í„° ì°¸ê³ **:\n"
                for data in context["current_data"][:3]:
                    tag = data.get('tag_name', '')
                    value = data.get('value', '')
                    response += f"â€¢ {tag}: {value}\n"
                    
            return response
            
        else:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”."


# ì „ì—­ RAG ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤
rag_engine = RAGEngine()


async def initialize_rag_engine():
    """RAG ì—”ì§„ ì´ˆê¸°í™”"""
    await rag_engine.initialize()


async def get_rag_response(query: str) -> str:
    """RAG ê¸°ë°˜ ì‘ë‹µ ìƒì„±"""
    return await rag_engine.generate_response(query)