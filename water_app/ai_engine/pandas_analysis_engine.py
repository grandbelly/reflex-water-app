"""
ğŸ“Š íŒë‹¤ìŠ¤ ê¸°ë°˜ ë™ì  ë°ì´í„° ë¶„ì„ ì—”ì§„
íˆíŠ¸ë§µ, ìƒê´€ì„±, ì˜ˆì¸¡ ë¶„ì„ê¹Œì§€ í¬í•¨í•œ ì™„ì „í•œ ë°ì´í„° ë¶„ì„ ì‹œìŠ¤í…œ
"""

import asyncio
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats
from scipy.stats import pearsonr, spearmanr
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, field
import warnings
import io
import base64
import json
import logging

# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.ensemble import RandomForestRegressor, IsolationForest
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
import xgboost as xgb
import lightgbm as lgb
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.seasonal import seasonal_decompose

from water_app.db import q

warnings.filterwarnings('ignore')

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)


@dataclass
class AnalysisResult:
    """ë¶„ì„ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    analysis_type: str
    title: str
    description: str
    
    # ë°ì´í„°
    dataframe: Optional[pd.DataFrame] = None
    summary_stats: Dict[str, Any] = field(default_factory=dict)
    
    # ì‹œê°í™”
    chart_data: Dict[str, Any] = field(default_factory=dict)
    heatmap_data: Optional[Dict] = None
    
    # ë¶„ì„ ê²°ê³¼
    insights: List[str] = field(default_factory=list)
    correlations: Dict[str, float] = field(default_factory=dict)
    predictions: Dict[str, Any] = field(default_factory=dict)
    anomalies: List[Dict] = field(default_factory=list)
    
    # ë©”íƒ€ë°ì´í„°
    analysis_time: datetime = field(default_factory=datetime.now)
    confidence_score: float = 0.0
    data_quality_score: float = 0.0


class PandasAnalysisEngine:
    """ğŸ“Š íŒë‹¤ìŠ¤ ê¸°ë°˜ ë™ì  ë°ì´í„° ë¶„ì„ ì—”ì§„"""
    
    def __init__(self):
        self.name = "PandasAnalysisEngine"
        
        # ë¶„ì„ ì„¤ì •
        self.default_lookback_hours = 168  # 1ì£¼ì¼
        self.min_data_points = 30          # ìµœì†Œ ë°ì´í„° í¬ì¸íŠ¸ (100 -> 30ìœ¼ë¡œ ì™„í™”)
        self.correlation_threshold = 0.3   # ìƒê´€ì„± ì„ê³„ê°’
        self.anomaly_threshold = 2.0       # ì´ìƒì¹˜ ì„ê³„ê°’ (í‘œì¤€í¸ì°¨)
        
        # ì‹œê°í™” ì„¤ì •
        plt.style.use('default')
        sns.set_palette("husl")
    
    async def analyze_sensor_data(
        self,
        sensors: List[str],
        analysis_type: str,
        hours: int = 168,
        **kwargs
    ) -> AnalysisResult:
        """ë™ì  ì„¼ì„œ ë°ì´í„° ë¶„ì„"""
        
        print(f"[ANALYSIS] {analysis_type} - sensors: {sensors}, hours: {hours}")
        
        # 1. ë°ì´í„° ìˆ˜ì§‘
        df = await self._collect_sensor_data(sensors, hours)
        
        if df.empty or len(df) < self.min_data_points:
            return AnalysisResult(
                analysis_type=analysis_type,
                title="ë°ì´í„° ë¶€ì¡±",
                description=f"ë¶„ì„ì— ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (ìµœì†Œ {self.min_data_points}ê°œ í•„ìš”)",
                insights=["ë°ì´í„° ìˆ˜ì§‘ ê¸°ê°„ì„ ëŠ˜ë¦¬ê±°ë‚˜ ì„¼ì„œë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”."]
            )
        
        print(f"[DONE] Data collected - {len(df)} rows x {len(df.columns)} columns")
        
        # 2. ë¶„ì„ íƒ€ì…ë³„ ì²˜ë¦¬
        if analysis_type == "correlation":
            return await self._analyze_correlations(df, sensors, hours)
        elif analysis_type == "heatmap":
            return await self._analyze_heatmaps(df, sensors, hours)
        elif analysis_type == "prediction":
            return await self._analyze_predictions(df, sensors, hours, **kwargs)
        elif analysis_type == "anomaly":
            return await self._analyze_anomalies(df, sensors, hours)
        elif analysis_type == "comprehensive":
            return await self._comprehensive_analysis(df, sensors, hours)
        else:
            return await self._basic_statistical_analysis(df, sensors, hours)
    
    async def _collect_sensor_data(self, sensors: List[str], hours: int) -> pd.DataFrame:
        """ì„¼ì„œ ë°ì´í„° ìˆ˜ì§‘ ë° DataFrame ë³€í™˜ - í’ˆì§ˆ ê°œì„ """
        
        logger.info(f"[INFO] Data collection start - sensors: {sensors}, hours: {hours}")
        
        end_time = datetime.now()
        start_time = end_time - timedelta(hours=hours)
        
        logger.debug(f"ì‹œê°„ ë²”ìœ„: {start_time} ~ {end_time}")
        
        # SQL ì¿¼ë¦¬ë¡œ ë°ì´í„° ìˆ˜ì§‘ - í’ˆì§ˆ í•„í„° ì¶”ê°€
        sensor_placeholders = ','.join(['%s'] * len(sensors))
        query = f"""
        SELECT 
            ts,
            tag_name,
            value
        FROM public.influx_hist 
        WHERE tag_name IN ({sensor_placeholders})
        AND ts >= %s 
        AND ts <= %s
        AND value IS NOT NULL
        -- ì„¼ì„œë³„ ìœ íš¨ ë²”ìœ„ ì ìš© (D1xx: -50~500, D2xx: -100~5000, D3xx: -1000~30000)
        AND (
            (tag_name LIKE 'D1%%' AND value >= -50 AND value <= 500) OR
            (tag_name LIKE 'D2%%' AND value >= -100 AND value <= 5000) OR
            (tag_name LIKE 'D3%%' AND value >= -1000 AND value <= 30000) OR
            (tag_name NOT LIKE 'D%%' AND value > 0 AND value < 10000)
        )
        ORDER BY ts, tag_name
        """
        
        logger.debug(f"SQL ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°: ì„¼ì„œ={sensors}, ì‹œì‘={start_time}, ì¢…ë£Œ={end_time}")
        
        try:
            result = await q(query, sensors + [start_time, end_time])
            
            if not result:
                logger.warning(f"ì¿¼ë¦¬ ê²°ê³¼ ì—†ìŒ - ì„¼ì„œ: {sensors}")
                return pd.DataFrame()
            
            logger.info(f"ì¿¼ë¦¬ ê²°ê³¼: {len(result)}ê°œ ë ˆì½”ë“œ ë°˜í™˜")
            
            # DataFrame ìƒì„±
            data = []
            sensor_value_ranges = {}  # ì„¼ì„œë³„ ê°’ ë²”ìœ„ ì¶”ì 
            
            for row in result:
                sensor_name = row['tag_name']
                value = float(row['value'])
                
                # ì„¼ì„œë³„ ê°’ ë²”ìœ„ ì¶”ì 
                if sensor_name not in sensor_value_ranges:
                    sensor_value_ranges[sensor_name] = {'min': value, 'max': value, 'count': 0}
                sensor_value_ranges[sensor_name]['min'] = min(sensor_value_ranges[sensor_name]['min'], value)
                sensor_value_ranges[sensor_name]['max'] = max(sensor_value_ranges[sensor_name]['max'], value)
                sensor_value_ranges[sensor_name]['count'] += 1
                
                data.append({
                    'timestamp': row['ts'],
                    'sensor': sensor_name,
                    'value': value
                })
            
            # ì„¼ì„œë³„ ê°’ ë²”ìœ„ ë¡œê¹…
            logger.info(f"[INFO] Sensor value ranges:")
            for sensor, ranges in sensor_value_ranges.items():
                logger.info(f"   - {sensor}: min={ranges['min']:.2f}, max={ranges['max']:.2f}, count={ranges['count']}")
            
            df = pd.DataFrame(data)
            
            # íƒ€ì„ìŠ¤íƒ¬í”„ ì¸ë±ìŠ¤ ì„¤ì •
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            
            print(f"[INFO] Collected data info:")
            print(f"   - ì´ ë ˆì½”ë“œ: {len(df)}ê°œ")
            print(f"   - ì„¼ì„œë³„ ë°ì´í„°: {df.groupby('sensor').size().to_dict()}")
            print(f"   - ì‹œê°„ ë²”ìœ„: {df['timestamp'].min()} ~ {df['timestamp'].max()}")
            
            logger.info(f"DataFrame ìƒì„± ì™„ë£Œ: {len(df)}ê°œ ë ˆì½”ë“œ, {df['sensor'].nunique()}ê°œ ì„¼ì„œ")
            
            # í”¼ë²— í…Œì´ë¸” ìƒì„± ì „ì— ë°ì´í„°ê°€ ì¶©ë¶„í•œì§€ í™•ì¸
            sensor_counts = df.groupby('sensor').size()
            # ìµœì†Œ ë°ì´í„° í¬ì¸íŠ¸ ìš”êµ¬ì‚¬í•­ ì™„í™” (10 -> 3)
            MIN_DATA_POINTS = 3  # ìƒê´€ë¶„ì„ì—ëŠ” ìµœì†Œ 3ê°œë©´ ì¶©ë¶„
            sufficient_sensors = sensor_counts[sensor_counts >= MIN_DATA_POINTS].index.tolist()
            
            print(f"[INFO] Sensor data points check:")
            for sensor, count in sensor_counts.items():
                status = "[OK]" if count >= MIN_DATA_POINTS else "[FAIL]"
                print(f"   {status} {sensor}: {count}ê°œ (ìµœì†Œ {MIN_DATA_POINTS}ê°œ í•„ìš”)")
                logger.debug(f"ì„¼ì„œ {sensor}: {count}ê°œ ë°ì´í„° í¬ì¸íŠ¸ {'(ì¶©ë¶„)' if count >= MIN_DATA_POINTS else '(ë¶€ì¡±)'}")
            
            if not sufficient_sensors:
                print("[ERROR] Not enough data for analysis")
                logger.error(f"ëª¨ë“  ì„¼ì„œê°€ ìµœì†Œ ë°ì´í„° í¬ì¸íŠ¸ ìš”êµ¬ì‚¬í•­({MIN_DATA_POINTS}ê°œ)ì„ ë§Œì¡±í•˜ì§€ ëª»í•¨")
                return pd.DataFrame()
            
            # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆëŠ” ì„¼ì„œë§Œ ì‚¬ìš©
            df_filtered = df[df['sensor'].isin(sufficient_sensors)]
            logger.info(f"í•„í„°ë§ í›„: {len(df_filtered)}ê°œ ë ˆì½”ë“œ, ì„¼ì„œ: {sufficient_sensors}")
            
            # í”¼ë²— í…Œì´ë¸”ë¡œ ì„¼ì„œë³„ ì»¬ëŸ¼ ìƒì„±
            pivot_df = df_filtered.pivot_table(
                index='timestamp', 
                columns='sensor', 
                values='value',
                aggfunc='mean'  # ê°™ì€ ì‹œê°„ì— ì—¬ëŸ¬ ê°’ì´ ìˆìœ¼ë©´ í‰ê· 
            )
            
            print(f"[INFO] After pivot:")
            print(f"   - Shape: {pivot_df.shape}")
            print(f"   - ì„¼ì„œ: {list(pivot_df.columns)}")
            print(f"   - ê²°ì¸¡ì¹˜ ë¹„ìœ¨: {pivot_df.isnull().sum().sum() / (pivot_df.shape[0] * pivot_df.shape[1]):.3f}")
            
            logger.info(f"í”¼ë²— í…Œì´ë¸” ìƒì„±: shape={pivot_df.shape}, ì»¬ëŸ¼={list(pivot_df.columns)}")
            
            # ê²°ì¸¡ì¹˜ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ë³´ê°„ ìŠ¤í‚µ
            missing_ratio = pivot_df.isnull().sum().sum() / (pivot_df.shape[0] * pivot_df.shape[1])
            if missing_ratio > 0.5:
                print("[WARNING] Missing values > 50%, skipping interpolation")
                logger.warning(f"ë†’ì€ ê²°ì¸¡ì¹˜ ë¹„ìœ¨ ({missing_ratio:.1%}), ë³´ê°„ ìŠ¤í‚µ")
            else:
                # í•„ìš”í•œ ê²½ìš°ì—ë§Œ ê°„ë‹¨í•œ ë³´ê°„
                pivot_df = pivot_df.fillna(method='ffill').fillna(method='bfill')
                logger.debug("ê²°ì¸¡ì¹˜ ë³´ê°„ ì™„ë£Œ (forward fill + backward fill)")
            
            # ì‹œê°„ ê´€ë ¨ íŠ¹ì„± ì¶”ê°€
            result_df = pivot_df.reset_index()
            result_df['hour'] = result_df['timestamp'].dt.hour
            result_df['day_of_week'] = result_df['timestamp'].dt.dayofweek
            result_df['is_weekend'] = result_df['day_of_week'].isin([5, 6])
            
            logger.info(f"[SUCCESS] Final DataFrame: shape={result_df.shape}, sensors={[c for c in result_df.columns if c not in ['timestamp', 'hour', 'day_of_week', 'is_weekend']]}")
            
            return result_df
            
        except Exception as e:
            print(f"[ERROR] Data collection error: {e}")
            logger.error(f"ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return pd.DataFrame()
    
    async def _analyze_correlations(self, df: pd.DataFrame, sensors: List[str], hours: int) -> AnalysisResult:
        """ì„¼ì„œ ê°„ ìƒê´€ì„± ë¶„ì„"""
        
        print(f"[START] Correlation analysis:")
        print(f"   - Requested sensors: {sensors}")
        print(f"   - DataFrame columns: {list(df.columns)}")
        
        # DataFrameì— ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì„¼ì„œë§Œ ì¶”ì¶œ
        available_sensors = [s for s in sensors if s in df.columns]
        print(f"   - Available sensors: {available_sensors}")
        
        if len(available_sensors) < 2:
            print("[ERROR] Not enough sensors for correlation analysis (need at least 2)")
            return AnalysisResult(
                analysis_type="correlation",
                title="ìƒê´€ê´€ê³„ ë¶„ì„ ì‹¤íŒ¨",
                description="ë¶„ì„ì— í•„ìš”í•œ ì„¼ì„œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤",
                dataframe=df,
                insights=["ì„¼ì„œ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"],
                confidence_score=0.0,
                data_quality_score=0.0
            )
        
        # ì„¼ì„œ ë°ì´í„°ë§Œ ì¶”ì¶œ
        sensor_data = df[available_sensors].select_dtypes(include=[np.number])
        
        # NaN ê°’ ì²˜ë¦¬ - ë°ì´í„° í’ˆì§ˆ í™•ì¸
        print(f"[INFO] Data quality check:")
        for sensor in available_sensors:
            null_count = sensor_data[sensor].isnull().sum()
            total_count = len(sensor_data[sensor])
            null_pct = (null_count / total_count) * 100 if total_count > 0 else 100
            print(f"   - {sensor}: {null_count}/{total_count} NaN ({null_pct:.1f}%)")
        
        # NaN ê°’ì´ ìˆëŠ” í–‰ ì œê±° (ìƒê´€ê³„ìˆ˜ ê³„ì‚°ì„ ìœ„í•´)
        sensor_data_clean = sensor_data.dropna()
        print(f"[INFO] After cleaning: {len(sensor_data)} -> {len(sensor_data_clean)} rows")
        
        if len(sensor_data_clean) < 5:
            print("[ERROR] Not enough valid data for correlation analysis (need at least 5 rows)")
            return AnalysisResult(
                analysis_type="correlation",
                title="ìƒê´€ê´€ê³„ ë¶„ì„ ì‹¤íŒ¨",
                description="NaN ì œê±° í›„ ìœ íš¨ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤",
                dataframe=df,
                insights=["ìœ íš¨í•œ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤", f"ì „ì²´ {len(sensor_data)}í–‰ ì¤‘ ìœ íš¨ {len(sensor_data_clean)}í–‰"],
                confidence_score=0.0,
                data_quality_score=len(sensor_data_clean) / len(sensor_data) if len(sensor_data) > 0 else 0.0
            )
        
        # í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ (NaNì´ ì—†ëŠ” ì •ì œëœ ë°ì´í„°ë¡œ ê³„ì‚°)
        pearson_corr = sensor_data_clean.corr(method='pearson')
        
        # ìŠ¤í”¼ì–´ë§Œ ìƒê´€ê³„ìˆ˜
        spearman_corr = sensor_data_clean.corr(method='spearman')
        
        # NaN ê°’ í™•ì¸
        pearson_nan_count = pearson_corr.isnull().sum().sum()
        if pearson_nan_count > 0:
            print(f"[WARNING] NaN found in correlation matrix: {pearson_nan_count}")
            # NaNì„ 0ìœ¼ë¡œ ì±„ìš°ê¸° (ëŒ€ê°ì„  ì™¸)
            pearson_corr = pearson_corr.fillna(0)
            spearman_corr = spearman_corr.fillna(0)
        
        print(f"[SUCCESS] Final correlation matrix shape: {pearson_corr.shape}")
        print(f"   - NaN ê°œìˆ˜: {pearson_corr.isnull().sum().sum()}")
        
        # ìƒê´€ì„± íˆíŠ¸ë§µ ë°ì´í„°
        heatmap_data = {
            'pearson': pearson_corr.round(3).to_dict(),
            'spearman': spearman_corr.round(3).to_dict(),
            'sensors': available_sensors
        }
        
        print(f"[SUCCESS] Correlation heatmap data created:")
        print(f"   - Pearson ìƒê´€ê³„ìˆ˜ í¬ê¸°: {pearson_corr.shape}")
        print(f"   - ì„¼ì„œ: {available_sensors}")
        print(f"   - ìƒ˜í”Œ ìƒê´€ê³„ìˆ˜: {pearson_corr.iloc[0, 1] if pearson_corr.shape[0] > 1 else 'N/A'}")
        
        # ë†’ì€ ìƒê´€ì„± ìŒ ì°¾ê¸° (available_sensors ê¸°ì¤€ìœ¼ë¡œ ìˆ˜ì •)
        correlations = {}
        strong_correlations = []
        
        for i in range(len(available_sensors)):
            for j in range(i+1, len(available_sensors)):
                sensor1, sensor2 = available_sensors[i], available_sensors[j]
                if sensor1 in pearson_corr.index and sensor2 in pearson_corr.columns:
                    corr_val = pearson_corr.loc[sensor1, sensor2]
                    # NaN ì²´í¬ ë° ì²˜ë¦¬
                    if pd.isna(corr_val):
                        print(f"[WARNING] {sensor1}-{sensor2} correlation is NaN")
                        corr_val = 0.0
                    
                    correlations[f"{sensor1}-{sensor2}"] = corr_val
                    
                    if abs(corr_val) > self.correlation_threshold:
                        strong_correlations.append({
                            'sensor1': sensor1,
                            'sensor2': sensor2,
                            'correlation': corr_val,
                            'strength': 'strong' if abs(corr_val) > 0.7 else 'moderate'
                        })
        
        # ì¸ì‚¬ì´íŠ¸ ìƒì„±
        insights = []
        if strong_correlations:
            insights.append(f"ì´ {len(strong_correlations)}ê°œì˜ ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„ ë°œê²¬")
            
            # ê°€ì¥ ë†’ì€ ìƒê´€ê´€ê³„
            highest = max(strong_correlations, key=lambda x: abs(x['correlation']))
            insights.append(f"ê°€ì¥ ë†’ì€ ìƒê´€ê´€ê³„: {highest['sensor1']} â†” {highest['sensor2']} ({highest['correlation']:.3f})")
            
            # ì–‘ì˜ ìƒê´€ê´€ê³„
            positive_corrs = [c for c in strong_correlations if c['correlation'] > 0]
            if positive_corrs:
                insights.append(f"ì–‘ì˜ ìƒê´€ê´€ê³„: {len(positive_corrs)}ê°œ (ë™ì‹œ ì¦ê° ê²½í–¥)")
            
            # ìŒì˜ ìƒê´€ê´€ê³„  
            negative_corrs = [c for c in strong_correlations if c['correlation'] < 0]
            if negative_corrs:
                insights.append(f"ìŒì˜ ìƒê´€ê´€ê³„: {len(negative_corrs)}ê°œ (ë°˜ëŒ€ ë³€í™” ê²½í–¥)")
        else:
            insights.append("ì„¼ì„œ ê°„ ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„ê°€ ë°œê²¬ë˜ì§€ ì•ŠìŒ")
            insights.append("ê° ì„¼ì„œê°€ ë…ë¦½ì ìœ¼ë¡œ ë™ì‘í•˜ëŠ” ê²ƒìœ¼ë¡œ ì¶”ì •")
        
        # ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        data_quality = len(sensor_data_clean) / len(sensor_data) if len(sensor_data) > 0 else 0.0
        confidence = min(1.0, len(sensor_data_clean) / 100) * (1 - pearson_nan_count / (len(available_sensors) ** 2))
        
        return AnalysisResult(
            analysis_type="correlation",
            title=f"ì„¼ì„œ ìƒê´€ì„± ë¶„ì„ ({hours}ì‹œê°„)",
            description=f"{len(available_sensors)}ê°œ ì„¼ì„œ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„ (ìœ íš¨ ë°ì´í„° {len(sensor_data_clean)}í–‰)",
            dataframe=df,
            heatmap_data=heatmap_data,
            correlations=correlations,
            insights=insights,
            confidence_score=confidence,
            data_quality_score=data_quality
        )
    
    async def _analyze_heatmaps(self, df: pd.DataFrame, sensors: List[str], hours: int) -> AnalysisResult:
        """ë‹¤ì°¨ì› íˆíŠ¸ë§µ ë¶„ì„ (ì‹œê°„ íŒ¨í„´, ì„¼ì„œ ê´€ê³„ ë“±)"""
        
        # 1. ì‹œê°„ëŒ€ë³„ íˆíŠ¸ë§µ (24ì‹œê°„ x ì„¼ì„œ)
        df['hour'] = df['timestamp'].dt.hour
        hourly_heatmap = df.groupby('hour')[sensors].mean()
        
        # 2. ìš”ì¼ë³„ íˆíŠ¸ë§µ (7ì¼ x ì„¼ì„œ)  
        df['day_of_week'] = df['timestamp'].dt.day_name()
        daily_heatmap = df.groupby('day_of_week')[sensors].mean()
        
        # 3. ì„¼ì„œ ê°„ ìƒê´€ì„± íˆíŠ¸ë§µ
        correlation_heatmap = df[sensors].corr()
        
        # 4. ì´ìƒì¹˜ íˆíŠ¸ë§µ (Z-score ê¸°ë°˜)
        z_scores = np.abs(stats.zscore(df[sensors]))
        anomaly_heatmap = (z_scores > 2).astype(int)  # ì´ìƒì¹˜ë¥¼ 1ë¡œ í‘œì‹œ
        
        # íˆíŠ¸ë§µ ë°ì´í„° êµ¬ì„±
        heatmap_data = {
            'hourly': {
                'data': hourly_heatmap.round(2).to_dict(),
                'index': list(range(24)),
                'columns': sensors,
                'title': 'ì‹œê°„ëŒ€ë³„ ì„¼ì„œ í‰ê· ê°’'
            },
            'daily': {
                'data': daily_heatmap.round(2).to_dict(),
                'index': ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'],
                'columns': sensors,
                'title': 'ìš”ì¼ë³„ ì„¼ì„œ í‰ê· ê°’'
            },
            'correlation': {
                'data': correlation_heatmap.round(3).to_dict(),
                'index': sensors,
                'columns': sensors,
                'title': 'ì„¼ì„œ ê°„ ìƒê´€ì„±'
            },
            'anomaly': {
                'data': {sensor: int(anomaly_heatmap[sensor].sum()) for sensor in sensors},
                'index': ['ì´ìƒì¹˜ ê°œìˆ˜'],
                'columns': sensors,
                'title': 'ì„¼ì„œë³„ ì´ìƒì¹˜ ë°œìƒ ë¹ˆë„'
            }
        }
        
        # ì‹œê°„ íŒ¨í„´ ì¸ì‚¬ì´íŠ¸
        insights = []
        
        # ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ë¶„ì„
        peak_hours = {}
        for sensor in sensors:
            if sensor in hourly_heatmap.columns:
                peak_hour = hourly_heatmap[sensor].idxmax()
                min_hour = hourly_heatmap[sensor].idxmin()
                peak_hours[sensor] = {'peak': peak_hour, 'min': min_hour}
                insights.append(f"{sensor}: {peak_hour}ì‹œ ìµœëŒ€, {min_hour}ì‹œ ìµœì†Œ")
        
        # ìš”ì¼ë³„ íŒ¨í„´ ë¶„ì„
        weekend_avg = daily_heatmap.loc[['Saturday', 'Sunday'], sensors].mean()
        weekday_avg = daily_heatmap.loc[['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'], sensors].mean()
        
        for sensor in sensors:
            if sensor in weekend_avg.index:
                weekend_val = weekend_avg[sensor]
                weekday_val = weekday_avg[sensor]
                ratio = weekend_val / weekday_val if weekday_val != 0 else 1
                
                if ratio > 1.1:
                    insights.append(f"{sensor}: ì£¼ë§ì´ í‰ì¼ë³´ë‹¤ {(ratio-1)*100:.1f}% ë†’ìŒ")
                elif ratio < 0.9:
                    insights.append(f"{sensor}: ì£¼ë§ì´ í‰ì¼ë³´ë‹¤ {(1-ratio)*100:.1f}% ë‚®ìŒ")
        
        return AnalysisResult(
            analysis_type="heatmap",
            title=f"ë‹¤ì°¨ì› íˆíŠ¸ë§µ ë¶„ì„ ({hours}ì‹œê°„)",
            description="ì‹œê°„ë³„, ìš”ì¼ë³„, ìƒê´€ì„±, ì´ìƒì¹˜ íˆíŠ¸ë§µ ì¢…í•© ë¶„ì„",
            dataframe=df,
            heatmap_data=heatmap_data,
            insights=insights,
            confidence_score=0.85,
            data_quality_score=min(1.0, len(df) / (hours * 60))
        )
    
    async def _analyze_predictions(self, df: pd.DataFrame, sensors: List[str], hours: int, **kwargs) -> AnalysisResult:
        """ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì˜ˆì¸¡ ë¶„ì„"""
        
        target_sensor = kwargs.get('target_sensor', sensors[0])
        prediction_horizon = kwargs.get('prediction_horizon', 24)  # 24ì‹œê°„ ì˜ˆì¸¡
        
        if target_sensor not in df.columns:
            return AnalysisResult(
                analysis_type="prediction",
                title="ì˜ˆì¸¡ ì‹¤íŒ¨",
                description=f"íƒ€ê²Ÿ ì„¼ì„œ {target_sensor}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                insights=["ìœ íš¨í•œ ì„¼ì„œ ì´ë¦„ì„ ì§€ì •í•´ì£¼ì„¸ìš”"]
            )
        
        # íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§
        feature_df = df.copy()
        
        # ì‹œê°„ íŠ¹ì„±
        feature_df['hour_sin'] = np.sin(2 * np.pi * feature_df['hour'] / 24)
        feature_df['hour_cos'] = np.cos(2 * np.pi * feature_df['hour'] / 24)
        feature_df['day_sin'] = np.sin(2 * np.pi * feature_df['day_of_week'] / 7)
        feature_df['day_cos'] = np.cos(2 * np.pi * feature_df['day_of_week'] / 7)
        
        # ì§€ì—° íŠ¹ì„± (lag features)
        for lag in [1, 3, 6, 12, 24]:
            feature_df[f'{target_sensor}_lag_{lag}'] = feature_df[target_sensor].shift(lag)
        
        # ì´ë™í‰ê·  íŠ¹ì„±
        for window in [3, 6, 12, 24]:
            feature_df[f'{target_sensor}_ma_{window}'] = feature_df[target_sensor].rolling(window=window).mean()
        
        # ê²°ì¸¡ì¹˜ ì œê±°
        feature_df = feature_df.dropna()
        
        if len(feature_df) < 100:
            return AnalysisResult(
                analysis_type="prediction",
                title="ë°ì´í„° ë¶€ì¡±",
                description="ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµì— í•„ìš”í•œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤",
                insights=["ë” ê¸´ ê¸°ê°„ì˜ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤"]
            )
        
        # íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
        feature_columns = [col for col in feature_df.columns if col not in ['timestamp', target_sensor]]
        X = feature_df[feature_columns]
        y = feature_df[target_sensor]
        
        # í•™ìŠµ/ê²€ì¦ ë¶„í• 
        train_size = int(len(X) * 0.8)
        X_train, X_test = X[:train_size], X[train_size:]
        y_train, y_test = y[:train_size], y[train_size:]
        
        # ì—¬ëŸ¬ ëª¨ë¸ë¡œ ì˜ˆì¸¡
        models = {}
        predictions = {}
        
        # 1. Random Forest
        try:
            rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
            rf_model.fit(X_train, y_train)
            rf_pred = rf_model.predict(X_test)
            rf_score = r2_score(y_test, rf_pred)
            models['RandomForest'] = {'model': rf_model, 'score': rf_score, 'predictions': rf_pred}
        except Exception as e:
            print(f"RandomForest ëª¨ë¸ ì˜¤ë¥˜: {e}")
        
        # 2. XGBoost
        try:
            xgb_model = xgb.XGBRegressor(n_estimators=100, random_state=42)
            xgb_model.fit(X_train, y_train)
            xgb_pred = xgb_model.predict(X_test)
            xgb_score = r2_score(y_test, xgb_pred)
            models['XGBoost'] = {'model': xgb_model, 'score': xgb_score, 'predictions': xgb_pred}
        except Exception as e:
            print(f"XGBoost ëª¨ë¸ ì˜¤ë¥˜: {e}")
        
        # 3. Linear Regression
        try:
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            lr_model = LinearRegression()
            lr_model.fit(X_train_scaled, y_train)
            lr_pred = lr_model.predict(X_test_scaled)
            lr_score = r2_score(y_test, lr_pred)
            models['LinearRegression'] = {'model': lr_model, 'score': lr_score, 'predictions': lr_pred, 'scaler': scaler}
        except Exception as e:
            print(f"LinearRegression ëª¨ë¸ ì˜¤ë¥˜: {e}")
        
        # ìµœì  ëª¨ë¸ ì„ íƒ
        if not models:
            return AnalysisResult(
                analysis_type="prediction",
                title="ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨",
                description="ëª¨ë“  ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤",
                insights=["ë°ì´í„° í’ˆì§ˆì„ í™•ì¸í•´ì£¼ì„¸ìš”"]
            )
        
        best_model_name = max(models.keys(), key=lambda k: models[k]['score'])
        best_model = models[best_model_name]
        
        # ë¯¸ë˜ ì˜ˆì¸¡ ìƒì„±
        last_timestamp = df['timestamp'].max()
        future_timestamps = pd.date_range(
            start=last_timestamp + timedelta(hours=1),
            periods=prediction_horizon,
            freq='H'
        )
        
        # ë¯¸ë˜ ì˜ˆì¸¡ì„ ìœ„í•œ íŠ¹ì„± ìƒì„± (ë‹¨ìˆœí™”)
        future_predictions = []
        last_known_value = y.iloc[-1]
        
        for i in range(prediction_horizon):
            # ë‹¨ìˆœ íŠ¸ë Œë“œ ê¸°ë°˜ ì˜ˆì¸¡ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë¡œì§ í•„ìš”)
            trend = np.mean(np.diff(y.tail(24)))  # ìµœê·¼ 24ì‹œê°„ íŠ¸ë Œë“œ
            predicted_value = last_known_value + trend * (i + 1)
            future_predictions.append(predicted_value)
        
        # ì˜ˆì¸¡ ê²°ê³¼ êµ¬ì„±
        predictions = {
            'model_name': best_model_name,
            'model_score': best_model['score'],
            'test_mae': mean_absolute_error(y_test, best_model['predictions']),
            'test_rmse': np.sqrt(mean_squared_error(y_test, best_model['predictions'])),
            'future_predictions': [
                {
                    'timestamp': ts.isoformat(),
                    'predicted_value': pred,
                    'confidence': max(0.5, best_model['score'])  # ëª¨ë¸ ìŠ¤ì½”ì–´ ê¸°ë°˜ ì‹ ë¢°ë„
                }
                for ts, pred in zip(future_timestamps, future_predictions)
            ]
        }
        
        # ì¸ì‚¬ì´íŠ¸ ìƒì„±
        insights = []
        insights.append(f"ìµœì  ëª¨ë¸: {best_model_name} (RÂ² = {best_model['score']:.3f})")
        insights.append(f"ì˜ˆì¸¡ ì˜¤ì°¨: MAE = {predictions['test_mae']:.2f}")
        
        if best_model['score'] > 0.8:
            insights.append("ë†’ì€ ì˜ˆì¸¡ ì •í™•ë„ - ì‹ ë¢°í•  ë§Œí•œ ì˜ˆì¸¡")
        elif best_model['score'] > 0.6:
            insights.append("ë³´í†µ ì˜ˆì¸¡ ì •í™•ë„ - ì°¸ê³ ìš©ìœ¼ë¡œ í™œìš©")
        else:
            insights.append("ë‚®ì€ ì˜ˆì¸¡ ì •í™•ë„ - ì¶”ê°€ íŠ¹ì„± í•„ìš”")
        
        # íŠ¸ë Œë“œ ë¶„ì„
        trend = np.mean(future_predictions) - last_known_value
        if abs(trend) > 0.1:
            direction = "ìƒìŠ¹" if trend > 0 else "í•˜ë½"
            insights.append(f"{prediction_horizon}ì‹œê°„ í›„ {direction} íŠ¸ë Œë“œ ì˜ˆì¸¡")
        
        return AnalysisResult(
            analysis_type="prediction",
            title=f"{target_sensor} ì˜ˆì¸¡ ë¶„ì„ ({prediction_horizon}ì‹œê°„)",
            description=f"{best_model_name} ëª¨ë¸ ê¸°ë°˜ {target_sensor} ì„¼ì„œ ì˜ˆì¸¡",
            dataframe=feature_df,
            predictions=predictions,
            insights=insights,
            confidence_score=best_model['score'],
            data_quality_score=min(1.0, len(df) / (hours * 60))
        )
    
    async def _analyze_anomalies(self, df: pd.DataFrame, sensors: List[str], hours: int) -> AnalysisResult:
        """ì´ìƒì¹˜ íƒì§€ ë¶„ì„"""
        
        anomalies = []
        sensor_data = df[sensors].select_dtypes(include=[np.number])
        
        # 1. í†µê³„ì  ì´ìƒì¹˜ íƒì§€ (Z-score)
        z_scores = np.abs(stats.zscore(sensor_data))
        statistical_anomalies = []
        
        for sensor in sensors:
            if sensor in z_scores.columns:
                anomaly_mask = z_scores[sensor] > self.anomaly_threshold
                anomaly_indices = df[anomaly_mask].index.tolist()
                
                for idx in anomaly_indices:
                    statistical_anomalies.append({
                        'sensor': sensor,
                        'timestamp': df.loc[idx, 'timestamp'],
                        'value': df.loc[idx, sensor],
                        'z_score': z_scores.loc[idx, sensor],
                        'type': 'statistical',
                        'severity': 'high' if z_scores.loc[idx, sensor] > 3 else 'medium'
                    })
        
        # 2. ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€ (Isolation Forest)
        ml_anomalies = []
        try:
            iso_forest = IsolationForest(contamination=0.1, random_state=42)
            anomaly_labels = iso_forest.fit_predict(sensor_data)
            anomaly_scores = iso_forest.score_samples(sensor_data)
            
            anomaly_mask = anomaly_labels == -1
            anomaly_indices = df[anomaly_mask].index.tolist()
            
            for idx in anomaly_indices:
                # ê°€ì¥ ì´ìƒí•œ ì„¼ì„œ ì°¾ê¸°
                sensor_values = {sensor: abs(z_scores.loc[idx, sensor]) for sensor in sensors if sensor in z_scores.columns}
                most_anomalous_sensor = max(sensor_values.keys(), key=sensor_values.get) if sensor_values else sensors[0]
                
                ml_anomalies.append({
                    'sensor': most_anomalous_sensor,
                    'timestamp': df.loc[idx, 'timestamp'],
                    'value': df.loc[idx, most_anomalous_sensor],
                    'anomaly_score': anomaly_scores[idx],
                    'type': 'ml_isolation',
                    'severity': 'high' if anomaly_scores[idx] < -0.5 else 'medium'
                })
        except Exception as e:
            print(f"Isolation Forest ì˜¤ë¥˜: {e}")
        
        # ëª¨ë“  ì´ìƒì¹˜ í•©ì¹˜ê¸°
        all_anomalies = statistical_anomalies + ml_anomalies
        
        # ì‹œê°„ìˆœ ì •ë ¬ ë° ìƒìœ„ 20ê°œë§Œ ì„ íƒ
        all_anomalies.sort(key=lambda x: x['timestamp'], reverse=True)
        top_anomalies = all_anomalies[:20]
        
        # ì¸ì‚¬ì´íŠ¸ ìƒì„±
        insights = []
        insights.append(f"ì´ {len(all_anomalies)}ê°œì˜ ì´ìƒì¹˜ íƒì§€")
        
        if top_anomalies:
            # ì„¼ì„œë³„ ì´ìƒì¹˜ ê°œìˆ˜
            sensor_anomaly_counts = {}
            for anomaly in all_anomalies:
                sensor = anomaly['sensor']
                sensor_anomaly_counts[sensor] = sensor_anomaly_counts.get(sensor, 0) + 1
            
            worst_sensor = max(sensor_anomaly_counts.keys(), key=sensor_anomaly_counts.get)
            insights.append(f"ê°€ì¥ ë§ì€ ì´ìƒì¹˜: {worst_sensor} ({sensor_anomaly_counts[worst_sensor]}ê°œ)")
            
            # ì‹¬ê°ë„ë³„ ë¶„ë¥˜
            high_severity = len([a for a in all_anomalies if a['severity'] == 'high'])
            medium_severity = len([a for a in all_anomalies if a['severity'] == 'medium'])
            
            insights.append(f"ì‹¬ê°ë„ - ë†’ìŒ: {high_severity}ê°œ, ë³´í†µ: {medium_severity}ê°œ")
            
            # ìµœê·¼ ì´ìƒì¹˜
            recent_anomaly = top_anomalies[0]
            insights.append(f"ìµœê·¼ ì´ìƒì¹˜: {recent_anomaly['sensor']} at {recent_anomaly['timestamp']}")
        
        return AnalysisResult(
            analysis_type="anomaly",
            title=f"ì´ìƒì¹˜ íƒì§€ ë¶„ì„ ({hours}ì‹œê°„)",
            description=f"í†µê³„ì /ML ê¸°ë°˜ ì´ìƒì¹˜ {len(all_anomalies)}ê°œ íƒì§€",
            dataframe=df,
            anomalies=top_anomalies,
            insights=insights,
            confidence_score=0.8,
            data_quality_score=min(1.0, len(df) / (hours * 60))
        )
    
    async def _comprehensive_analysis(self, df: pd.DataFrame, sensors: List[str], hours: int) -> AnalysisResult:
        """ì¢…í•© ë¶„ì„ (ëª¨ë“  ë¶„ì„ íƒ€ì… ê²°í•©)"""
        
        results = []
        
        # ê° ë¶„ì„ ìˆ˜í–‰
        correlation_result = await self._analyze_correlations(df, sensors, hours)
        results.append(correlation_result)
        
        heatmap_result = await self._analyze_heatmaps(df, sensors, hours)  
        results.append(heatmap_result)
        
        anomaly_result = await self._analyze_anomalies(df, sensors, hours)
        results.append(anomaly_result)
        
        # ì˜ˆì¸¡ì€ ì²« ë²ˆì§¸ ì„¼ì„œë¡œ
        prediction_result = await self._analyze_predictions(df, sensors, hours, target_sensor=sensors[0])
        results.append(prediction_result)
        
        # ì¢…í•© ì¸ì‚¬ì´íŠ¸
        comprehensive_insights = []
        comprehensive_insights.append(f"=== ì¢…í•© ë¶„ì„ ê²°ê³¼ ({hours}ì‹œê°„) ===")
        
        for result in results:
            comprehensive_insights.extend([f"[{result.analysis_type.upper()}] {insight}" for insight in result.insights[:2]])
        
        # ì¢…í•© ë°ì´í„°
        comprehensive_data = {
            'correlation': correlation_result.heatmap_data,
            'heatmap': heatmap_result.heatmap_data,
            'prediction': prediction_result.predictions,
            'anomalies': anomaly_result.anomalies[:5]  # ìƒìœ„ 5ê°œë§Œ
        }
        
        return AnalysisResult(
            analysis_type="comprehensive",
            title=f"ì¢…í•© ë°ì´í„° ë¶„ì„ ({hours}ì‹œê°„)",
            description=f"{len(sensors)}ê°œ ì„¼ì„œ ì¢…í•© ë¶„ì„ (ìƒê´€ì„±, íˆíŠ¸ë§µ, ì˜ˆì¸¡, ì´ìƒì¹˜)",
            dataframe=df,
            heatmap_data=comprehensive_data,
            insights=comprehensive_insights,
            confidence_score=0.85,
            data_quality_score=min(1.0, len(df) / (hours * 60))
        )
    
    async def _basic_statistical_analysis(self, df: pd.DataFrame, sensors: List[str], hours: int) -> AnalysisResult:
        """ê¸°ë³¸ í†µê³„ ë¶„ì„"""
        
        sensor_data = df[sensors].select_dtypes(include=[np.number])
        summary_stats = sensor_data.describe().round(3).to_dict()
        
        insights = []
        insights.append(f"{len(sensors)}ê°œ ì„¼ì„œ {hours}ì‹œê°„ ê¸°ë³¸ í†µê³„")
        insights.append(f"ì´ ë°ì´í„° í¬ì¸íŠ¸: {len(df):,}ê°œ")
        
        for sensor in sensors:
            if sensor in summary_stats:
                mean_val = summary_stats[sensor]['mean']
                std_val = summary_stats[sensor]['std']
                cv = std_val / mean_val if mean_val != 0 else 0
                
                insights.append(f"{sensor}: í‰ê· ={mean_val:.2f}, í‘œì¤€í¸ì°¨={std_val:.2f}, CV={cv:.3f}")
        
        return AnalysisResult(
            analysis_type="statistical",
            title=f"ê¸°ë³¸ í†µê³„ ë¶„ì„ ({hours}ì‹œê°„)",
            description=f"{len(sensors)}ê°œ ì„¼ì„œ ê¸°ë³¸ í†µê³„ëŸ‰",
            dataframe=df,
            summary_stats=summary_stats,
            insights=insights,
            confidence_score=0.9,
            data_quality_score=min(1.0, len(df) / (hours * 60))
        )


# ì „ì—­ ë¶„ì„ ì—”ì§„
pandas_engine = PandasAnalysisEngine()


async def analyze_sensors_dynamic(
    sensors: List[str],
    analysis_type: str = "comprehensive",
    hours: int = 168,
    **kwargs
) -> AnalysisResult:
    """ë™ì  ì„¼ì„œ ë°ì´í„° ë¶„ì„"""
    return await pandas_engine.analyze_sensor_data(sensors, analysis_type, hours, **kwargs)


async def quick_correlation_analysis(sensors: List[str], hours: int = 24) -> Dict:
    """ë¹ ë¥¸ ìƒê´€ì„± ë¶„ì„"""
    result = await pandas_engine.analyze_sensor_data(sensors, "correlation", hours)
    return {
        'correlations': result.correlations,
        'insights': result.insights,
        'heatmap_data': result.heatmap_data
    }


async def predict_sensor_values(sensor: str, hours_ahead: int = 24) -> Dict:
    """ì„¼ì„œ ê°’ ì˜ˆì¸¡"""
    result = await pandas_engine.analyze_sensor_data([sensor], "prediction", 168, 
                                                   target_sensor=sensor, 
                                                   prediction_horizon=hours_ahead)
    return result.predictions